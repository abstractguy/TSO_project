{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXYd05_VOo0x"
   },
   "source": [
    "Convert a MobileNet's weights to a C array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLUeP4ZvO46e"
   },
   "source": [
    "TODO: Implement MobileNetv1-SSDLite with various optimizations and keep it fitting in an ESP32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSegYUsJrlKp"
   },
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9_hpZG66D3Le",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.models.Model as Model\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.regularizers.l2 as l2\n",
    "import tensorflow.keras.utils as keras_utils\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTQYjL_Rr2p_"
   },
   "source": [
    "Modifiable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uBQWLaz7NlWd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_type = 'full'\n",
    "dataset = 'coco17'\n",
    "training = True\n",
    "fine_tuning = False\n",
    "default_size = 96\n",
    "channels = 3\n",
    "alpha = 0.35\n",
    "learning_rate = 0.000025\n",
    "include_top = True\n",
    "pooling = None\n",
    "lr_patience = 8\n",
    "stopper_patience = 8\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "weights = None\n",
    "split_weights = (8, 1, 1)\n",
    "shuffle_buffer_size = 1024\n",
    "data_dir = '/media/samuel/DATA2/datasets/' # Set to None to load complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf2c54JIr6uE"
   },
   "source": [
    "Try not to modify these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-6U8XIfYrbks",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (default_size, default_size, channels)\n",
    "x_size, y_size, channel_size = input_shape\n",
    "origin = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'\n",
    "base_weight_path = ('https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/')\n",
    "url = 'https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip'\n",
    "model_filename = 'mobilenet_model_quantized'\n",
    "model_tflite = model_filename + '.tflite'\n",
    "model_cc = model_filename + '.cc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coordinates(tensor, start_index, conversion, border_pixels='half'):\n",
    "    '''\n",
    "    Convert coordinates for axis-aligned 2D boxes between two coordinate formats.\n",
    "    Creates a copy of `tensor`, i.e. does not operate in place. Currently there are\n",
    "    three supported coordinate formats that can be converted from and to each other:\n",
    "        1) (xmin, xmax, ymin, ymax) - the 'minmax' format\n",
    "        2) (xmin, ymin, xmax, ymax) - the 'corners' format\n",
    "        2) (cx, cy, w, h) - the 'centroids' format\n",
    "    Arguments:\n",
    "        tensor (array): A Numpy nD array containing the four consecutive coordinates\n",
    "            to be converted somewhere in the last axis.\n",
    "        start_index (int): The index of the first coordinate in the last axis of `tensor`.\n",
    "        conversion (str, optional): The conversion direction. Can be 'minmax2centroids',\n",
    "            'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners',\n",
    "            or 'corners2minmax'.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "    Returns:\n",
    "        A Numpy nD array, a copy of the input tensor with the converted coordinates\n",
    "        in place of the original coordinates and the unaltered elements of the original\n",
    "        tensor elsewhere.\n",
    "    '''\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1\n",
    "\n",
    "    ind = start_index\n",
    "    tensor1 = np.copy(tensor).astype(np.float)\n",
    "    if conversion == 'minmax2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+1]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+2] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+2] + d # Set h\n",
    "    elif conversion == 'centroids2minmax':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif conversion == 'corners2centroids':\n",
    "        tensor1[..., ind] = (tensor[..., ind] + tensor[..., ind+2]) / 2.0 # Set cx\n",
    "        tensor1[..., ind+1] = (tensor[..., ind+1] + tensor[..., ind+3]) / 2.0 # Set cy\n",
    "        tensor1[..., ind+2] = tensor[..., ind+2] - tensor[..., ind] + d # Set w\n",
    "        tensor1[..., ind+3] = tensor[..., ind+3] - tensor[..., ind+1] + d # Set h\n",
    "    elif conversion == 'centroids2corners':\n",
    "        tensor1[..., ind] = tensor[..., ind] - tensor[..., ind+2] / 2.0 # Set xmin\n",
    "        tensor1[..., ind+1] = tensor[..., ind+1] - tensor[..., ind+3] / 2.0 # Set ymin\n",
    "        tensor1[..., ind+2] = tensor[..., ind] + tensor[..., ind+2] / 2.0 # Set xmax\n",
    "        tensor1[..., ind+3] = tensor[..., ind+1] + tensor[..., ind+3] / 2.0 # Set ymax\n",
    "    elif (conversion == 'minmax2corners') or (conversion == 'corners2minmax'):\n",
    "        tensor1[..., ind+1] = tensor[..., ind+2]\n",
    "        tensor1[..., ind+2] = tensor[..., ind+1]\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids', 'centroids2minmax', 'corners2centroids', 'centroids2corners', 'minmax2corners', and 'corners2minmax'.\")\n",
    "\n",
    "    return tensor1\n",
    "\n",
    "def convert_coordinates2(tensor, start_index, conversion):\n",
    "    '''\n",
    "    A matrix multiplication implementation of `convert_coordinates()`.\n",
    "    Supports only conversion between the 'centroids' and 'minmax' formats.\n",
    "    This function is marginally slower on average than `convert_coordinates()`,\n",
    "    probably because it involves more (unnecessary) arithmetic operations (unnecessary\n",
    "    because the two matrices are sparse).\n",
    "    For details please refer to the documentation of `convert_coordinates()`.\n",
    "    '''\n",
    "    ind = start_index\n",
    "    tensor1 = np.copy(tensor).astype(np.float)\n",
    "    if conversion == 'minmax2centroids':\n",
    "        M = np.array([[0.5, 0. , -1.,  0.],\n",
    "                      [0.5, 0. ,  1.,  0.],\n",
    "                      [0. , 0.5,  0., -1.],\n",
    "                      [0. , 0.5,  0.,  1.]])\n",
    "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
    "    elif conversion == 'centroids2minmax':\n",
    "        M = np.array([[ 1. , 1. ,  0. , 0. ],\n",
    "                      [ 0. , 0. ,  1. , 1. ],\n",
    "                      [-0.5, 0.5,  0. , 0. ],\n",
    "                      [ 0. , 0. , -0.5, 0.5]]) # The multiplicative inverse of the matrix above\n",
    "        tensor1[..., ind:ind+4] = np.dot(tensor1[..., ind:ind+4], M)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected conversion value. Supported values are 'minmax2centroids' and 'centroids2minmax'.\")\n",
    "\n",
    "    return tensor1\n",
    "\n",
    "def intersection_area(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
    "    '''\n",
    "    Computes the intersection areas of two sets of axis-aligned 2D rectangular boxes.\n",
    "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
    "    In 'outer_product' mode, returns an `(m,n)` matrix with the intersection areas for all possible\n",
    "    combinations of the boxes in `boxes1` and `boxes2`.\n",
    "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
    "    of the `mode` argument for details.\n",
    "    Arguments:\n",
    "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
    "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
    "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
    "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
    "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
    "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
    "            `(xmin, ymin, xmax, ymax)`.\n",
    "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
    "            `(m,n)` matrix with the intersection areas for all possible combinations of the `m` boxes in `boxes1` with the\n",
    "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
    "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
    "            length `m` where the i-th position contains the intersection area of `boxes1[i]` with `boxes2[i]`.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "    Returns:\n",
    "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values with\n",
    "        the intersection areas of the boxes in `boxes1` and `boxes2`.\n",
    "    '''\n",
    "\n",
    "    # Make sure the boxes have the right shapes.\n",
    "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
    "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
    "\n",
    "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
    "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
    "\n",
    "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
    "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\",format(mode))\n",
    "\n",
    "    # Convert the coordinates if necessary.\n",
    "    if coords == 'centroids':\n",
    "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
    "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
    "        coords = 'corners'\n",
    "    elif not (coords in {'minmax', 'corners'}):\n",
    "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
    "\n",
    "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
    "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
    "\n",
    "    # Set the correct coordinate indices for the respective formats.\n",
    "    if coords == 'corners':\n",
    "        xmin = 0\n",
    "        ymin = 1\n",
    "        xmax = 2\n",
    "        ymax = 3\n",
    "    elif coords == 'minmax':\n",
    "        xmin = 0\n",
    "        xmax = 1\n",
    "        ymin = 2\n",
    "        ymax = 3\n",
    "\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
    "\n",
    "    # Compute the intersection areas.\n",
    "\n",
    "    if mode == 'outer_product':\n",
    "\n",
    "        # For all possible box combinations, get the greater xmin and ymin values.\n",
    "        # This is a tensor of shape (m,n,2).\n",
    "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
    "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
    "\n",
    "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
    "        # This is a tensor of shape (m,n,2).\n",
    "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
    "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
    "\n",
    "        # Compute the side lengths of the intersection rectangles.\n",
    "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
    "\n",
    "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
    "\n",
    "    elif mode == 'element-wise':\n",
    "\n",
    "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
    "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
    "\n",
    "        # Compute the side lengths of the intersection rectangles.\n",
    "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
    "\n",
    "        return side_lengths[:,0] * side_lengths[:,1]\n",
    "\n",
    "def intersection_area_(boxes1, boxes2, coords='corners', mode='outer_product', border_pixels='half'):\n",
    "    '''\n",
    "    The same as 'intersection_area()' but for internal use, i.e. without all the safety checks.\n",
    "    '''\n",
    "\n",
    "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
    "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
    "\n",
    "    # Set the correct coordinate indices for the respective formats.\n",
    "    if coords == 'corners':\n",
    "        xmin = 0\n",
    "        ymin = 1\n",
    "        xmax = 2\n",
    "        ymax = 3\n",
    "    elif coords == 'minmax':\n",
    "        xmin = 0\n",
    "        xmax = 1\n",
    "        ymin = 2\n",
    "        ymax = 3\n",
    "\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
    "\n",
    "    # Compute the intersection areas.\n",
    "\n",
    "    if mode == 'outer_product':\n",
    "\n",
    "        # For all possible box combinations, get the greater xmin and ymin values.\n",
    "        # This is a tensor of shape (m,n,2).\n",
    "        min_xy = np.maximum(np.tile(np.expand_dims(boxes1[:,[xmin,ymin]], axis=1), reps=(1, n, 1)),\n",
    "                            np.tile(np.expand_dims(boxes2[:,[xmin,ymin]], axis=0), reps=(m, 1, 1)))\n",
    "\n",
    "        # For all possible box combinations, get the smaller xmax and ymax values.\n",
    "        # This is a tensor of shape (m,n,2).\n",
    "        max_xy = np.minimum(np.tile(np.expand_dims(boxes1[:,[xmax,ymax]], axis=1), reps=(1, n, 1)),\n",
    "                            np.tile(np.expand_dims(boxes2[:,[xmax,ymax]], axis=0), reps=(m, 1, 1)))\n",
    "\n",
    "        # Compute the side lengths of the intersection rectangles.\n",
    "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
    "\n",
    "        return side_lengths[:,:,0] * side_lengths[:,:,1]\n",
    "\n",
    "    elif mode == 'element-wise':\n",
    "\n",
    "        min_xy = np.maximum(boxes1[:,[xmin,ymin]], boxes2[:,[xmin,ymin]])\n",
    "        max_xy = np.minimum(boxes1[:,[xmax,ymax]], boxes2[:,[xmax,ymax]])\n",
    "\n",
    "        # Compute the side lengths of the intersection rectangles.\n",
    "        side_lengths = np.maximum(0, max_xy - min_xy + d)\n",
    "\n",
    "        return side_lengths[:,0] * side_lengths[:,1]\n",
    "\n",
    "\n",
    "def iou(boxes1, boxes2, coords='centroids', mode='outer_product', border_pixels='half'):\n",
    "    '''\n",
    "    Computes the intersection-over-union similarity (also known as Jaccard similarity)\n",
    "    of two sets of axis-aligned 2D rectangular boxes.\n",
    "    Let `boxes1` and `boxes2` contain `m` and `n` boxes, respectively.\n",
    "    In 'outer_product' mode, returns an `(m,n)` matrix with the IoUs for all possible\n",
    "    combinations of the boxes in `boxes1` and `boxes2`.\n",
    "    In 'element-wise' mode, `m` and `n` must be broadcast-compatible. Refer to the explanation\n",
    "    of the `mode` argument for details.\n",
    "    Arguments:\n",
    "        boxes1 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
    "            format specified by `coords` or a 2D Numpy array of shape `(m, 4)` containing the coordinates for `m` boxes.\n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes2`.\n",
    "        boxes2 (array): Either a 1D Numpy array of shape `(4, )` containing the coordinates for one box in the\n",
    "            format specified by `coords` or a 2D Numpy array of shape `(n, 4)` containing the coordinates for `n` boxes.\n",
    "            If `mode` is set to 'element_wise', the shape must be broadcast-compatible with `boxes1`.\n",
    "        coords (str, optional): The coordinate format in the input arrays. Can be either 'centroids' for the format\n",
    "            `(cx, cy, w, h)`, 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format\n",
    "            `(xmin, ymin, xmax, ymax)`.\n",
    "        mode (str, optional): Can be one of 'outer_product' and 'element-wise'. In 'outer_product' mode, returns an\n",
    "            `(m,n)` matrix with the IoU overlaps for all possible combinations of the `m` boxes in `boxes1` with the\n",
    "            `n` boxes in `boxes2`. In 'element-wise' mode, returns a 1D array and the shapes of `boxes1` and `boxes2`\n",
    "            must be boadcast-compatible. If both `boxes1` and `boxes2` have `m` boxes, then this returns an array of\n",
    "            length `m` where the i-th position contains the IoU overlap of `boxes1[i]` with `boxes2[i]`.\n",
    "        border_pixels (str, optional): How to treat the border pixels of the bounding boxes.\n",
    "            Can be 'include', 'exclude', or 'half'. If 'include', the border pixels belong\n",
    "            to the boxes. If 'exclude', the border pixels do not belong to the boxes.\n",
    "            If 'half', then one of each of the two horizontal and vertical borders belong\n",
    "            to the boxex, but not the other.\n",
    "    Returns:\n",
    "        A 1D or 2D Numpy array (refer to the `mode` argument for details) of dtype float containing values in [0,1],\n",
    "        the Jaccard similarity of the boxes in `boxes1` and `boxes2`. 0 means there is no overlap between two given\n",
    "        boxes, 1 means their coordinates are identical.\n",
    "    '''\n",
    "\n",
    "    # Make sure the boxes have the right shapes.\n",
    "    if boxes1.ndim > 2: raise ValueError(\"boxes1 must have rank either 1 or 2, but has rank {}.\".format(boxes1.ndim))\n",
    "    if boxes2.ndim > 2: raise ValueError(\"boxes2 must have rank either 1 or 2, but has rank {}.\".format(boxes2.ndim))\n",
    "\n",
    "    if boxes1.ndim == 1: boxes1 = np.expand_dims(boxes1, axis=0)\n",
    "    if boxes2.ndim == 1: boxes2 = np.expand_dims(boxes2, axis=0)\n",
    "\n",
    "    if not (boxes1.shape[1] == boxes2.shape[1] == 4): raise ValueError(\"All boxes must consist of 4 coordinates, but the boxes in `boxes1` and `boxes2` have {} and {} coordinates, respectively.\".format(boxes1.shape[1], boxes2.shape[1]))\n",
    "    if not mode in {'outer_product', 'element-wise'}: raise ValueError(\"`mode` must be one of 'outer_product' and 'element-wise', but got '{}'.\".format(mode))\n",
    "\n",
    "    # Convert the coordinates if necessary.\n",
    "    if coords == 'centroids':\n",
    "        boxes1 = convert_coordinates(boxes1, start_index=0, conversion='centroids2corners')\n",
    "        boxes2 = convert_coordinates(boxes2, start_index=0, conversion='centroids2corners')\n",
    "        coords = 'corners'\n",
    "    elif not (coords in {'minmax', 'corners'}):\n",
    "        raise ValueError(\"Unexpected value for `coords`. Supported values are 'minmax', 'corners' and 'centroids'.\")\n",
    "\n",
    "    # Compute the IoU.\n",
    "\n",
    "    # Compute the interesection areas.\n",
    "\n",
    "    intersection_areas = intersection_area_(boxes1, boxes2, coords=coords, mode=mode)\n",
    "\n",
    "    m = boxes1.shape[0] # The number of boxes in `boxes1`\n",
    "    n = boxes2.shape[0] # The number of boxes in `boxes2`\n",
    "\n",
    "    # Compute the union areas.\n",
    "\n",
    "    # Set the correct coordinate indices for the respective formats.\n",
    "    if coords == 'corners':\n",
    "        xmin = 0\n",
    "        ymin = 1\n",
    "        xmax = 2\n",
    "        ymax = 3\n",
    "    elif coords == 'minmax':\n",
    "        xmin = 0\n",
    "        xmax = 1\n",
    "        ymin = 2\n",
    "        ymax = 3\n",
    "\n",
    "    if border_pixels == 'half':\n",
    "        d = 0\n",
    "    elif border_pixels == 'include':\n",
    "        d = 1 # If border pixels are supposed to belong to the bounding boxes, we have to add one pixel to any difference `xmax - xmin` or `ymax - ymin`.\n",
    "    elif border_pixels == 'exclude':\n",
    "        d = -1 # If border pixels are not supposed to belong to the bounding boxes, we have to subtract one pixel from any difference `xmax - xmin` or `ymax - ymin`.\n",
    "\n",
    "    if mode == 'outer_product':\n",
    "\n",
    "        boxes1_areas = np.tile(np.expand_dims((boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d), axis=1), reps=(1,n))\n",
    "        boxes2_areas = np.tile(np.expand_dims((boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d), axis=0), reps=(m,1))\n",
    "\n",
    "    elif mode == 'element-wise':\n",
    "\n",
    "        boxes1_areas = (boxes1[:,xmax] - boxes1[:,xmin] + d) * (boxes1[:,ymax] - boxes1[:,ymin] + d)\n",
    "        boxes2_areas = (boxes2[:,xmax] - boxes2[:,xmin] + d) * (boxes2[:,ymax] - boxes2[:,ymin] + d)\n",
    "\n",
    "    union_areas = boxes1_areas + boxes2_areas - intersection_areas\n",
    "\n",
    "    return intersection_areas / union_areas\n",
    "\n",
    "class L2Normalization(layers.Layer):\n",
    "    '''\n",
    "    Performs L2 normalization on the input tensor with a learnable scaling parameter\n",
    "    as described in the paper \"Parsenet: Looking Wider to See Better\" (see references)\n",
    "    and as used in the original SSD model.\n",
    "    Arguments:\n",
    "        gamma_init (int): The initial scaling parameter. Defaults to 20 following the\n",
    "            SSD paper.\n",
    "    Input shape:\n",
    "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
    "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
    "    Returns:\n",
    "        The scaled tensor. Same shape as the input tensor.\n",
    "    References:\n",
    "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
    "    '''\n",
    "\n",
    "    def __init__(self, gamma_init=20, **kwargs):\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            self.axis = 3\n",
    "        else:\n",
    "            self.axis = 1\n",
    "        self.gamma_init = gamma_init\n",
    "        super(L2Normalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [layers.InputSpec(shape=input_shape)]\n",
    "        gamma = self.gamma_init * np.ones((input_shape[self.axis],))\n",
    "        self.gamma = K.variable(gamma, name='{}_gamma'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma]\n",
    "        super(L2Normalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.l2_normalize(x, self.axis)\n",
    "        return output * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'gamma_init': self.gamma_init\n",
    "        }\n",
    "        base_config = super(L2Normalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class DecodeDetectionsFast(layers.Layer):\n",
    "    '''\n",
    "    A Keras layer to decode the raw SSD prediction output.\n",
    "    Input shape:\n",
    "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
    "    Output shape:\n",
    "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 confidence_thresh=0.01,\n",
    "                 iou_threshold=0.45,\n",
    "                 top_k=200,\n",
    "                 nms_max_output_size=400,\n",
    "                 coords='centroids',\n",
    "                 normalize_coords=True,\n",
    "                 img_height=None,\n",
    "                 img_width=None,\n",
    "                 **kwargs):\n",
    "        '''\n",
    "        All default argument values follow the Caffe implementation.\n",
    "        Arguments:\n",
    "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
    "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
    "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
    "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
    "                thresholding stage.\n",
    "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
    "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
    "                to the box score.\n",
    "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
    "                non-maximum suppression stage.\n",
    "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
    "                suppression.\n",
    "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
    "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
    "                currently not supported.\n",
    "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
    "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
    "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
    "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
    "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
    "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
    "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
    "        '''\n",
    "        if K.backend() != 'tensorflow':\n",
    "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
    "\n",
    "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
    "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
    "\n",
    "        if coords != 'centroids':\n",
    "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
    "\n",
    "        # We need these members for the config.\n",
    "        self.confidence_thresh = confidence_thresh\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.top_k = top_k\n",
    "        self.normalize_coords = normalize_coords\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.coords = coords\n",
    "        self.nms_max_output_size = nms_max_output_size\n",
    "\n",
    "        # We need these members for TensorFlow.\n",
    "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
    "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
    "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
    "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
    "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
    "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
    "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
    "\n",
    "        super(DecodeDetectionsFast, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [layers.InputSpec(shape=input_shape)]\n",
    "        super(DecodeDetectionsFast, self).build(input_shape)\n",
    "\n",
    "    def call(self, y_pred, mask=None):\n",
    "        '''\n",
    "        Returns:\n",
    "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
    "            to always yield `top_k` predictions per batch item. The last axis contains\n",
    "            the coordinates for each predicted box in the format\n",
    "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
    "        '''\n",
    "\n",
    "        #####################################################################################\n",
    "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
    "        #    absolute coordinates\n",
    "        #####################################################################################\n",
    "\n",
    "        # Extract the predicted class IDs as the indices of the highest confidence values.\n",
    "        class_ids = tf.expand_dims(tf.to_float(tf.argmax(y_pred[...,:-12], axis=-1)), axis=-1)\n",
    "        # Extract the confidences of the maximal classes.\n",
    "        confidences = tf.reduce_max(y_pred[...,:-12], axis=-1, keep_dims=True)\n",
    "\n",
    "        # Convert anchor box offsets to image offsets.\n",
    "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
    "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
    "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
    "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
    "\n",
    "        # Convert 'centroids' to 'corners'.\n",
    "        xmin = cx - 0.5 * w\n",
    "        ymin = cy - 0.5 * h\n",
    "        xmax = cx + 0.5 * w\n",
    "        ymax = cy + 0.5 * h\n",
    "\n",
    "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
    "        # to be converted back to absolute coordinates, do that.\n",
    "        def normalized_coords():\n",
    "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
    "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
    "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
    "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
    "            return xmin1, ymin1, xmax1, ymax1\n",
    "        def non_normalized_coords():\n",
    "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
    "\n",
    "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
    "\n",
    "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
    "        y_pred = tf.concat(values=[class_ids, confidences, xmin, ymin, xmax, ymax], axis=-1)\n",
    "\n",
    "        #####################################################################################\n",
    "        # 2. Perform confidence thresholding, non-maximum suppression, and top-k filtering.\n",
    "        #####################################################################################\n",
    "\n",
    "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "        n_boxes = tf.shape(y_pred)[1]\n",
    "        n_classes = y_pred.shape[2] - 4\n",
    "        class_indices = tf.range(1, n_classes)\n",
    "\n",
    "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
    "        # - confidence thresholding\n",
    "        # - non-maximum suppression (NMS)\n",
    "        # - top-k filtering\n",
    "        def filter_predictions(batch_item):\n",
    "\n",
    "            # Keep only the non-background boxes.\n",
    "            positive_boxes = tf.not_equal(batch_item[...,0], 0.0)\n",
    "            predictions = tf.boolean_mask(tensor=batch_item,\n",
    "                                          mask=positive_boxes)\n",
    "\n",
    "            def perform_confidence_thresholding():\n",
    "                # Apply confidence thresholding.\n",
    "                threshold_met = predictions[:,1] > self.tf_confidence_thresh\n",
    "                return tf.boolean_mask(tensor=predictions,\n",
    "                                       mask=threshold_met)\n",
    "            def no_positive_boxes():\n",
    "                return tf.constant(value=0.0, shape=(1,6))\n",
    "\n",
    "            # If there are any positive predictions, perform confidence thresholding.\n",
    "            predictions_conf_thresh = tf.cond(tf.equal(tf.size(predictions), 0), no_positive_boxes, perform_confidence_thresholding)\n",
    "\n",
    "            def perform_nms():\n",
    "                scores = predictions_conf_thresh[...,1]\n",
    "\n",
    "                # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
    "                xmin = tf.expand_dims(predictions_conf_thresh[...,-4], axis=-1)\n",
    "                ymin = tf.expand_dims(predictions_conf_thresh[...,-3], axis=-1)\n",
    "                xmax = tf.expand_dims(predictions_conf_thresh[...,-2], axis=-1)\n",
    "                ymax = tf.expand_dims(predictions_conf_thresh[...,-1], axis=-1)\n",
    "                boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
    "\n",
    "                maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
    "                                                              scores=scores,\n",
    "                                                              max_output_size=self.tf_nms_max_output_size,\n",
    "                                                              iou_threshold=self.iou_threshold,\n",
    "                                                              name='non_maximum_suppresion')\n",
    "                maxima = tf.gather(params=predictions_conf_thresh,\n",
    "                                   indices=maxima_indices,\n",
    "                                   axis=0)\n",
    "                return maxima\n",
    "            def no_confident_predictions():\n",
    "                return tf.constant(value=0.0, shape=(1,6))\n",
    "\n",
    "            # If any boxes made the threshold, perform NMS.\n",
    "            predictions_nms = tf.cond(tf.equal(tf.size(predictions_conf_thresh), 0), no_confident_predictions, perform_nms)\n",
    "\n",
    "            # Perform top-k filtering for this batch item or pad it in case there are\n",
    "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
    "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
    "            # for the whole batch, all batch items must have the same number of predicted\n",
    "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
    "            # predictions are left after the filtering process above, we pad the missing\n",
    "            # predictions with zeros as dummy entries.\n",
    "            def top_k():\n",
    "                return tf.gather(params=predictions_nms,\n",
    "                                 indices=tf.nn.top_k(predictions_nms[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
    "                                 axis=0)\n",
    "            def pad_and_top_k():\n",
    "                padded_predictions = tf.pad(tensor=predictions_nms,\n",
    "                                            paddings=[[0, self.tf_top_k - tf.shape(predictions_nms)[0]], [0, 0]],\n",
    "                                            mode='CONSTANT',\n",
    "                                            constant_values=0.0)\n",
    "                return tf.gather(params=padded_predictions,\n",
    "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
    "                                 axis=0)\n",
    "\n",
    "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(predictions_nms)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
    "\n",
    "            return top_k_boxes\n",
    "\n",
    "        # Iterate `filter_predictions()` over all batch items.\n",
    "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
    "                                  elems=y_pred,\n",
    "                                  dtype=None,\n",
    "                                  parallel_iterations=128,\n",
    "                                  back_prop=False,\n",
    "                                  swap_memory=False,\n",
    "                                  infer_shape=True,\n",
    "                                  name='loop_over_batch')\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size, n_boxes, last_axis = input_shape\n",
    "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'confidence_thresh': self.confidence_thresh,\n",
    "            'iou_threshold': self.iou_threshold,\n",
    "            'top_k': self.top_k,\n",
    "            'nms_max_output_size': self.nms_max_output_size,\n",
    "            'coords': self.coords,\n",
    "            'normalize_coords': self.normalize_coords,\n",
    "            'img_height': self.img_height,\n",
    "            'img_width': self.img_width,\n",
    "        }\n",
    "        base_config = super(DecodeDetectionsFast, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class DecodeDetections(layers.Layer):\n",
    "    '''\n",
    "    A Keras layer to decode the raw SSD prediction output.\n",
    "    Input shape:\n",
    "        3D tensor of shape `(batch_size, n_boxes, n_classes + 12)`.\n",
    "    Output shape:\n",
    "        3D tensor of shape `(batch_size, top_k, 6)`.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 confidence_thresh=0.01,\n",
    "                 iou_threshold=0.45,\n",
    "                 top_k=200,\n",
    "                 nms_max_output_size=400,\n",
    "                 coords='centroids',\n",
    "                 normalize_coords=True,\n",
    "                 img_height=None,\n",
    "                 img_width=None,\n",
    "                 **kwargs):\n",
    "        '''\n",
    "        All default argument values follow the Caffe implementation.\n",
    "        Arguments:\n",
    "            confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
    "                positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
    "                A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
    "                stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
    "                thresholding stage.\n",
    "            iou_threshold (float, optional): A float in [0,1]. All boxes with a Jaccard similarity of greater than `iou_threshold`\n",
    "                with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
    "                to the box score.\n",
    "            top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
    "                non-maximum suppression stage.\n",
    "            nms_max_output_size (int, optional): The maximum number of predictions that will be left after performing non-maximum\n",
    "                suppression.\n",
    "            coords (str, optional): The box coordinate format that the model outputs. Must be 'centroids'\n",
    "                i.e. the format `(cx, cy, w, h)` (box center coordinates, width, and height). Other coordinate formats are\n",
    "                currently not supported.\n",
    "            normalize_coords (bool, optional): Set to `True` if the model outputs relative coordinates (i.e. coordinates in [0,1])\n",
    "                and you wish to transform these relative coordinates back to absolute coordinates. If the model outputs\n",
    "                relative coordinates, but you do not want to convert them back to absolute coordinates, set this to `False`.\n",
    "                Do not set this to `True` if the model already outputs absolute coordinates, as that would result in incorrect\n",
    "                coordinates. Requires `img_height` and `img_width` if set to `True`.\n",
    "            img_height (int, optional): The height of the input images. Only needed if `normalize_coords` is `True`.\n",
    "            img_width (int, optional): The width of the input images. Only needed if `normalize_coords` is `True`.\n",
    "        '''\n",
    "        if K.backend() != 'tensorflow':\n",
    "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
    "\n",
    "        if normalize_coords and ((img_height is None) or (img_width is None)):\n",
    "            raise ValueError(\"If relative box coordinates are supposed to be converted to absolute coordinates, the decoder needs the image size in order to decode the predictions, but `img_height == {}` and `img_width == {}`\".format(img_height, img_width))\n",
    "\n",
    "        if coords != 'centroids':\n",
    "            raise ValueError(\"The DetectionOutput layer currently only supports the 'centroids' coordinate format.\")\n",
    "\n",
    "        # We need these members for the config.\n",
    "        self.confidence_thresh = confidence_thresh\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.top_k = top_k\n",
    "        self.normalize_coords = normalize_coords\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.coords = coords\n",
    "        self.nms_max_output_size = nms_max_output_size\n",
    "\n",
    "        # We need these members for TensorFlow.\n",
    "        self.tf_confidence_thresh = tf.constant(self.confidence_thresh, name='confidence_thresh')\n",
    "        self.tf_iou_threshold = tf.constant(self.iou_threshold, name='iou_threshold')\n",
    "        self.tf_top_k = tf.constant(self.top_k, name='top_k')\n",
    "        self.tf_normalize_coords = tf.constant(self.normalize_coords, name='normalize_coords')\n",
    "        self.tf_img_height = tf.constant(self.img_height, dtype=tf.float32, name='img_height')\n",
    "        self.tf_img_width = tf.constant(self.img_width, dtype=tf.float32, name='img_width')\n",
    "        self.tf_nms_max_output_size = tf.constant(self.nms_max_output_size, name='nms_max_output_size')\n",
    "\n",
    "        super(DecodeDetections, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [layers.InputSpec(shape=input_shape)]\n",
    "        super(DecodeDetections, self).build(input_shape)\n",
    "\n",
    "    def call(self, y_pred, mask=None):\n",
    "        '''\n",
    "        Returns:\n",
    "            3D tensor of shape `(batch_size, top_k, 6)`. The second axis is zero-padded\n",
    "            to always yield `top_k` predictions per batch item. The last axis contains\n",
    "            the coordinates for each predicted box in the format\n",
    "            `[class_id, confidence, xmin, ymin, xmax, ymax]`.\n",
    "        '''\n",
    "\n",
    "        #####################################################################################\n",
    "        # 1. Convert the box coordinates from predicted anchor box offsets to predicted\n",
    "        #    absolute coordinates\n",
    "        #####################################################################################\n",
    "\n",
    "        # Convert anchor box offsets to image offsets.\n",
    "        cx = y_pred[...,-12] * y_pred[...,-4] * y_pred[...,-6] + y_pred[...,-8] # cx = cx_pred * cx_variance * w_anchor + cx_anchor\n",
    "        cy = y_pred[...,-11] * y_pred[...,-3] * y_pred[...,-5] + y_pred[...,-7] # cy = cy_pred * cy_variance * h_anchor + cy_anchor\n",
    "        w = tf.exp(y_pred[...,-10] * y_pred[...,-2]) * y_pred[...,-6] # w = exp(w_pred * variance_w) * w_anchor\n",
    "        h = tf.exp(y_pred[...,-9] * y_pred[...,-1]) * y_pred[...,-5] # h = exp(h_pred * variance_h) * h_anchor\n",
    "\n",
    "        # Convert 'centroids' to 'corners'.\n",
    "        xmin = cx - 0.5 * w\n",
    "        ymin = cy - 0.5 * h\n",
    "        xmax = cx + 0.5 * w\n",
    "        ymax = cy + 0.5 * h\n",
    "\n",
    "        # If the model predicts box coordinates relative to the image dimensions and they are supposed\n",
    "        # to be converted back to absolute coordinates, do that.\n",
    "        def normalized_coords():\n",
    "            xmin1 = tf.expand_dims(xmin * self.tf_img_width, axis=-1)\n",
    "            ymin1 = tf.expand_dims(ymin * self.tf_img_height, axis=-1)\n",
    "            xmax1 = tf.expand_dims(xmax * self.tf_img_width, axis=-1)\n",
    "            ymax1 = tf.expand_dims(ymax * self.tf_img_height, axis=-1)\n",
    "            return xmin1, ymin1, xmax1, ymax1\n",
    "        def non_normalized_coords():\n",
    "            return tf.expand_dims(xmin, axis=-1), tf.expand_dims(ymin, axis=-1), tf.expand_dims(xmax, axis=-1), tf.expand_dims(ymax, axis=-1)\n",
    "\n",
    "        xmin, ymin, xmax, ymax = tf.cond(self.tf_normalize_coords, normalized_coords, non_normalized_coords)\n",
    "\n",
    "        # Concatenate the one-hot class confidences and the converted box coordinates to form the decoded predictions tensor.\n",
    "        y_pred = tf.concat(values=[y_pred[...,:-12], xmin, ymin, xmax, ymax], axis=-1)\n",
    "\n",
    "        #####################################################################################\n",
    "        # 2. Perform confidence thresholding, per-class non-maximum suppression, and\n",
    "        #    top-k filtering.\n",
    "        #####################################################################################\n",
    "\n",
    "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "        n_boxes = tf.shape(y_pred)[1]\n",
    "        n_classes = y_pred.shape[2] - 4\n",
    "        class_indices = tf.range(1, n_classes)\n",
    "\n",
    "        # Create a function that filters the predictions for the given batch item. Specifically, it performs:\n",
    "        # - confidence thresholding\n",
    "        # - non-maximum suppression (NMS)\n",
    "        # - top-k filtering\n",
    "        def filter_predictions(batch_item):\n",
    "\n",
    "            # Create a function that filters the predictions for one single class.\n",
    "            def filter_single_class(index):\n",
    "\n",
    "                # From a tensor of shape (n_boxes, n_classes + 4 coordinates) extract\n",
    "                # a tensor of shape (n_boxes, 1 + 4 coordinates) that contains the\n",
    "                # confidnece values for just one class, determined by `index`.\n",
    "                confidences = tf.expand_dims(batch_item[..., index], axis=-1)\n",
    "                class_id = tf.fill(dims=tf.shape(confidences), value=tf.to_float(index))\n",
    "                box_coordinates = batch_item[...,-4:]\n",
    "\n",
    "                single_class = tf.concat([class_id, confidences, box_coordinates], axis=-1)\n",
    "\n",
    "                # Apply confidence thresholding with respect to the class defined by `index`.\n",
    "                threshold_met = single_class[:,1] > self.tf_confidence_thresh\n",
    "                single_class = tf.boolean_mask(tensor=single_class,\n",
    "                                               mask=threshold_met)\n",
    "\n",
    "                # If any boxes made the threshold, perform NMS.\n",
    "                def perform_nms():\n",
    "                    scores = single_class[...,1]\n",
    "\n",
    "                    # `tf.image.non_max_suppression()` needs the box coordinates in the format `(ymin, xmin, ymax, xmax)`.\n",
    "                    xmin = tf.expand_dims(single_class[...,-4], axis=-1)\n",
    "                    ymin = tf.expand_dims(single_class[...,-3], axis=-1)\n",
    "                    xmax = tf.expand_dims(single_class[...,-2], axis=-1)\n",
    "                    ymax = tf.expand_dims(single_class[...,-1], axis=-1)\n",
    "                    boxes = tf.concat(values=[ymin, xmin, ymax, xmax], axis=-1)\n",
    "\n",
    "                    maxima_indices = tf.image.non_max_suppression(boxes=boxes,\n",
    "                                                                  scores=scores,\n",
    "                                                                  max_output_size=self.tf_nms_max_output_size,\n",
    "                                                                  iou_threshold=self.iou_threshold,\n",
    "                                                                  name='non_maximum_suppresion')\n",
    "                    maxima = tf.gather(params=single_class,\n",
    "                                       indices=maxima_indices,\n",
    "                                       axis=0)\n",
    "                    return maxima\n",
    "\n",
    "                def no_confident_predictions():\n",
    "                    return tf.constant(value=0.0, shape=(1,6))\n",
    "\n",
    "                single_class_nms = tf.cond(tf.equal(tf.size(single_class), 0), no_confident_predictions, perform_nms)\n",
    "\n",
    "                # Make sure `single_class` is exactly `self.nms_max_output_size` elements long.\n",
    "                padded_single_class = tf.pad(tensor=single_class_nms,\n",
    "                                             paddings=[[0, self.tf_nms_max_output_size - tf.shape(single_class_nms)[0]], [0, 0]],\n",
    "                                             mode='CONSTANT',\n",
    "                                             constant_values=0.0)\n",
    "\n",
    "                return padded_single_class\n",
    "\n",
    "            # Iterate `filter_single_class()` over all class indices.\n",
    "            filtered_single_classes = tf.map_fn(fn=lambda i: filter_single_class(i),\n",
    "                                                elems=tf.range(1,n_classes),\n",
    "                                                dtype=tf.float32,\n",
    "                                                parallel_iterations=128,\n",
    "                                                back_prop=False,\n",
    "                                                swap_memory=False,\n",
    "                                                infer_shape=True,\n",
    "                                                name='loop_over_classes')\n",
    "\n",
    "            # Concatenate the filtered results for all individual classes to one tensor.\n",
    "            filtered_predictions = tf.reshape(tensor=filtered_single_classes, shape=(-1,6))\n",
    "\n",
    "            # Perform top-k filtering for this batch item or pad it in case there are\n",
    "            # fewer than `self.top_k` boxes left at this point. Either way, produce a\n",
    "            # tensor of length `self.top_k`. By the time we return the final results tensor\n",
    "            # for the whole batch, all batch items must have the same number of predicted\n",
    "            # boxes so that the tensor dimensions are homogenous. If fewer than `self.top_k`\n",
    "            # predictions are left after the filtering process above, we pad the missing\n",
    "            # predictions with zeros as dummy entries.\n",
    "            def top_k():\n",
    "                return tf.gather(params=filtered_predictions,\n",
    "                                 indices=tf.nn.top_k(filtered_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
    "                                 axis=0)\n",
    "            def pad_and_top_k():\n",
    "                padded_predictions = tf.pad(tensor=filtered_predictions,\n",
    "                                            paddings=[[0, self.tf_top_k - tf.shape(filtered_predictions)[0]], [0, 0]],\n",
    "                                            mode='CONSTANT',\n",
    "                                            constant_values=0.0)\n",
    "                return tf.gather(params=padded_predictions,\n",
    "                                 indices=tf.nn.top_k(padded_predictions[:, 1], k=self.tf_top_k, sorted=True).indices,\n",
    "                                 axis=0)\n",
    "\n",
    "            top_k_boxes = tf.cond(tf.greater_equal(tf.shape(filtered_predictions)[0], self.tf_top_k), top_k, pad_and_top_k)\n",
    "\n",
    "            return top_k_boxes\n",
    "\n",
    "        # Iterate `filter_predictions()` over all batch items.\n",
    "        output_tensor = tf.map_fn(fn=lambda x: filter_predictions(x),\n",
    "                                  elems=y_pred,\n",
    "                                  dtype=None,\n",
    "                                  parallel_iterations=128,\n",
    "                                  back_prop=False,\n",
    "                                  swap_memory=False,\n",
    "                                  infer_shape=True,\n",
    "                                  name='loop_over_batch')\n",
    "\n",
    "        return output_tensor\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size, n_boxes, last_axis = input_shape\n",
    "        return (batch_size, self.tf_top_k, 6) # Last axis: (class_ID, confidence, 4 box coordinates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'confidence_thresh': self.confidence_thresh,\n",
    "            'iou_threshold': self.iou_threshold,\n",
    "            'top_k': self.top_k,\n",
    "            'nms_max_output_size': self.nms_max_output_size,\n",
    "            'coords': self.coords,\n",
    "            'normalize_coords': self.normalize_coords,\n",
    "            'img_height': self.img_height,\n",
    "            'img_width': self.img_width,\n",
    "        }\n",
    "        base_config = super(DecodeDetections, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class AnchorBoxes(layers.Layer):\n",
    "    '''\n",
    "    A Keras layer to create an output tensor containing anchor box coordinates\n",
    "    and variances based on the input tensor and the passed arguments.\n",
    "    A set of 2D anchor boxes of different aspect ratios is created for each spatial unit of\n",
    "    the input tensor. The number of anchor boxes created per unit depends on the arguments\n",
    "    `aspect_ratios` and `two_boxes_for_ar1`, in the default case it is 4. The boxes\n",
    "    are parameterized by the coordinate tuple `(xmin, xmax, ymin, ymax)`.\n",
    "    The logic implemented by this layer is identical to the logic in the module\n",
    "    `ssd_box_encode_decode_utils.py`.\n",
    "    The purpose of having this layer in the network is to make the model self-sufficient\n",
    "    at inference time. Since the model is predicting offsets to the anchor boxes\n",
    "    (rather than predicting absolute box coordinates directly), one needs to know the anchor\n",
    "    box coordinates in order to construct the final prediction boxes from the predicted offsets.\n",
    "    If the model's output tensor did not contain the anchor box coordinates, the necessary\n",
    "    information to convert the predicted offsets back to absolute coordinates would be missing\n",
    "    in the model output. The reason why it is necessary to predict offsets to the anchor boxes\n",
    "    rather than to predict absolute box coordinates directly is explained in `README.md`.\n",
    "    Input shape:\n",
    "        4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
    "        or `(batch, height, width, channels)` if `dim_ordering = 'tf'`.\n",
    "    Output shape:\n",
    "        5D tensor of shape `(batch, height, width, n_boxes, 8)`. The last axis contains\n",
    "        the four anchor box coordinates and the four variance values for each box.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_height,\n",
    "                 img_width,\n",
    "                 this_scale,\n",
    "                 next_scale,\n",
    "                 aspect_ratios=[0.5, 1.0, 2.0],\n",
    "                 two_boxes_for_ar1=True,\n",
    "                 this_steps=None,\n",
    "                 this_offsets=None,\n",
    "                 clip_boxes=False,\n",
    "                 variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                 coords='centroids',\n",
    "                 normalize_coords=False,\n",
    "                 **kwargs):\n",
    "        '''\n",
    "        All arguments need to be set to the same values as in the box encoding process, otherwise the behavior is undefined.\n",
    "        Some of these arguments are explained in more detail in the documentation of the `SSDBoxEncoder` class.\n",
    "        Arguments:\n",
    "            img_height (int): The height of the input images.\n",
    "            img_width (int): The width of the input images.\n",
    "            this_scale (float): A float in [0, 1], the scaling factor for the size of the generated anchor boxes\n",
    "                as a fraction of the shorter side of the input image.\n",
    "            next_scale (float): A float in [0, 1], the next larger scaling factor. Only relevant if\n",
    "                `self.two_boxes_for_ar1 == True`.\n",
    "            aspect_ratios (list, optional): The list of aspect ratios for which default boxes are to be\n",
    "                generated for this layer.\n",
    "            two_boxes_for_ar1 (bool, optional): Only relevant if `aspect_ratios` contains 1.\n",
    "                If `True`, two default boxes will be generated for aspect ratio 1. The first will be generated\n",
    "                using the scaling factor for the respective layer, the second one will be generated using\n",
    "                geometric mean of said scaling factor and next bigger scaling factor.\n",
    "            clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
    "            variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
    "                its respective variance value.\n",
    "            coords (str, optional): The box coordinate format to be used internally in the model (i.e. this is not the input format\n",
    "                of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width, and height),\n",
    "                'corners' for the format `(xmin, ymin, xmax,  ymax)`, or 'minmax' for the format `(xmin, xmax, ymin, ymax)`.\n",
    "            normalize_coords (bool, optional): Set to `True` if the model uses relative instead of absolute coordinates,\n",
    "                i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
    "        '''\n",
    "        if K.backend() != 'tensorflow':\n",
    "            raise TypeError(\"This layer only supports TensorFlow at the moment, but you are using the {} backend.\".format(K.backend()))\n",
    "\n",
    "        if (this_scale < 0) or (next_scale < 0) or (this_scale > 1):\n",
    "            raise ValueError(\"`this_scale` must be in [0, 1] and `next_scale` must be >0, but `this_scale` == {}, `next_scale` == {}\".format(this_scale, next_scale))\n",
    "\n",
    "        if len(variances) != 4:\n",
    "            raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
    "        variances = np.array(variances)\n",
    "        if np.any(variances <= 0):\n",
    "            raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
    "\n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.this_scale = this_scale\n",
    "        self.next_scale = next_scale\n",
    "        self.aspect_ratios = aspect_ratios\n",
    "        self.two_boxes_for_ar1 = two_boxes_for_ar1\n",
    "        self.this_steps = this_steps\n",
    "        self.this_offsets = this_offsets\n",
    "        self.clip_boxes = clip_boxes\n",
    "        self.variances = variances\n",
    "        self.coords = coords\n",
    "        self.normalize_coords = normalize_coords\n",
    "        # Compute the number of boxes per cell\n",
    "        if (1 in aspect_ratios) and two_boxes_for_ar1:\n",
    "            self.n_boxes = len(aspect_ratios) + 1\n",
    "        else:\n",
    "            self.n_boxes = len(aspect_ratios)\n",
    "        super(AnchorBoxes, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [layers.InputSpec(shape=input_shape)]\n",
    "        super(AnchorBoxes, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''\n",
    "        Return an anchor box tensor based on the shape of the input tensor.\n",
    "        The logic implemented here is identical to the logic in the module `ssd_box_encode_decode_utils.py`.\n",
    "        Note that this tensor does not participate in any graph computations at runtime. It is being created\n",
    "        as a constant once during graph creation and is just being output along with the rest of the model output\n",
    "        during runtime. Because of this, all logic is implemented as Numpy array operations and it is sufficient\n",
    "        to convert the resulting Numpy array into a Keras tensor at the very end before outputting it.\n",
    "        Arguments:\n",
    "            x (tensor): 4D tensor of shape `(batch, channels, height, width)` if `dim_ordering = 'th'`\n",
    "                or `(batch, height, width, channels)` if `dim_ordering = 'tf'`. The input for this\n",
    "                layer must be the output of the localization predictor layer.\n",
    "        '''\n",
    "\n",
    "        # Compute box width and height for each aspect ratio\n",
    "        # The shorter side of the image will be used to compute `w` and `h` using `scale` and `aspect_ratios`.\n",
    "        size = min(self.img_height, self.img_width)\n",
    "        # Compute the box widths and and heights for all aspect ratios\n",
    "        wh_list = []\n",
    "        for ar in self.aspect_ratios:\n",
    "            if (ar == 1):\n",
    "                # Compute the regular anchor box for aspect ratio 1.\n",
    "                box_height = box_width = self.this_scale * size\n",
    "                wh_list.append((box_width, box_height))\n",
    "                if self.two_boxes_for_ar1:\n",
    "                    # Compute one slightly larger version using the geometric mean of this scale value and the next.\n",
    "                    box_height = box_width = np.sqrt(self.this_scale * self.next_scale) * size\n",
    "                    wh_list.append((box_width, box_height))\n",
    "            else:\n",
    "                box_height = self.this_scale * size / np.sqrt(ar)\n",
    "                box_width = self.this_scale * size * np.sqrt(ar)\n",
    "                wh_list.append((box_width, box_height))\n",
    "        wh_list = np.array(wh_list)\n",
    "\n",
    "        # We need the shape of the input tensor\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            batch_size, feature_map_height, feature_map_width, feature_map_channels = x._keras_shape\n",
    "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
    "            batch_size, feature_map_channels, feature_map_height, feature_map_width = x._keras_shape\n",
    "\n",
    "        # Compute the grid of box center points. They are identical for all aspect ratios.\n",
    "\n",
    "        # Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\n",
    "        if (self.this_steps is None):\n",
    "            step_height = self.img_height / feature_map_height\n",
    "            step_width = self.img_width / feature_map_width\n",
    "        else:\n",
    "            if isinstance(self.this_steps, (list, tuple)) and (len(self.this_steps) == 2):\n",
    "                step_height = self.this_steps[0]\n",
    "                step_width = self.this_steps[1]\n",
    "            elif isinstance(self.this_steps, (int, float)):\n",
    "                step_height = self.this_steps\n",
    "                step_width = self.this_steps\n",
    "        # Compute the offsets, i.e. at what pixel values the first anchor box center point will be from the top and from the left of the image.\n",
    "        if (self.this_offsets is None):\n",
    "            offset_height = 0.5\n",
    "            offset_width = 0.5\n",
    "        else:\n",
    "            if isinstance(self.this_offsets, (list, tuple)) and (len(self.this_offsets) == 2):\n",
    "                offset_height = self.this_offsets[0]\n",
    "                offset_width = self.this_offsets[1]\n",
    "            elif isinstance(self.this_offsets, (int, float)):\n",
    "                offset_height = self.this_offsets\n",
    "                offset_width = self.this_offsets\n",
    "        # Now that we have the offsets and step sizes, compute the grid of anchor box center points.\n",
    "        cy = np.linspace(offset_height * step_height, (offset_height + feature_map_height - 1) * step_height, feature_map_height)\n",
    "        cx = np.linspace(offset_width * step_width, (offset_width + feature_map_width - 1) * step_width, feature_map_width)\n",
    "        cx_grid, cy_grid = np.meshgrid(cx, cy)\n",
    "        cx_grid = np.expand_dims(cx_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "        cy_grid = np.expand_dims(cy_grid, -1) # This is necessary for np.tile() to do what we want further down\n",
    "\n",
    "        # Create a 4D tensor template of shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
    "        # where the last dimension will contain `(cx, cy, w, h)`\n",
    "        boxes_tensor = np.zeros((feature_map_height, feature_map_width, self.n_boxes, 4))\n",
    "\n",
    "        boxes_tensor[:, :, :, 0] = np.tile(cx_grid, (1, 1, self.n_boxes)) # Set cx\n",
    "        boxes_tensor[:, :, :, 1] = np.tile(cy_grid, (1, 1, self.n_boxes)) # Set cy\n",
    "        boxes_tensor[:, :, :, 2] = wh_list[:, 0] # Set w\n",
    "        boxes_tensor[:, :, :, 3] = wh_list[:, 1] # Set h\n",
    "\n",
    "        # Convert `(cx, cy, w, h)` to `(xmin, xmax, ymin, ymax)`\n",
    "        boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='centroids2corners')\n",
    "\n",
    "        # If `clip_boxes` is enabled, clip the coordinates to lie within the image boundaries\n",
    "        if self.clip_boxes:\n",
    "            x_coords = boxes_tensor[:,:,:,[0, 2]]\n",
    "            x_coords[x_coords >= self.img_width] = self.img_width - 1\n",
    "            x_coords[x_coords < 0] = 0\n",
    "            boxes_tensor[:,:,:,[0, 2]] = x_coords\n",
    "            y_coords = boxes_tensor[:,:,:,[1, 3]]\n",
    "            y_coords[y_coords >= self.img_height] = self.img_height - 1\n",
    "            y_coords[y_coords < 0] = 0\n",
    "            boxes_tensor[:,:,:,[1, 3]] = y_coords\n",
    "\n",
    "        # If `normalize_coords` is enabled, normalize the coordinates to be within [0,1]\n",
    "        if self.normalize_coords:\n",
    "            boxes_tensor[:, :, :, [0, 2]] /= self.img_width\n",
    "            boxes_tensor[:, :, :, [1, 3]] /= self.img_height\n",
    "\n",
    "        # TODO: Implement box limiting directly for `(cx, cy, w, h)` so that we don't have to unnecessarily convert back and forth.\n",
    "        if self.coords == 'centroids':\n",
    "            # Convert `(xmin, ymin, xmax, ymax)` back to `(cx, cy, w, h)`.\n",
    "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2centroids', border_pixels='half')\n",
    "        elif self.coords == 'minmax':\n",
    "            # Convert `(xmin, ymin, xmax, ymax)` to `(xmin, xmax, ymin, ymax).\n",
    "            boxes_tensor = convert_coordinates(boxes_tensor, start_index=0, conversion='corners2minmax', border_pixels='half')\n",
    "\n",
    "        # Create a tensor to contain the variances and append it to `boxes_tensor`. This tensor has the same shape\n",
    "        # as `boxes_tensor` and simply contains the same 4 variance values for every position in the last axis.\n",
    "        variances_tensor = np.zeros_like(boxes_tensor) # Has shape `(feature_map_height, feature_map_width, n_boxes, 4)`\n",
    "        variances_tensor += self.variances # Long live broadcasting\n",
    "        # Now `boxes_tensor` becomes a tensor of shape `(feature_map_height, feature_map_width, n_boxes, 8)`\n",
    "        boxes_tensor = np.concatenate((boxes_tensor, variances_tensor), axis=-1)\n",
    "\n",
    "        # Now prepend one dimension to `boxes_tensor` to account for the batch size and tile it along\n",
    "        # The result will be a 5D tensor of shape `(batch_size, feature_map_height, feature_map_width, n_boxes, 8)`\n",
    "        boxes_tensor = np.expand_dims(boxes_tensor, axis=0)\n",
    "        boxes_tensor = K.tile(K.constant(boxes_tensor, dtype='float32'), (K.shape(x)[0], 1, 1, 1, 1))\n",
    "\n",
    "        return boxes_tensor\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            batch_size, feature_map_height, feature_map_width, feature_map_channels = input_shape\n",
    "        else: # Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\n",
    "            batch_size, feature_map_channels, feature_map_height, feature_map_width = input_shape\n",
    "        return (batch_size, feature_map_height, feature_map_width, self.n_boxes, 8)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'img_height': self.img_height,\n",
    "            'img_width': self.img_width,\n",
    "            'this_scale': self.this_scale,\n",
    "            'next_scale': self.next_scale,\n",
    "            'aspect_ratios': list(self.aspect_ratios),\n",
    "            'two_boxes_for_ar1': self.two_boxes_for_ar1,\n",
    "            'clip_boxes': self.clip_boxes,\n",
    "            'variances': list(self.variances),\n",
    "            'coords': self.coords,\n",
    "            'normalize_coords': self.normalize_coords\n",
    "        }\n",
    "        base_config = super(AnchorBoxes, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SSDLoss:\n",
    "    '''\n",
    "    The SSD loss, see https://arxiv.org/abs/1512.02325.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neg_pos_ratio=3,\n",
    "                 n_neg_min=0,\n",
    "                 alpha=1.0):\n",
    "        '''\n",
    "        Arguments:\n",
    "            neg_pos_ratio (int, optional): The maximum ratio of negative (i.e. background)\n",
    "                to positive ground truth boxes to include in the loss computation.\n",
    "                There are no actual background ground truth boxes of course, but `y_true`\n",
    "                contains anchor boxes labeled with the background class. Since\n",
    "                the number of background boxes in `y_true` will usually exceed\n",
    "                the number of positive boxes by far, it is necessary to balance\n",
    "                their influence on the loss. Defaults to 3 following the paper.\n",
    "            n_neg_min (int, optional): The minimum number of negative ground truth boxes to\n",
    "                enter the loss computation *per batch*. This argument can be used to make\n",
    "                sure that the model learns from a minimum number of negatives in batches\n",
    "                in which there are very few, or even none at all, positive ground truth\n",
    "                boxes. It defaults to 0 and if used, it should be set to a value that\n",
    "                stands in reasonable proportion to the batch size used for training.\n",
    "            alpha (float, optional): A factor to weight the localization loss in the\n",
    "                computation of the total loss. Defaults to 1.0 following the paper.\n",
    "        '''\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.n_neg_min = n_neg_min\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def smooth_L1_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute smooth L1 loss, see references.\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape `(batch_size, #boxes, 4)` and\n",
    "                contains the ground truth bounding box coordinates, where the last dimension\n",
    "                contains `(xmin, xmax, ymin, ymax)`.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box coordinates.\n",
    "        Returns:\n",
    "            The smooth L1 loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "        References:\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        '''\n",
    "        absolute_loss = tf.abs(y_true - y_pred)\n",
    "        square_loss = 0.5 * (y_true - y_pred)**2\n",
    "        l1_loss = tf.where(tf.less(absolute_loss, 1.0), square_loss, absolute_loss - 0.5)\n",
    "        return tf.reduce_sum(l1_loss, axis=-1)\n",
    "\n",
    "    def log_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the softmax log loss.\n",
    "        Arguments:\n",
    "            y_true (nD tensor): A TensorFlow tensor of any shape containing the ground truth data.\n",
    "                In this context, the expected tensor has shape (batch_size, #boxes, #classes)\n",
    "                and contains the ground truth bounding box categories.\n",
    "            y_pred (nD tensor): A TensorFlow tensor of identical structure to `y_true` containing\n",
    "                the predicted data, in this context the predicted bounding box categories.\n",
    "        Returns:\n",
    "            The softmax log loss, a nD-1 Tensorflow tensor. In this context a 2D tensor\n",
    "            of shape (batch, n_boxes_total).\n",
    "        '''\n",
    "        # Make sure that `y_pred` doesn't contain any zeros (which would break the log function)\n",
    "        y_pred = tf.maximum(y_pred, 1e-15)\n",
    "        # Compute the log loss\n",
    "        log_loss = -tf.reduce_sum(y_true * tf.log(y_pred), axis=-1)\n",
    "        return log_loss\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        '''\n",
    "        Compute the loss of the SSD model prediction against the ground truth.\n",
    "        Arguments:\n",
    "            y_true (array): A Numpy array of shape `(batch_size, #boxes, #classes + 12)`,\n",
    "                where `#boxes` is the total number of boxes that the model predicts\n",
    "                per image. Be careful to make sure that the index of each given\n",
    "                box in `y_true` is the same as the index for the corresponding\n",
    "                box in `y_pred`. The last axis must have length `#classes + 12` and contain\n",
    "                `[classes one-hot encoded, 4 ground truth box coordinate offsets, 8 arbitrary entries]`\n",
    "                in this order, including the background class. The last eight entries of the\n",
    "                last axis are not used by this function and therefore their contents are\n",
    "                irrelevant, they only exist so that `y_true` has the same shape as `y_pred`,\n",
    "                where the last four entries of the last axis contain the anchor box\n",
    "                coordinates, which are needed during inference. Important: Boxes that\n",
    "                you want the cost function to ignore need to have a one-hot\n",
    "                class vector of all zeros.\n",
    "            y_pred (Keras tensor): The model prediction. The shape is identical\n",
    "                to that of `y_true`, i.e. `(batch_size, #boxes, #classes + 12)`.\n",
    "                The last axis must contain entries in the format\n",
    "                `[classes one-hot encoded, 4 predicted box coordinate offsets, 8 arbitrary entries]`.\n",
    "        Returns:\n",
    "            A scalar, the total multitask loss for classification and localization.\n",
    "        '''\n",
    "        self.neg_pos_ratio = tf.constant(self.neg_pos_ratio)\n",
    "        self.n_neg_min = tf.constant(self.n_neg_min)\n",
    "        self.alpha = tf.constant(self.alpha)\n",
    "\n",
    "        batch_size = tf.shape(y_pred)[0] # Output dtype: tf.int32\n",
    "        n_boxes = tf.shape(y_pred)[1] # Output dtype: tf.int32, note that `n_boxes` in this context denotes the total number of boxes per image, not the number of boxes per cell.\n",
    "\n",
    "        # 1: Compute the losses for class and box predictions for every box.\n",
    "\n",
    "        classification_loss = tf.to_float(self.log_loss(y_true[:,:,:-12], y_pred[:,:,:-12])) # Output shape: (batch_size, n_boxes)\n",
    "        localization_loss = tf.to_float(self.smooth_L1_loss(y_true[:,:,-12:-8], y_pred[:,:,-12:-8])) # Output shape: (batch_size, n_boxes)\n",
    "\n",
    "        # 2: Compute the classification losses for the positive and negative targets.\n",
    "\n",
    "        # Create masks for the positive and negative ground truth classes.\n",
    "        negatives = y_true[:,:,0] # Tensor of shape (batch_size, n_boxes)\n",
    "        positives = tf.to_float(tf.reduce_max(y_true[:,:,1:-12], axis=-1)) # Tensor of shape (batch_size, n_boxes)\n",
    "\n",
    "        # Count the number of positive boxes (classes 1 to n) in y_true across the whole batch.\n",
    "        n_positive = tf.reduce_sum(positives)\n",
    "\n",
    "        # Now mask all negative boxes and sum up the losses for the positive boxes PER batch item\n",
    "        # (Keras loss functions must output one scalar loss value PER batch item, rather than just\n",
    "        # one scalar for the entire batch, that's why we're not summing across all axes).\n",
    "        pos_class_loss = tf.reduce_sum(classification_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "        # Compute the classification loss for the negative default boxes (if there are any).\n",
    "\n",
    "        # First, compute the classification loss for all negative boxes.\n",
    "        neg_class_loss_all = classification_loss * negatives # Tensor of shape (batch_size, n_boxes)\n",
    "        n_neg_losses = tf.count_nonzero(neg_class_loss_all, dtype=tf.int32) # The number of non-zero loss entries in `neg_class_loss_all`\n",
    "        # What's the point of `n_neg_losses`? For the next step, which will be to compute which negative boxes enter the classification\n",
    "        # loss, we don't just want to know how many negative ground truth boxes there are, but for how many of those there actually is\n",
    "        # a positive (i.e. non-zero) loss. This is necessary because `tf.nn.top-k()` in the function below will pick the top k boxes with\n",
    "        # the highest losses no matter what, even if it receives a vector where all losses are zero. In the unlikely event that all negative\n",
    "        # classification losses ARE actually zero though, this behavior might lead to `tf.nn.top-k()` returning the indices of positive\n",
    "        # boxes, leading to an incorrect negative classification loss computation, and hence an incorrect overall loss computation.\n",
    "        # We therefore need to make sure that `n_negative_keep`, which assumes the role of the `k` argument in `tf.nn.top-k()`,\n",
    "        # is at most the number of negative boxes for which there is a positive classification loss.\n",
    "\n",
    "        # Compute the number of negative examples we want to account for in the loss.\n",
    "        # We'll keep at most `self.neg_pos_ratio` times the number of positives in `y_true`, but at least `self.n_neg_min` (unless `n_neg_loses` is smaller).\n",
    "        n_negative_keep = tf.minimum(tf.maximum(self.neg_pos_ratio * tf.to_int32(n_positive), self.n_neg_min), n_neg_losses)\n",
    "\n",
    "        # In the unlikely case when either (1) there are no negative ground truth boxes at all\n",
    "        # or (2) the classification loss for all negative boxes is zero, return zero as the `neg_class_loss`.\n",
    "        def f1():\n",
    "            return tf.zeros([batch_size])\n",
    "        # Otherwise compute the negative loss.\n",
    "        def f2():\n",
    "            # Now we'll identify the top-k (where k == `n_negative_keep`) boxes with the highest confidence loss that\n",
    "            # belong to the background class in the ground truth data. Note that this doesn't necessarily mean that the model\n",
    "            # predicted the wrong class for those boxes, it just means that the loss for those boxes is the highest.\n",
    "\n",
    "            # To do this, we reshape `neg_class_loss_all` to 1D...\n",
    "            neg_class_loss_all_1D = tf.reshape(neg_class_loss_all, [-1]) # Tensor of shape (batch_size * n_boxes,)\n",
    "            # ...and then we get the indices for the `n_negative_keep` boxes with the highest loss out of those...\n",
    "            values, indices = tf.nn.top_k(neg_class_loss_all_1D,\n",
    "                                          k=n_negative_keep,\n",
    "                                          sorted=False) # We don't need them sorted.\n",
    "            # ...and with these indices we'll create a mask...\n",
    "            negatives_keep = tf.scatter_nd(indices=tf.expand_dims(indices, axis=1),\n",
    "                                           updates=tf.ones_like(indices, dtype=tf.int32),\n",
    "                                           shape=tf.shape(neg_class_loss_all_1D)) # Tensor of shape (batch_size * n_boxes,)\n",
    "            negatives_keep = tf.to_float(tf.reshape(negatives_keep, [batch_size, n_boxes])) # Tensor of shape (batch_size, n_boxes)\n",
    "            # ...and use it to keep only those boxes and mask all other classification losses\n",
    "            neg_class_loss = tf.reduce_sum(classification_loss * negatives_keep, axis=-1) # Tensor of shape (batch_size,)\n",
    "            return neg_class_loss\n",
    "\n",
    "        neg_class_loss = tf.cond(tf.equal(n_neg_losses, tf.constant(0)), f1, f2)\n",
    "\n",
    "        class_loss = pos_class_loss + neg_class_loss # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 3: Compute the localization loss for the positive targets.\n",
    "        #    We don't compute a localization loss for negative predicted boxes (obviously: there are no ground truth boxes they would correspond to).\n",
    "\n",
    "        loc_loss = tf.reduce_sum(localization_loss * positives, axis=-1) # Tensor of shape (batch_size,)\n",
    "\n",
    "        # 4: Compute the total loss.\n",
    "\n",
    "        total_loss = (class_loss + self.alpha * loc_loss) / tf.maximum(1.0, n_positive) # In case `n_positive == 0`\n",
    "        # Keras has the annoying habit of dividing the loss by the batch size, which sucks in our case\n",
    "        # because the relevant criterion to average our loss over is the number of positive boxes in the batch\n",
    "        # (by which we're dividing in the line above), not the batch size. So in order to revert Keras' averaging\n",
    "        # over the batch size, we'll have to multiply by it.\n",
    "        total_loss = total_loss * tf.to_float(batch_size)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "def build_model(image_size,\n",
    "                n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0,\n",
    "                min_scale=0.1,\n",
    "                max_scale=0.9,\n",
    "                scales=None,\n",
    "                aspect_ratios_global=[0.5, 1.0, 2.0],\n",
    "                aspect_ratios_per_layer=None,\n",
    "                two_boxes_for_ar1=True,\n",
    "                steps=None,\n",
    "                offsets=None,\n",
    "                clip_boxes=False,\n",
    "                variances=[1.0, 1.0, 1.0, 1.0],\n",
    "                coords='centroids',\n",
    "                normalize_coords=False,\n",
    "                subtract_mean=None,\n",
    "                divide_by_stddev=None,\n",
    "                swap_channels=False,\n",
    "                confidence_thresh=0.01,\n",
    "                iou_threshold=0.45,\n",
    "                top_k=200,\n",
    "                nms_max_output_size=400,\n",
    "                return_predictor_sizes=False):\n",
    "    '''\n",
    "    Build a Keras model with SSD architecture, see references.\n",
    "\n",
    "    The model consists of convolutional feature layers and a number of convolutional\n",
    "    predictor layers that take their input from different feature layers.\n",
    "    The model is fully convolutional.\n",
    "\n",
    "    The implementation found here is a smaller version of the original architecture\n",
    "    used in the paper (where the base network consists of a modified VGG-16 extended\n",
    "    by a few convolutional feature layers), but of course it could easily be changed to\n",
    "    an arbitrarily large SSD architecture by following the general design pattern used here.\n",
    "    This implementation has 7 convolutional layers and 4 convolutional predictor\n",
    "    layers that take their input from layers 4, 5, 6, and 7, respectively.\n",
    "\n",
    "    Most of the arguments that this function takes are only needed for the anchor\n",
    "    box layers. In case you're training the network, the parameters passed here must\n",
    "    be the same as the ones used to set up `SSDBoxEncoder`. In case you're loading\n",
    "    trained weights, the parameters passed here must be the same as the ones used\n",
    "    to produce the trained weights.\n",
    "\n",
    "    Some of these arguments are explained in more detail in the documentation of the\n",
    "    `SSDBoxEncoder` class.\n",
    "\n",
    "    Note: Requires Keras v2.0 or later. Training currently works only with the\n",
    "    TensorFlow backend (v1.0 or later).\n",
    "\n",
    "    Arguments:\n",
    "        image_size (tuple): The input image size in the format `(height, width, channels)`.\n",
    "        n_classes (int): The number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO.\n",
    "        mode (str, optional): One of 'training', 'inference' and 'inference_fast'. In 'training' mode,\n",
    "            the model outputs the raw prediction tensor, while in 'inference' and 'inference_fast' modes,\n",
    "            the raw predictions are decoded into absolute coordinates and filtered via confidence thresholding,\n",
    "            non-maximum suppression, and top-k filtering. The difference between latter two modes is that\n",
    "            'inference' follows the exact procedure of the original Caffe implementation, while\n",
    "            'inference_fast' uses a faster prediction decoding procedure.\n",
    "        l2_regularization (float, optional): The L2-regularization rate. Applies to all convolutional layers.\n",
    "        min_scale (float, optional): The smallest scaling factor for the size of the anchor boxes as a fraction\n",
    "            of the shorter side of the input images.\n",
    "        max_scale (float, optional): The largest scaling factor for the size of the anchor boxes as a fraction\n",
    "            of the shorter side of the input images. All scaling factors between the smallest and the\n",
    "            largest will be linearly interpolated. Note that the second to last of the linearly interpolated\n",
    "            scaling factors will actually be the scaling factor for the last predictor layer, while the last\n",
    "            scaling factor is used for the second box for aspect ratio 1 in the last predictor layer\n",
    "            if `two_boxes_for_ar1` is `True`.\n",
    "        scales (list, optional): A list of floats containing scaling factors per convolutional predictor layer.\n",
    "            This list must be one element longer than the number of predictor layers. The first `k` elements are the\n",
    "            scaling factors for the `k` predictor layers, while the last element is used for the second box\n",
    "            for aspect ratio 1 in the last predictor layer if `two_boxes_for_ar1` is `True`. This additional\n",
    "            last scaling factor must be passed either way, even if it is not being used. If a list is passed,\n",
    "            this argument overrides `min_scale` and `max_scale`. All scaling factors must be greater than zero.\n",
    "        aspect_ratios_global (list, optional): The list of aspect ratios for which anchor boxes are to be\n",
    "            generated. This list is valid for all predictor layers. The original implementation uses more aspect ratios\n",
    "            for some predictor layers and fewer for others. If you want to do that, too, then use the next argument instead.\n",
    "        aspect_ratios_per_layer (list, optional): A list containing one aspect ratio list for each predictor layer.\n",
    "            This allows you to set the aspect ratios for each predictor layer individually. If a list is passed,\n",
    "            it overrides `aspect_ratios_global`.\n",
    "        two_boxes_for_ar1 (bool, optional): Only relevant for aspect ratio lists that contain 1. Will be ignored otherwise.\n",
    "            If `True`, two anchor boxes will be generated for aspect ratio 1. The first will be generated\n",
    "            using the scaling factor for the respective layer, the second one will be generated using\n",
    "            geometric mean of said scaling factor and next bigger scaling factor.\n",
    "        steps (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
    "            either ints/floats or tuples of two ints/floats. These numbers represent for each predictor layer how many\n",
    "            pixels apart the anchor box center points should be vertically and horizontally along the spatial grid over\n",
    "            the image. If the list contains ints/floats, then that value will be used for both spatial dimensions.\n",
    "            If the list contains tuples of two ints/floats, then they represent `(step_height, step_width)`.\n",
    "            If no steps are provided, then they will be computed such that the anchor box center points will form an\n",
    "            equidistant grid within the image dimensions.\n",
    "        offsets (list, optional): `None` or a list with as many elements as there are predictor layers. The elements can be\n",
    "            either floats or tuples of two floats. These numbers represent for each predictor layer how many\n",
    "            pixels from the top and left boarders of the image the top-most and left-most anchor box center points should be\n",
    "            as a fraction of `steps`. The last bit is important: The offsets are not absolute pixel values, but fractions\n",
    "            of the step size specified in the `steps` argument. If the list contains floats, then that value will\n",
    "            be used for both spatial dimensions. If the list contains tuples of two floats, then they represent\n",
    "            `(vertical_offset, horizontal_offset)`. If no offsets are provided, then they will default to 0.5 of the step size,\n",
    "            which is also the recommended setting.\n",
    "        clip_boxes (bool, optional): If `True`, clips the anchor box coordinates to stay within image boundaries.\n",
    "        variances (list, optional): A list of 4 floats >0. The anchor box offset for each coordinate will be divided by\n",
    "            its respective variance value.\n",
    "        coords (str, optional): The box coordinate format to be used internally by the model (i.e. this is not the input format\n",
    "            of the ground truth labels). Can be either 'centroids' for the format `(cx, cy, w, h)` (box center coordinates, width,\n",
    "            and height), 'minmax' for the format `(xmin, xmax, ymin, ymax)`, or 'corners' for the format `(xmin, ymin, xmax, ymax)`.\n",
    "        normalize_coords (bool, optional): Set to `True` if the model is supposed to use relative instead of absolute coordinates,\n",
    "            i.e. if the model predicts box coordinates within [0,1] instead of absolute coordinates.\n",
    "        subtract_mean (array-like, optional): `None` or an array-like object of integers or floating point values\n",
    "            of any shape that is broadcast-compatible with the image shape. The elements of this array will be\n",
    "            subtracted from the image pixel intensity values. For example, pass a list of three integers\n",
    "            to perform per-channel mean normalization for color images.\n",
    "        divide_by_stddev (array-like, optional): `None` or an array-like object of non-zero integers or\n",
    "            floating point values of any shape that is broadcast-compatible with the image shape. The image pixel\n",
    "            intensity values will be divided by the elements of this array. For example, pass a list\n",
    "            of three integers to perform per-channel standard deviation normalization for color images.\n",
    "        swap_channels (list, optional): Either `False` or a list of integers representing the desired order in which the input\n",
    "            image channels should be swapped.\n",
    "        confidence_thresh (float, optional): A float in [0,1), the minimum classification confidence in a specific\n",
    "            positive class in order to be considered for the non-maximum suppression stage for the respective class.\n",
    "            A lower value will result in a larger part of the selection process being done by the non-maximum suppression\n",
    "            stage, while a larger value will result in a larger part of the selection process happening in the confidence\n",
    "            thresholding stage.\n",
    "        iou_threshold (float, optional): A float in [0,1]. All boxes that have a Jaccard similarity of greater than `iou_threshold`\n",
    "            with a locally maximal box will be removed from the set of predictions for a given class, where 'maximal' refers\n",
    "            to the box's confidence score.\n",
    "        top_k (int, optional): The number of highest scoring predictions to be kept for each batch item after the\n",
    "            non-maximum suppression stage.\n",
    "        nms_max_output_size (int, optional): The maximal number of predictions that will be left over after the NMS stage.\n",
    "        return_predictor_sizes (bool, optional): If `True`, this function not only returns the model, but also\n",
    "            a list containing the spatial dimensions of the predictor layers. This isn't strictly necessary since\n",
    "            you can always get their sizes easily via the Keras API, but it's convenient and less error-prone\n",
    "            to get them this way. They are only relevant for training anyway (SSDBoxEncoder needs to know the\n",
    "            spatial dimensions of the predictor layers), for inference you don't need them.\n",
    "\n",
    "    Returns:\n",
    "        model: The Keras SSD model.\n",
    "        predictor_sizes (optional): A Numpy array containing the `(height, width)` portion\n",
    "            of the output tensor shape for each convolutional predictor layer. During\n",
    "            training, the generator function needs this in order to transform\n",
    "            the ground truth labels into tensors of identical structure as the\n",
    "            output tensors of the model, which is in turn needed for the cost\n",
    "            function.\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/abs/1512.02325v5\n",
    "    '''\n",
    "\n",
    "    n_predictor_layers = 4 # The number of predictor conv layers in the network\n",
    "    n_classes += 1 # Account for the background class.\n",
    "    l2_reg = l2_regularization # Make the internal name shorter.\n",
    "    img_height, img_width, img_channels = image_size[0], image_size[1], image_size[2]\n",
    "\n",
    "    ############################################################################\n",
    "    # Get a few exceptions out of the way.\n",
    "    ############################################################################\n",
    "\n",
    "    if aspect_ratios_global is None and aspect_ratios_per_layer is None:\n",
    "        raise ValueError(\"`aspect_ratios_global` and `aspect_ratios_per_layer` cannot both be None. At least one needs to be specified.\")\n",
    "    if aspect_ratios_per_layer:\n",
    "        if len(aspect_ratios_per_layer) != n_predictor_layers:\n",
    "            raise ValueError(\"It must be either aspect_ratios_per_layer is None or len(aspect_ratios_per_layer) == {}, but len(aspect_ratios_per_layer) == {}.\".format(n_predictor_layers, len(aspect_ratios_per_layer)))\n",
    "\n",
    "    if (min_scale is None or max_scale is None) and scales is None:\n",
    "        raise ValueError(\"Either `min_scale` and `max_scale` or `scales` need to be specified.\")\n",
    "    if scales:\n",
    "        if len(scales) != n_predictor_layers+1:\n",
    "            raise ValueError(\"It must be either scales is None or len(scales) == {}, but len(scales) == {}.\".format(n_predictor_layers+1, len(scales)))\n",
    "    else: # If no explicit list of scaling factors was passed, compute the list of scaling factors from `min_scale` and `max_scale`\n",
    "        scales = np.linspace(min_scale, max_scale, n_predictor_layers + 1)\n",
    "\n",
    "    if len(variances) != 4: # We need one variance value for each of the four box coordinates\n",
    "        raise ValueError(\"4 variance values must be pased, but {} values were received.\".format(len(variances)))\n",
    "    variances = np.array(variances)\n",
    "    if np.any(variances <= 0):\n",
    "        raise ValueError(\"All variances must be >0, but the variances given are {}\".format(variances))\n",
    "\n",
    "    if (not (steps is None)) and (len(steps) != n_predictor_layers):\n",
    "        raise ValueError(\"You must provide at least one step value per predictor layer.\")\n",
    "\n",
    "    if (not (offsets is None)) and (len(offsets) != n_predictor_layers):\n",
    "        raise ValueError(\"You must provide at least one offset value per predictor layer.\")\n",
    "\n",
    "    ############################################################################\n",
    "    # Compute the anchor box parameters.\n",
    "    ############################################################################\n",
    "\n",
    "    # Set the aspect ratios for each predictor layer. These are only needed for the anchor box layers.\n",
    "    if aspect_ratios_per_layer:\n",
    "        aspect_ratios = aspect_ratios_per_layer\n",
    "    else:\n",
    "        aspect_ratios = [aspect_ratios_global] * n_predictor_layers\n",
    "\n",
    "    # Compute the number of boxes to be predicted per cell for each predictor layer.\n",
    "    # We need this so that we know how many channels the predictor layers need to have.\n",
    "    if aspect_ratios_per_layer:\n",
    "        n_boxes = []\n",
    "        for ar in aspect_ratios_per_layer:\n",
    "            if (1 in ar) & two_boxes_for_ar1:\n",
    "                n_boxes.append(len(ar) + 1) # +1 for the second box for aspect ratio 1\n",
    "            else:\n",
    "                n_boxes.append(len(ar))\n",
    "    else: # If only a global aspect ratio list was passed, then the number of boxes is the same for each predictor layer\n",
    "        if (1 in aspect_ratios_global) & two_boxes_for_ar1:\n",
    "            n_boxes = len(aspect_ratios_global) + 1\n",
    "        else:\n",
    "            n_boxes = len(aspect_ratios_global)\n",
    "        n_boxes = [n_boxes] * n_predictor_layers\n",
    "\n",
    "    if steps is None:\n",
    "        steps = [None] * n_predictor_layers\n",
    "    if offsets is None:\n",
    "        offsets = [None] * n_predictor_layers\n",
    "\n",
    "    ############################################################################\n",
    "    # Define functions for the Lambda layers below.\n",
    "    ############################################################################\n",
    "\n",
    "    def identity_layer(tensor):\n",
    "        return tensor\n",
    "\n",
    "    def input_mean_normalization(tensor):\n",
    "        return tensor - np.array(subtract_mean)\n",
    "\n",
    "    def input_stddev_normalization(tensor):\n",
    "        return tensor / np.array(divide_by_stddev)\n",
    "\n",
    "    def input_channel_swap(tensor):\n",
    "        if len(swap_channels) == 3:\n",
    "            return K.stack([tensor[..., swap_channels[0]], \n",
    "                            tensor[..., swap_channels[1]], \n",
    "                            tensor[..., swap_channels[2]]], \n",
    "                           axis=-1)\n",
    "        elif len(swap_channels) == 4:\n",
    "            return K.stack([tensor[..., swap_channels[0]], \n",
    "                            tensor[..., swap_channels[1]], \n",
    "                            tensor[..., swap_channels[2]], \n",
    "                            tensor[..., swap_channels[3]]], \n",
    "                           axis=-1)\n",
    "\n",
    "    ############################################################################\n",
    "    # Build the network.\n",
    "    ############################################################################\n",
    "\n",
    "    x = layers.Input(shape=(img_height, img_width, img_channels))\n",
    "\n",
    "    # The following identity layer is only needed so that the subsequent lambda layers can be optional.\n",
    "    x1 = layers.Lambda(identity_layer, output_shape=(img_height, img_width, img_channels), name='identity_layer')(x)\n",
    "    if not (subtract_mean is None):\n",
    "        x1 = layers.Lambda(input_mean_normalization, output_shape=(img_height, img_width, img_channels), name='input_mean_normalization')(x1)\n",
    "    if not (divide_by_stddev is None):\n",
    "        x1 = layers.Lambda(input_stddev_normalization, output_shape=(img_height, img_width, img_channels), name='input_stddev_normalization')(x1)\n",
    "    if swap_channels:\n",
    "        x1 = layers.Lambda(input_channel_swap, output_shape=(img_height, img_width, img_channels), name='input_channel_swap')(x1)\n",
    "\n",
    "    conv1 = layers.Conv2D(32, (5, 5), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv1')(x1)\n",
    "    conv1 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn1')(conv1) # Tensorflow uses filter format [filter_height, filter_width, in_channels, out_channels], hence axis = 3\n",
    "    conv1 = layers.ELU(name='elu1')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2), name='pool1')(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv2')(pool1)\n",
    "    conv2 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn2')(conv2)\n",
    "    conv2 = layers.ELU(name='elu2')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2), name='pool2')(conv2)\n",
    "\n",
    "    conv3 = layers.Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv3')(pool2)\n",
    "    conv3 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn3')(conv3)\n",
    "    conv3 = layers.ELU(name='elu3')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2), name='pool3')(conv3)\n",
    "\n",
    "    conv4 = layers.Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv4')(pool3)\n",
    "    conv4 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn4')(conv4)\n",
    "    conv4 = layers.ELU(name='elu4')(conv4)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2), name='pool4')(conv4)\n",
    "\n",
    "    conv5 = layers.Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv5')(pool4)\n",
    "    conv5 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn5')(conv5)\n",
    "    conv5 = layers.ELU(name='elu5')(conv5)\n",
    "    pool5 = layers.MaxPooling2D(pool_size=(2, 2), name='pool5')(conv5)\n",
    "\n",
    "    conv6 = layers.Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv6')(pool5)\n",
    "    conv6 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn6')(conv6)\n",
    "    conv6 = layers.ELU(name='elu6')(conv6)\n",
    "    pool6 = layers.MaxPooling2D(pool_size=(2, 2), name='pool6')(conv6)\n",
    "\n",
    "    conv7 = layers.Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\", kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), name='conv7')(pool6)\n",
    "    conv7 = layers.BatchNormalization(axis=3, momentum=0.99, name='bn7')(conv7)\n",
    "    conv7 = layers.ELU(name='elu7')(conv7)\n",
    "\n",
    "    # The next part is to add the convolutional predictor layers on top of the base network\n",
    "    # that we defined above. Note that I use the term \"base network\" differently than the paper does.\n",
    "    # To me, the base network is everything that is not convolutional predictor layers or anchor\n",
    "    # box layers. In this case we'll have four predictor layers, but of course you could\n",
    "    # easily rewrite this into an arbitrarily deep base network and add an arbitrary number of\n",
    "    # predictor layers on top of the base network by simply following the pattern shown here.\n",
    "\n",
    "    # Build the convolutional predictor layers on top of conv layers 4, 5, 6, and 7.\n",
    "    # We build two predictor layers on top of each of these layers: One for class prediction (classification), one for box coordinate prediction (localization)\n",
    "    # We precidt `n_classes` confidence values for each box, hence the `classes` predictors have depth `n_boxes * n_classes`\n",
    "    # We predict 4 box coordinates for each box, hence the `boxes` predictors have depth `n_boxes * 4`\n",
    "    # Output shape of `classes`: `(batch, height, width, n_boxes * n_classes)`\n",
    "    classes4 = layers.Conv2D(n_boxes[0] * n_classes, \n",
    "                             (3, 3), \n",
    "                             strides=(1, 1), \n",
    "                             padding='same', \n",
    "                             kernel_initializer='he_normal', \n",
    "                             kernel_regularizer=l2(l2_reg), \n",
    "                             name='classes4')(conv4)\n",
    "    classes5 = layers.Conv2D(n_boxes[1] * n_classes, \n",
    "                             (3, 3), \n",
    "                             strides=(1, 1), \n",
    "                             padding='same', \n",
    "                             kernel_initializer='he_normal', \n",
    "                             kernel_regularizer=l2(l2_reg), \n",
    "                             name='classes5')(conv5)\n",
    "    classes6 = layers.Conv2D(n_boxes[2] * n_classes, \n",
    "                             (3, 3), \n",
    "                             strides=(1, 1), \n",
    "                             padding='same', \n",
    "                             kernel_initializer='he_normal', \n",
    "                             kernel_regularizer=l2(l2_reg), \n",
    "                             name='classes6')(conv6)\n",
    "    classes7 = layers.Conv2D(n_boxes[3] * n_classes, \n",
    "                             (3, 3), \n",
    "                             strides=(1, 1), \n",
    "                             padding='same', \n",
    "                             kernel_initializer='he_normal', \n",
    "                             kernel_regularizer=l2(l2_reg), \n",
    "                             name='classes7')(conv7)\n",
    "    # Output shape of `boxes`: `(batch, height, width, n_boxes * 4)`\n",
    "    boxes4 = layers.Conv2D(n_boxes[0] * 4, \n",
    "                           (3, 3), \n",
    "                           strides=(1, 1), \n",
    "                           padding='same', \n",
    "                           kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), \n",
    "                           name='boxes4')(conv4)\n",
    "    boxes5 = layers.Conv2D(n_boxes[1] * 4, \n",
    "                           (3, 3), \n",
    "                           strides=(1, 1), \n",
    "                           padding='same', \n",
    "                           kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), \n",
    "                           name='boxes5')(conv5)\n",
    "    boxes6 = layers.Conv2D(n_boxes[2] * 4, \n",
    "                           (3, 3), \n",
    "                           strides=(1, 1), \n",
    "                           padding='same', \n",
    "                           kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), \n",
    "                           name='boxes6')(conv6)\n",
    "    boxes7 = layers.Conv2D(n_boxes[3] * 4, \n",
    "                           (3, 3), \n",
    "                           strides=(1, 1), \n",
    "                           padding='same', \n",
    "                           kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=l2(l2_reg), \n",
    "                           name='boxes7')(conv7)\n",
    "\n",
    "    # Generate the anchor boxes\n",
    "    # Output shape of `anchors`: `(batch, height, width, n_boxes, 8)`\n",
    "    anchors4 = AnchorBoxes(img_height, \n",
    "                           img_width, \n",
    "                           this_scale=scales[0], \n",
    "                           next_scale=scales[1], \n",
    "                           aspect_ratios=aspect_ratios[0],\n",
    "                           two_boxes_for_ar1=two_boxes_for_ar1, \n",
    "                           this_steps=steps[0], \n",
    "                           this_offsets=offsets[0],\n",
    "                           clip_boxes=clip_boxes, \n",
    "                           variances=variances, \n",
    "                           coords=coords, \n",
    "                           normalize_coords=normalize_coords, \n",
    "                           name='anchors4')(boxes4)\n",
    "    anchors5 = AnchorBoxes(img_height, \n",
    "                           img_width, \n",
    "                           this_scale=scales[1], \n",
    "                           next_scale=scales[2], \n",
    "                           aspect_ratios=aspect_ratios[1],\n",
    "                           two_boxes_for_ar1=two_boxes_for_ar1, \n",
    "                           this_steps=steps[1], \n",
    "                           this_offsets=offsets[1],\n",
    "                           clip_boxes=clip_boxes, \n",
    "                           variances=variances, \n",
    "                           coords=coords, \n",
    "                           normalize_coords=normalize_coords, \n",
    "                           name='anchors5')(boxes5)\n",
    "    anchors6 = AnchorBoxes(img_height, \n",
    "                           img_width, \n",
    "                           this_scale=scales[2], \n",
    "                           next_scale=scales[3], \n",
    "                           aspect_ratios=aspect_ratios[2],\n",
    "                           two_boxes_for_ar1=two_boxes_for_ar1, \n",
    "                           this_steps=steps[2], \n",
    "                           this_offsets=offsets[2],\n",
    "                           clip_boxes=clip_boxes, \n",
    "                           variances=variances, \n",
    "                           coords=coords, \n",
    "                           normalize_coords=normalize_coords, \n",
    "                           name='anchors6')(boxes6)\n",
    "    anchors7 = AnchorBoxes(img_height, \n",
    "                           img_width, \n",
    "                           this_scale=scales[3], \n",
    "                           next_scale=scales[4], \n",
    "                           aspect_ratios=aspect_ratios[3],\n",
    "                           two_boxes_for_ar1=two_boxes_for_ar1, \n",
    "                           this_steps=steps[3], \n",
    "                           this_offsets=offsets[3],\n",
    "                           clip_boxes=clip_boxes, \n",
    "                           variances=variances, \n",
    "                           coords=coords, \n",
    "                           normalize_coords=normalize_coords, \n",
    "                           name='anchors7')(boxes7)\n",
    "\n",
    "    # Reshape the class predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, n_classes)`\n",
    "    # We want the classes isolated in the last axis to perform softmax on them\n",
    "    classes4_reshaped = layers.Reshape((-1, n_classes), name='classes4_reshape')(classes4)\n",
    "    classes5_reshaped = layers.Reshape((-1, n_classes), name='classes5_reshape')(classes5)\n",
    "    classes6_reshaped = layers.Reshape((-1, n_classes), name='classes6_reshape')(classes6)\n",
    "    classes7_reshaped = layers.Reshape((-1, n_classes), name='classes7_reshape')(classes7)\n",
    "    # Reshape the box coordinate predictions, yielding 3D tensors of shape `(batch, height * width * n_boxes, 4)`\n",
    "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
    "    boxes4_reshaped = layers.Reshape((-1, 4), name='boxes4_reshape')(boxes4)\n",
    "    boxes5_reshaped = layers.Reshape((-1, 4), name='boxes5_reshape')(boxes5)\n",
    "    boxes6_reshaped = layers.Reshape((-1, 4), name='boxes6_reshape')(boxes6)\n",
    "    boxes7_reshaped = layers.Reshape((-1, 4), name='boxes7_reshape')(boxes7)\n",
    "    # Reshape the anchor box tensors, yielding 3D tensors of shape `(batch, height * width * n_boxes, 8)`\n",
    "    anchors4_reshaped = layers.Reshape((-1, 8), name='anchors4_reshape')(anchors4)\n",
    "    anchors5_reshaped = layers.Reshape((-1, 8), name='anchors5_reshape')(anchors5)\n",
    "    anchors6_reshaped = layers.Reshape((-1, 8), name='anchors6_reshape')(anchors6)\n",
    "    anchors7_reshaped = layers.Reshape((-1, 8), name='anchors7_reshape')(anchors7)\n",
    "\n",
    "    # Concatenate the predictions from the different layers and the assosciated anchor box tensors\n",
    "    # Axis 0 (batch) and axis 2 (n_classes or 4, respectively) are identical for all layer predictions,\n",
    "    # so we want to concatenate along axis 1\n",
    "    # Output shape of `classes_concat`: (batch, n_boxes_total, n_classes)\n",
    "    classes_concat = layers.Concatenate(axis=1, name='classes_concat')([classes4_reshaped, \n",
    "                                                                        classes5_reshaped, \n",
    "                                                                        classes6_reshaped, \n",
    "                                                                        classes7_reshaped])\n",
    "\n",
    "    # Output shape of `boxes_concat`: (batch, n_boxes_total, 4)\n",
    "    boxes_concat = layers.Concatenate(axis=1, name='boxes_concat')([boxes4_reshaped, \n",
    "                                                                    boxes5_reshaped, \n",
    "                                                                    boxes6_reshaped, \n",
    "                                                                    boxes7_reshaped])\n",
    "\n",
    "    # Output shape of `anchors_concat`: (batch, n_boxes_total, 8)\n",
    "    anchors_concat = layers.Concatenate(axis=1, name='anchors_concat')([anchors4_reshaped, \n",
    "                                                                        anchors5_reshaped, \n",
    "                                                                        anchors6_reshaped, \n",
    "                                                                        anchors7_reshaped])\n",
    "\n",
    "    # The box coordinate predictions will go into the loss function just the way they are,\n",
    "    # but for the class predictions, we'll apply a softmax activation layer first\n",
    "    classes_softmax = layers.Activation('softmax', name='classes_softmax')(classes_concat)\n",
    "\n",
    "    # Concatenate the class and box coordinate predictions and the anchors to one large predictions tensor\n",
    "    # Output shape of `predictions`: (batch, n_boxes_total, n_classes + 4 + 8)\n",
    "    predictions = layers.Concatenate(axis=2, name='predictions')([classes_softmax, boxes_concat, anchors_concat])\n",
    "\n",
    "    if mode == 'training':\n",
    "        model = Model(inputs=x, outputs=predictions)\n",
    "    elif mode == 'inference':\n",
    "        decoded_predictions = DecodeDetections(confidence_thresh=confidence_thresh,\n",
    "                                               iou_threshold=iou_threshold,\n",
    "                                               top_k=top_k,\n",
    "                                               nms_max_output_size=nms_max_output_size,\n",
    "                                               coords=coords,\n",
    "                                               normalize_coords=normalize_coords,\n",
    "                                               img_height=img_height,\n",
    "                                               img_width=img_width,\n",
    "                                               name='decoded_predictions')(predictions)\n",
    "        model = Model(inputs=x, outputs=decoded_predictions)\n",
    "    elif mode == 'inference_fast':\n",
    "        decoded_predictions = DecodeDetectionsFast(confidence_thresh=confidence_thresh,\n",
    "                                                   iou_threshold=iou_threshold,\n",
    "                                                   top_k=top_k,\n",
    "                                                   nms_max_output_size=nms_max_output_size,\n",
    "                                                   coords=coords,\n",
    "                                                   normalize_coords=normalize_coords,\n",
    "                                                   img_height=img_height,\n",
    "                                                   img_width=img_width,\n",
    "                                                   name='decoded_predictions')(predictions)\n",
    "        model = Model(inputs=x, outputs=decoded_predictions)\n",
    "    else:\n",
    "        raise ValueError(\"`mode` must be one of 'training', 'inference' or 'inference_fast', but received '{}'.\".format(mode))\n",
    "\n",
    "    if return_predictor_sizes:\n",
    "        # The spatial dimensions are the same for the `classes` and `boxes` predictor layers.\n",
    "        predictor_sizes = np.array([classes4._keras_shape[1:3],\n",
    "                                    classes5._keras_shape[1:3],\n",
    "                                    classes6._keras_shape[1:3],\n",
    "                                    classes7._keras_shape[1:3]])\n",
    "        return model, predictor_sizes\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9BlIZiZshi2"
   },
   "source": [
    "Define MobileNetV2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(preds, top_k=5, classes=['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']):\n",
    "    if len(preds.shape) != 2:\n",
    "        raise ValueError('`decode_predictions` expects a batch of predictions '\n",
    "                         '(i.e. a 2D array of shape (samples, classes)). '\n",
    "                         'Found array with shape: ' + str(preds.shape))\n",
    "    results = []\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top_k:][::-1]\n",
    "        result = [tuple(classes[i]) + (pred[i],) for i in top_indices]\n",
    "        result.sort(key=lambda x: x[2], reverse=True)\n",
    "        results.append(result)\n",
    "        return results\n",
    "\n",
    "def prepare_image(img):\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array_expanded_dims = np.expand_dims(img_array, axis=0)\n",
    "    return tf.keras.applications.imagenet_utils.preprocess_input(img_array_expanded_dims, mode='tf')\n",
    "\n",
    "def resize_and_pad_image(image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0):\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32)\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def preprocess_data_detection(sample):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "    image = sample['image']\n",
    "    bbox = swap_xy(sample['objects']['bbox'])\n",
    "    class_id = tf.cast(sample['objects']['label'], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "\n",
    "    bbox = tf.stack([bbox[:, 0] * image_shape[1], \n",
    "                     bbox[:, 1] * image_shape[0], \n",
    "                     bbox[:, 2] * image_shape[1], \n",
    "                     bbox[:, 3] * image_shape[0]], \n",
    "                    axis=-1)\n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    return image, bbox, class_id\n",
    "\n",
    "def resize_normalize(image, label, image_size=default_size):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image = tf.image.resize(image, (image_size, image_size))\n",
    "    return image / 255.0, label\n",
    "\n",
    "def augment(image, label):  \n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return tf.image.random_contrast(image, lower=0.0, upper=1.0), label\n",
    "\n",
    "def correct_pad(inputs, kernel_size):\n",
    "    img_dim = 2 if K.image_data_format() == 'channels_first' else 1\n",
    "    input_size = K.int_shape(inputs)[img_dim:(img_dim + 2)]\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "\n",
    "    if input_size[0] is None:\n",
    "        adjust = (1, 1)\n",
    "    else:\n",
    "        adjust = (1 - input_size[0] % 2, 1 - input_size[1] % 2)\n",
    "\n",
    "    correct = (kernel_size[0] // 2, kernel_size[1] // 2)\n",
    "\n",
    "    return ((correct[0] - adjust[0], correct[0]), (correct[1] - adjust[1], correct[1]))\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "def MobileNetV2(input_shape=None,\n",
    "                alpha=1.0,\n",
    "                include_top=True,\n",
    "                weights='imagenet',\n",
    "                input_tensor=None,\n",
    "                pooling=None,\n",
    "                classes=1000):\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either `None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), or the path to the weights file to be loaded.')\n",
    "\n",
    "    if input_shape is not None and input_tensor is not None:\n",
    "        try:\n",
    "            is_input_t_tensor = K.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                is_input_t_tensor = K.is_keras_tensor(\n",
    "                    keras_utils.get_source_inputs(input_tensor))\n",
    "            except ValueError:\n",
    "                raise ValueError('input_tensor: ', input_tensor,\n",
    "                                 'is not type input_tensor')\n",
    "        if is_input_t_tensor:\n",
    "            if K.image_data_format == 'channels_first':\n",
    "                if K.int_shape(input_tensor)[1] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "            else:\n",
    "                if K.int_shape(input_tensor)[2] != input_shape[1]:\n",
    "                    raise ValueError('input_shape: ', input_shape,\n",
    "                                     'and input_tensor: ', input_tensor,\n",
    "                                     'do not meet the same shape requirements')\n",
    "        else:\n",
    "            raise ValueError('input_tensor specified: ', input_tensor, 'is not a keras tensor')\n",
    "\n",
    "    if input_shape is None and input_tensor is not None:\n",
    "        try:\n",
    "            K.is_keras_tensor(input_tensor)\n",
    "        except ValueError:\n",
    "            raise ValueError('input_tensor: ', input_tensor,\n",
    "                             'is type: ', type(input_tensor),\n",
    "                             'which is not a valid type')\n",
    "\n",
    "        if input_shape is None and K.is_keras_tensor(input_tensor):\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                rows = K.int_shape(input_tensor)[2]\n",
    "                cols = K.int_shape(input_tensor)[3]\n",
    "            else:\n",
    "                rows = K.int_shape(input_tensor)[1]\n",
    "                cols = K.int_shape(input_tensor)[2]\n",
    "\n",
    "            if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "                default_size = rows\n",
    "\n",
    "    else:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            rows = input_shape[0]\n",
    "            cols = input_shape[1]\n",
    "\n",
    "        if rows == cols and rows in [96, 128, 160, 192, 224]:\n",
    "            default_size = rows\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        row_axis, col_axis = (0, 1)\n",
    "    else:\n",
    "        row_axis, col_axis = (1, 2)\n",
    "    rows = input_shape[row_axis]\n",
    "    cols = input_shape[col_axis]\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'alpha can be one of `0.35`, `0.50`, `0.75`, `1.0`, `1.3` or `1.4` only.')\n",
    "\n",
    "        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n",
    "            rows = default_size\n",
    "            warnings.warn('`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224].'\n",
    "                          ' Weights for input shape (%d, %d) will be loaded as the default.' \\\n",
    "                          % (default_size, default_size))\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    first_block_filters = _make_divisible(32 * alpha, 8)\n",
    "    x = layers.ZeroPadding2D(padding=correct_pad(img_input, 3), name='Conv1_pad')(img_input)\n",
    "    x = layers.Conv2D(first_block_filters,\n",
    "                      kernel_size=3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='valid',\n",
    "                      use_bias=False,\n",
    "                      name='Conv1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name='bn_Conv1')(x)\n",
    "    x = layers.ReLU(6., name='Conv1_relu')(x)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=2, expansion=6, block_id=1)\n",
    "    x = _inverted_res_block(x, filters=24, alpha=alpha, stride=1, expansion=6, block_id=2)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=4)\n",
    "    x = _inverted_res_block(x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=5)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=6)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=7)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=8)\n",
    "    x = _inverted_res_block(x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=9)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=10)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=11)\n",
    "    x = _inverted_res_block(x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=12)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=2, expansion=6, block_id=13)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, expansion=6, block_id=14)\n",
    "    x = _inverted_res_block(x, filters=160, alpha=alpha, stride=1, expansion=6, block_id=15)\n",
    "\n",
    "    x = _inverted_res_block(x, filters=320, alpha=alpha, stride=1, expansion=6, block_id=16)\n",
    "\n",
    "    # no alpha applied to last conv as stated in the paper:\n",
    "    # if the width multiplier is greater than 1 we\n",
    "    # increase the number of output channels\n",
    "    if alpha > 1.0:\n",
    "        last_block_filters = _make_divisible(1280 * alpha, 8)\n",
    "    else:\n",
    "        last_block_filters = 1280\n",
    "\n",
    "    x = layers.Conv2D(last_block_filters, kernel_size=1, use_bias=False, name='Conv_1')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis, epsilon=1e-3, momentum=0.999, name='Conv_1_bn')(x)\n",
    "    x = layers.ReLU(6., name='out_relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(classes, activation='softmax', use_bias=True, name='Logits')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = models.Model(inputs, x, name='mobilenetv2_%0.2f_%s' % (alpha, rows))\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
    "                          str(alpha) + '_' + str(rows) + '.h5')\n",
    "            weight_path = base_weight_path + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        else:\n",
    "            model_name = ('mobilenet_v2_weights_tf_dim_ordering_tf_kernels_' +\n",
    "                          str(alpha) + '_' + str(rows) + '_no_top' + '.h5')\n",
    "            weight_path = base_weight_path + model_name\n",
    "            weights_path = keras_utils.get_file(model_name, weight_path, cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    in_channels = K.int_shape(inputs)[channel_axis]\n",
    "    pointwise_conv_filters = int(filters * alpha)\n",
    "    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n",
    "    x = inputs\n",
    "    prefix = 'block_{}_'.format(block_id)\n",
    "\n",
    "    if block_id:\n",
    "        # Expand\n",
    "        x = layers.Conv2D(expansion * in_channels,\n",
    "                          kernel_size=1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          activation=None,\n",
    "                          name=prefix + 'expand')(x)\n",
    "        x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                      epsilon=1e-3,\n",
    "                                      momentum=0.999,\n",
    "                                      name=prefix + 'expand_BN')(x)\n",
    "        x = layers.ReLU(6., name=prefix + 'expand_relu')(x)\n",
    "    else:\n",
    "        prefix = 'expanded_conv_'\n",
    "\n",
    "    # Depthwise\n",
    "    if stride == 2:\n",
    "        x = layers.ZeroPadding2D(padding=correct_pad(x, 3), name=prefix + 'pad')(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size=3,\n",
    "                               strides=stride,\n",
    "                               activation=None,\n",
    "                               use_bias=False,\n",
    "                               padding='same' if stride == 1 else 'valid',\n",
    "                               name=prefix + 'depthwise')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name=prefix + 'depthwise_BN')(x)\n",
    "\n",
    "    x = layers.ReLU(6., name=prefix + 'depthwise_relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(pointwise_filters,\n",
    "                      kernel_size=1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      activation=None,\n",
    "                      name=prefix + 'project')(x)\n",
    "    x = layers.BatchNormalization(axis=channel_axis,\n",
    "                                  epsilon=1e-3,\n",
    "                                  momentum=0.999,\n",
    "                                  name=prefix + 'project_BN')(x)\n",
    "\n",
    "    if in_channels == pointwise_filters and stride == 1:\n",
    "        return layers.Add(name=prefix + 'add')([inputs, x])\n",
    "    return x\n",
    "\n",
    "def training_statistics(history, loss, val_loss):\n",
    "    if training:\n",
    "        plt.plot(history[loss])\n",
    "        plt.plot(history[val_loss])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend([loss, val_loss], loc='upper left')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.getcwd(), 'data.zip')\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "with zipfile.ZipFile('data.zip', 'r') as z_fp:\n",
    "    z_fp.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, val_dataset), dataset_info = \\\n",
    "    tfds.load('coco/2017', split=['train', 'validation'], with_info=True, data_dir=data_dir)\n",
    "\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "train_dataset = train_dataset.map(preprocess_data_detection, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, \n",
    "                                           padding_values=(0.0, 1e-8, -1), \n",
    "                                           drop_remainder=True)\n",
    "\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)\n",
    "\n",
    "val_dataset = val_dataset.map(preprocess_data_detection, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1, \n",
    "                                       padding_values=(0.0, 1e-8, -1), \n",
    "                                       drop_remainder=True)\n",
    "\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "val_dataset = val_dataset.prefetch(autotune)\n",
    "\n",
    "if dataset_type == 'full':\n",
    "    train_steps_per_epoch = dataset_info.splits['train'].num_examples // batch_size\n",
    "    val_steps_per_epoch = \\\n",
    "        dataset_info.splits['validation'].num_examples // batch_size\n",
    "\n",
    "    train_steps = 4 * 100000\n",
    "    epochs = train_steps // train_steps_per_epoch\n",
    "\n",
    "if dataset == 'coco17':\n",
    "    if dataset_type != 'full':\n",
    "        train_dataset = train_dataset.take(100)\n",
    "        val_dataset = val_dataset.take(50)\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs=epochs, \n",
    "          callbacks=callbacks_list, \n",
    "          verbose=1)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "model.load_weights(latest_checkpoint)\n",
    "\n",
    "image = tf.keras.Input(shape=[None, None, 3], name='image')\n",
    "predictions = model(image, training=False)\n",
    "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
    "model = tf.keras.Model(inputs=image, outputs=detections)\n",
    "\n",
    "val_dataset = tfds.load('coco/2017', split='validation', data_dir=data_dir)\n",
    "int2str = dataset_info.features['objects']['label'].int2str\n",
    "\n",
    "for sample in val_dataset.take(2):\n",
    "    image = tf.cast(sample['image'], dtype=tf.float32)\n",
    "    input_image, ratio = prepare_image(image)\n",
    "    detections = model.predict(input_image)\n",
    "    num_detections = detections.valid_detections[0]\n",
    "    class_names = [int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]]\n",
    "    visualize_detections(image, \n",
    "                         detections.nmsed_boxes[0][:num_detections] / ratio, \n",
    "                         class_names, \n",
    "                         detections.nmsed_scores[0][:num_detections])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/media/samuel/DATA2; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cdc0e1529a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m (raw_train, raw_val, raw_test), metadata = tfds.load(name='tf_flowers', \n\u001b[0m\u001b[1;32m      3\u001b[0m                                                      \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                      \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                      \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    298\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mdbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_gcs_data_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     dl_manager = self._make_download_manager(\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         download_config=download_config)\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_make_download_manager\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmanual_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m     return download.DownloadManager(\n\u001b[0m\u001b[1;32m    656\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0mdownload_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/api_utils.py\u001b[0m in \u001b[0;36mdisallow_positional_args_dec\u001b[0;34m(fn, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_check_no_positional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0m_check_required\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdisallow_positional_args_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=no-value-for-parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, download_dir, extract_dir, manual_dir, dataset_name, force_download, force_extraction, register_checksums)\u001b[0m\n\u001b[1;32m    167\u001b[0m         extract_dir or os.path.join(download_dir, 'extracted'))\n\u001b[1;32m    168\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manual_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanual_dir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanual_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_force_download\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/school/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m   \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /media/samuel/DATA2; Permission denied"
     ]
    }
   ],
   "source": [
    "split = list(tfds.Split.TRAIN.subsplit(weighted=split_weights))\n",
    "(raw_train, raw_val, raw_test), metadata = tfds.load(name='tf_flowers', \n",
    "                                                     split=split, \n",
    "                                                     data_dir=data_dir, \n",
    "                                                     as_supervised=True, \n",
    "                                                     with_info=True)\n",
    "\n",
    "num_train, num_val, num_test = (metadata.splits['train'].num_examples * weight / 10 for weight in split_weights)\n",
    "\n",
    "print('Number of training samples %d' % num_train)\n",
    " \n",
    "train = raw_train.map(resize_normalize)\n",
    "val = raw_val.map(resize_normalize)\n",
    "test = raw_test.map(resize_normalize)\n",
    "\n",
    "train = train.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "val = [(np.expand_dims(x[0], axis=0), np.expand_dims(x[1], axis=0)) for x in list(val.as_numpy_iterator())]\n",
    "test = test.batch(batch_size)\n",
    "\n",
    "labels = metadata.features['label']\n",
    "num_classes = labels.num_classes\n",
    "get_label_name = labels.int2str\n",
    "classes = list(map(get_label_name, range(num_classes)))\n",
    "print(classes)\n",
    "\n",
    "plt.figure(figsize=(12, 12)) \n",
    "for btch in train.take(1):\n",
    "    for i in range(9):\n",
    "        img, lbl = btch[0][i], btch[1][i]\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.title(get_label_name(lbl.numpy()))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "steps_per_epoch = round(num_train) // batch_size\n",
    "validation_steps = round(num_val) // batch_size\n",
    "\n",
    "log_dir='runs/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0P0Z1dExKr6"
   },
   "source": [
    "Use Keras' image generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    featurewise_center=False, \n",
    "    samplewise_center=False, \n",
    "    featurewise_std_normalization=False, \n",
    "    samplewise_std_normalization=False, \n",
    "    zca_whitening=False, \n",
    "    rotation_range=45, \n",
    "    brightness_range=[0.2, 1.0], \n",
    "    shear_range=0.15, \n",
    "    width_shift_range=0.15, \n",
    "    height_shift_range=0.15, \n",
    "    horizontal_flip=True, \n",
    "    vertical_flip=True, \n",
    "    zoom_range=[0.5, 1.0], \n",
    "    fill_mode='nearest', \n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "#validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "'''\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, \n",
    "    target_size=(default_size, default_size), \n",
    "    color_mode='rgb', \n",
    "    class_mode='categorical', \n",
    "    batch_size=batch_size, \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, \n",
    "    target_size=(default_size, default_size), \n",
    "    color_mode='rgb', \n",
    "    class_mode='categorical', \n",
    "    batch_size=1, \n",
    "    subset='validation'\n",
    ")\n",
    "'''\n",
    "\n",
    "train_datagen.flow(X_train, y_train, batch_size=32)\n",
    "test_datagen.flow(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNcv2UBatAvd"
   },
   "source": [
    "Instanciate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf98STr5ETnG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MobileNetV2(input_shape=input_shape, \n",
    "                    alpha=alpha, \n",
    "                    include_top=include_top, \n",
    "                    weights=weights, \n",
    "                    pooling=pooling, \n",
    "                    classes=num_classes + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CnEK_vDsqtZ"
   },
   "source": [
    "Configure training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_DGz4J0ZqzQe",
    "outputId": "714c3f8e-0fac-4221-8e8a-cd6b3d137b72",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Optimization strategy.\n",
    "#optimizer = tf.keras.optimizers.Nadam()\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr=learning_rate)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9)\n",
    "optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=6, slow_step_size=0.5)\n",
    "\n",
    "# Backpropagated loss function.\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training configuration.\n",
    "model.compile(loss=loss, \n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy', \n",
    "                       'mean_squared_error', \n",
    "                       'mean_absolute_error', \n",
    "                       'mean_squared_logarithmic_error', \n",
    "                       'categorical_crossentropy'])\n",
    "\n",
    "# Save model weights after each epoch if validation loss decreased.\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint('model.h5', \n",
    "                                                  monitor='val_loss', \n",
    "                                                  mode='auto', \n",
    "                                                  verbose=1, \n",
    "                                                  save_best_only=True, \n",
    "                                                  save_weights_only=False, \n",
    "                                                  save_freq='epoch')\n",
    "\n",
    "# Reduce learning rate when there is less change in monitored value.\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 min_delta=0.01,\n",
    "                                                 patience=lr_patience,\n",
    "                                                 factor=0.25,\n",
    "                                                 verbose=1,\n",
    "                                                 cooldown=0,\n",
    "                                                 min_lr=0.00000001)\n",
    "\n",
    "# Automagic epoch setup.\n",
    "stopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                           mode='auto', \n",
    "                                           patience=stopper_patience, \n",
    "                                           restore_best_weights=True, \n",
    "                                           verbose=1)\n",
    "\n",
    "# Print model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKFaJBUQwiaS"
   },
   "source": [
    "Plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-njv4WHjbEDi",
    "outputId": "de02681f-62f8-42c6-cc1d-0f7b6328aae0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m579RoJu4UF"
   },
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDF-ro6UFO4m",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    if fine_tuning:\n",
    "        model.trainable = False\n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    history = model.fit(x=train_generator, \n",
    "                        validation_data=test_generator, \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        validation_steps=validation_steps, \n",
    "                        epochs=epochs, \n",
    "                        verbose=1, \n",
    "                        callbacks=[checkpointer, reduce_lr, stopper], \n",
    "                        shuffle=True).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show various graphs of metrics used while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'accuracy', 'val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'mean_squared_error', 'val_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'mean_absolute_error', 'val_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'mean_squared_logarithmic_error', 'val_mean_squared_logarithmic_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'categorical_crossentropy', 'val_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_statistics(history, 'lr', 'lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation on subsets of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    print(\"val\\n\", model.evaluate(val, verbose=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make actual predictions on subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "    valid_predictions = model.predict(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owVo8tOHsE0K"
   },
   "source": [
    "Retrieve and show image before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "7w4iMZmGOOS6",
    "outputId": "82b42d42-ae05-49d3-fc40-1d2cefd45f32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(tf.keras.utils.get_file(origin=origin, fname='flower_photos', untar=True))\n",
    "flower_path = list(data_dir.glob('*roses*/*'))[0]\n",
    "img = tf.keras.preprocessing.image.load_img(flower_path, target_size=input_shape)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UglOhtDWwsRE"
   },
   "source": [
    "Inference and display of decoded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "yTkkG1-SEi7_",
    "outputId": "d320b3e3-8bce-4f67-dfb5-7c5a24a80d74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_image = prepare_image(img)\n",
    "predictions = model.predict(preprocessed_image)\n",
    "decode_predictions(predictions, top_k=5, classes=classes[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9hNCDcZvnUU"
   },
   "source": [
    "Optimize for size (quantize weights) then convert to bytecode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYjiT8AS9Jsf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "open(model_tflite, 'wb').write(converter.convert())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lITyWcFev_vS"
   },
   "source": [
    "Convert the bytecode to characters representing C arrays (the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KbuBrfK9OL2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!xxd -i $model_tflite > $model_cc\n",
    "!cat $model_cc"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "projet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
