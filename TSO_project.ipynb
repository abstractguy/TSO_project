{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSO_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abstractguy/TSO_project/blob/master/TSO_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "outputId": "8da9c7b6-aa9e-4776-ab47-3d167d4fc1ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install cmake xvfb python-opengl ffmpeg libgle3 x11-utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.177)] [Connecting to security.u\r0% [Connecting to archive.ubuntu.com (91.189.88.177)] [Connecting to security.u\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.177)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.177)\r                                                                               \rHit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.177)\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rGet:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Waiting for headers] [5 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [760 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [926 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [628 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,279 kB]\n",
            "Fetched 3,849 kB in 3s (1,501 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libgle3 libxxf86dga1 python-opengl x11-utils xvfb\n",
            "0 upgraded, 5 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 1,527 kB of archives.\n",
            "After this operation, 8,533 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgle3 amd64 3.1.0-7.2 [37.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 1,527 kB in 1s (1,713 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package libgle3:amd64.\n",
            "Preparing to unpack .../libgle3_3.1.0-7.2_amd64.deb ...\n",
            "Unpacking libgle3:amd64 (3.1.0-7.2) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up libgle3:amd64 (3.1.0-7.2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNBBa7sDGbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "bb05c21a-9f55-4e64-f6e7-2b822ac4712e"
      },
      "source": [
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup gym gym[all] pyvirtualdisplay pyglet==1.3.2 keras-rl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.2.0)\n",
            "Collecting ez_setup\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/2c/743df41bd6b3298706dfe91b0c7ecdc47f2dc1a3104abeb6e9aa4a45fa5d/ez_setup-0.9.tar.gz\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.5MB/s \n",
            "\u001b[?25hCollecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.1)\n",
            "Collecting EasyProcess (from pyvirtualdisplay)\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.6.16)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Building wheels for collected packages: ez-setup, keras-rl\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez-setup: filename=ez_setup-0.9-cp36-none-any.whl size=11014 sha256=dab0aca08e7a26f2837deea896b5376c74801f44c9f150915d8a6c7d79780af8\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e8/6b/3d5ff5a3efd7b5338d1e173ac981771e2628ceb2f7866d49ad\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=0d3daaff0b0f43bda836a295d259af389e02f7116aed59c9d2ca60699c099ad7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built ez-setup keras-rl\n",
            "Installing collected packages: ez-setup, EasyProcess, pyvirtualdisplay, pyglet, keras-rl\n",
            "  Found existing installation: pyglet 1.4.2\n",
            "    Uninstalling pyglet-1.4.2:\n",
            "      Successfully uninstalled pyglet-1.4.2\n",
            "Successfully installed EasyProcess-0.2.7 ez-setup-0.9 keras-rl-0.4.2 pyglet-1.3.2 pyvirtualdisplay-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNwLFtjJCSMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g45rPVFxCVJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from glob import glob\n",
        "from io import open as io_open\n",
        "from base64 import b64encode\n",
        "from numpy.random import seed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxE2SltKCYpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b0f6aca-8f41-4a7d-9196-d585f8b6d48d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuuTJHGhCb6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym import logger as gymlogger, make as make_env\n",
        "from gym.wrappers import Monitor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env = wrap_env(env)\n",
        "def show_video():\n",
        "    mp4list = glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io_open(mp4, 'r+b').read()\n",
        "        encoded = b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                                    loop controls style=\"height: 400px;\">\n",
        "                                    <source src=\"data:video/mp4;base64,{0}\" \n",
        "                                    type=\"video/mp4\" /> </video>'''.format(\n",
        "                                        encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print('Could not find video')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKb9R2gSB2wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_env(env):\n",
        "  return Monitor(env, './video', force=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENV_NAME = 'CartPole-v1'\n",
        "env = wrap_env(make_env(ENV_NAME))\n",
        "env.seed(123)\n",
        "seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "gymlogger.set_level(40) # Error only.\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bKWVPU09vIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "a9bf034a-8738-4d62-f70b-0310a56bc6a9"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0831 23:46:42.291037 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0831 23:46:42.356542 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0831 23:46:42.398266 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 658\n",
            "Trainable params: 658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fFyBI_dBn_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKJ9i2EFBrdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enable the dueling network (dueling_type is one of {'avg', 'max', 'naive'}).\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=nb_actions, \n",
        "               memory=memory, \n",
        "               nb_steps_warmup=30,\n",
        "               enable_dueling_network=True, \n",
        "               dueling_type='avg', \n",
        "               target_model_update=0.01, \n",
        "               policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq34pCSUDPT3",
        "colab_type": "code",
        "outputId": "2a103e7c-1b46-46ec-e237-5c7384753322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dqn.compile(Adam(lr=0.001), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "dqn.save_weights('duel_dqn_' + ENV_NAME + '_weights.h5f', overwrite=True)\n",
        "dqn.test(env, nb_episodes=10, visualize=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0831 23:46:42.656624 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0831 23:46:42.657849 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0831 23:46:43.009269 140304730335104 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "    21/50000: episode: 1, duration: 2.742s, episode steps: 21, steps per second: 8, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.060 [-1.001, 1.740], loss: --, mean_absolute_error: --, mean_q: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    40/50000: episode: 2, duration: 1.807s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.030 [-2.725, 1.808], loss: 0.466300, mean_absolute_error: 0.525609, mean_q: -0.079362\n",
            "    80/50000: episode: 3, duration: 0.268s, episode steps: 40, steps per second: 149, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.027 [-1.102, 0.646], loss: 0.346425, mean_absolute_error: 0.467895, mean_q: 0.113354\n",
            "   102/50000: episode: 4, duration: 0.093s, episode steps: 22, steps per second: 237, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.080 [-1.588, 0.776], loss: 0.194776, mean_absolute_error: 0.500565, mean_q: 0.447426\n",
            "   138/50000: episode: 5, duration: 0.152s, episode steps: 36, steps per second: 237, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: -0.078 [-1.347, 0.651], loss: 0.060500, mean_absolute_error: 0.584992, mean_q: 0.898215\n",
            "   157/50000: episode: 6, duration: 0.072s, episode steps: 19, steps per second: 263, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.069 [-1.487, 0.985], loss: 0.012490, mean_absolute_error: 0.663386, mean_q: 1.221098\n",
            "   178/50000: episode: 7, duration: 0.087s, episode steps: 21, steps per second: 241, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.078 [-1.611, 0.842], loss: 0.012112, mean_absolute_error: 0.704040, mean_q: 1.313266\n",
            "   205/50000: episode: 8, duration: 0.108s, episode steps: 27, steps per second: 251, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.036 [-1.369, 2.038], loss: 0.009706, mean_absolute_error: 0.772137, mean_q: 1.470686\n",
            "   234/50000: episode: 9, duration: 0.653s, episode steps: 29, steps per second: 44, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.075 [-0.947, 1.889], loss: 0.013693, mean_absolute_error: 0.864465, mean_q: 1.659236\n",
            "   244/50000: episode: 10, duration: 0.149s, episode steps: 10, steps per second: 67, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.197, 2.053], loss: 0.012847, mean_absolute_error: 0.942178, mean_q: 1.820645\n",
            "   270/50000: episode: 11, duration: 0.140s, episode steps: 26, steps per second: 186, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.038 [-1.570, 0.952], loss: 0.026382, mean_absolute_error: 1.000809, mean_q: 1.906755\n",
            "   317/50000: episode: 12, duration: 0.219s, episode steps: 47, steps per second: 215, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: -0.085 [-0.981, 0.628], loss: 0.024688, mean_absolute_error: 1.128097, mean_q: 2.184313\n",
            "   328/50000: episode: 13, duration: 0.053s, episode steps: 11, steps per second: 206, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.126 [-1.134, 1.841], loss: 0.027725, mean_absolute_error: 1.243807, mean_q: 2.411137\n",
            "   342/50000: episode: 14, duration: 0.067s, episode steps: 14, steps per second: 209, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.084 [-1.671, 1.000], loss: 0.048321, mean_absolute_error: 1.316851, mean_q: 2.556565\n",
            "   365/50000: episode: 15, duration: 0.121s, episode steps: 23, steps per second: 190, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.060 [-1.384, 2.363], loss: 0.039930, mean_absolute_error: 1.372929, mean_q: 2.648012\n",
            "   378/50000: episode: 16, duration: 0.059s, episode steps: 13, steps per second: 219, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.104 [-0.973, 1.607], loss: 0.035172, mean_absolute_error: 1.451101, mean_q: 2.804656\n",
            "   398/50000: episode: 17, duration: 0.077s, episode steps: 20, steps per second: 260, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.063 [-1.358, 2.251], loss: 0.062020, mean_absolute_error: 1.511251, mean_q: 2.880831\n",
            "   417/50000: episode: 18, duration: 0.077s, episode steps: 19, steps per second: 248, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.069 [-0.996, 1.718], loss: 0.044041, mean_absolute_error: 1.588343, mean_q: 3.095797\n",
            "   436/50000: episode: 19, duration: 0.075s, episode steps: 19, steps per second: 252, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.067 [-0.946, 0.613], loss: 0.059134, mean_absolute_error: 1.657074, mean_q: 3.204015\n",
            "   489/50000: episode: 20, duration: 0.267s, episode steps: 53, steps per second: 199, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.080 [-0.801, 1.748], loss: 0.075700, mean_absolute_error: 1.812580, mean_q: 3.492441\n",
            "   543/50000: episode: 21, duration: 0.245s, episode steps: 54, steps per second: 221, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.574 [0.000, 1.000], mean observation: -0.004 [-2.765, 1.601], loss: 0.069554, mean_absolute_error: 2.022344, mean_q: 3.924169\n",
            "   569/50000: episode: 22, duration: 0.102s, episode steps: 26, steps per second: 256, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.075 [-0.619, 1.405], loss: 0.072389, mean_absolute_error: 2.187838, mean_q: 4.257987\n",
            "   597/50000: episode: 23, duration: 0.128s, episode steps: 28, steps per second: 220, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.739, 1.188], loss: 0.117813, mean_absolute_error: 2.295063, mean_q: 4.414195\n",
            "   616/50000: episode: 24, duration: 0.083s, episode steps: 19, steps per second: 230, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.089 [-0.772, 1.307], loss: 0.074707, mean_absolute_error: 2.383970, mean_q: 4.626080\n",
            "   626/50000: episode: 25, duration: 0.041s, episode steps: 10, steps per second: 243, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.143 [-1.595, 0.966], loss: 0.108939, mean_absolute_error: 2.436614, mean_q: 4.685891\n",
            "   638/50000: episode: 26, duration: 0.048s, episode steps: 12, steps per second: 248, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-0.970, 1.752], loss: 0.147174, mean_absolute_error: 2.467952, mean_q: 4.723111\n",
            "   656/50000: episode: 27, duration: 0.084s, episode steps: 18, steps per second: 213, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.068 [-0.792, 1.154], loss: 0.116589, mean_absolute_error: 2.533500, mean_q: 4.874907\n",
            "   674/50000: episode: 28, duration: 0.443s, episode steps: 18, steps per second: 41, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.098 [-1.703, 0.939], loss: 0.109665, mean_absolute_error: 2.589669, mean_q: 4.999579\n",
            "   701/50000: episode: 29, duration: 0.184s, episode steps: 27, steps per second: 147, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.036 [-1.746, 1.034], loss: 0.150465, mean_absolute_error: 2.720512, mean_q: 5.209149\n",
            "   718/50000: episode: 30, duration: 0.088s, episode steps: 17, steps per second: 194, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.072 [-1.266, 0.829], loss: 0.185748, mean_absolute_error: 2.779121, mean_q: 5.327576\n",
            "   747/50000: episode: 31, duration: 0.117s, episode steps: 29, steps per second: 248, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.044 [-1.024, 1.874], loss: 0.133926, mean_absolute_error: 2.863750, mean_q: 5.555019\n",
            "   774/50000: episode: 32, duration: 0.119s, episode steps: 27, steps per second: 228, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.029 [-1.020, 1.500], loss: 0.170176, mean_absolute_error: 2.997466, mean_q: 5.793501\n",
            "   814/50000: episode: 33, duration: 0.160s, episode steps: 40, steps per second: 249, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.575 [0.000, 1.000], mean observation: 0.003 [-1.848, 1.379], loss: 0.169841, mean_absolute_error: 3.121550, mean_q: 6.060914\n",
            "   852/50000: episode: 34, duration: 0.164s, episode steps: 38, steps per second: 232, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.749, 1.345], loss: 0.140191, mean_absolute_error: 3.264589, mean_q: 6.422947\n",
            "   886/50000: episode: 35, duration: 0.138s, episode steps: 34, steps per second: 247, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.005 [-1.369, 1.479], loss: 0.155580, mean_absolute_error: 3.422757, mean_q: 6.741056\n",
            "   943/50000: episode: 36, duration: 0.232s, episode steps: 57, steps per second: 246, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: 0.057 [-1.323, 2.034], loss: 0.167660, mean_absolute_error: 3.621539, mean_q: 7.127300\n",
            "  1015/50000: episode: 37, duration: 0.300s, episode steps: 72, steps per second: 240, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-1.397, 1.280], loss: 0.144295, mean_absolute_error: 3.860559, mean_q: 7.676626\n",
            "  1050/50000: episode: 38, duration: 0.139s, episode steps: 35, steps per second: 252, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.019 [-1.340, 1.965], loss: 0.174844, mean_absolute_error: 4.107448, mean_q: 8.125560\n",
            "  1090/50000: episode: 39, duration: 0.200s, episode steps: 40, steps per second: 200, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.095 [-1.046, 0.854], loss: 0.150562, mean_absolute_error: 4.275996, mean_q: 8.519712\n",
            "  1122/50000: episode: 40, duration: 0.153s, episode steps: 32, steps per second: 208, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: 0.060 [-1.200, 2.182], loss: 0.170422, mean_absolute_error: 4.413784, mean_q: 8.799347\n",
            "  1153/50000: episode: 41, duration: 0.149s, episode steps: 31, steps per second: 208, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.613 [0.000, 1.000], mean observation: 0.045 [-1.767, 1.356], loss: 0.174694, mean_absolute_error: 4.517577, mean_q: 9.043330\n",
            "  1283/50000: episode: 42, duration: 0.510s, episode steps: 130, steps per second: 255, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.005 [-1.532, 1.546], loss: 0.161036, mean_absolute_error: 4.906418, mean_q: 9.847714\n",
            "  1302/50000: episode: 43, duration: 0.074s, episode steps: 19, steps per second: 256, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.037 [-1.621, 1.205], loss: 0.161223, mean_absolute_error: 5.228844, mean_q: 10.439111\n",
            "  1340/50000: episode: 44, duration: 0.161s, episode steps: 38, steps per second: 236, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.553 [0.000, 1.000], mean observation: -0.031 [-1.627, 0.839], loss: 0.223508, mean_absolute_error: 5.350254, mean_q: 10.752708\n",
            "  1461/50000: episode: 45, duration: 0.518s, episode steps: 121, steps per second: 233, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.196 [-1.579, 0.947], loss: 0.203386, mean_absolute_error: 5.756504, mean_q: 11.585253\n",
            "  1539/50000: episode: 46, duration: 0.322s, episode steps: 78, steps per second: 242, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.218 [-1.119, 1.179], loss: 0.227039, mean_absolute_error: 6.202775, mean_q: 12.561547\n",
            "  1617/50000: episode: 47, duration: 0.323s, episode steps: 78, steps per second: 241, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.146 [-1.167, 1.551], loss: 0.314173, mean_absolute_error: 6.641834, mean_q: 13.410945\n",
            "  1733/50000: episode: 48, duration: 0.528s, episode steps: 116, steps per second: 220, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.151 [-1.924, 1.607], loss: 0.349209, mean_absolute_error: 7.068935, mean_q: 14.283759\n",
            "  1950/50000: episode: 49, duration: 0.868s, episode steps: 217, steps per second: 250, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.167 [-1.844, 1.145], loss: 0.366870, mean_absolute_error: 7.931378, mean_q: 16.096508\n",
            "  2081/50000: episode: 50, duration: 0.537s, episode steps: 131, steps per second: 244, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.221 [-1.668, 0.743], loss: 0.267465, mean_absolute_error: 8.784873, mean_q: 17.889355\n",
            "  2244/50000: episode: 51, duration: 0.635s, episode steps: 163, steps per second: 257, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.257 [-1.980, 1.266], loss: 0.578052, mean_absolute_error: 9.484679, mean_q: 19.270218\n",
            "  2368/50000: episode: 52, duration: 0.483s, episode steps: 124, steps per second: 257, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.159 [-0.976, 0.839], loss: 0.433912, mean_absolute_error: 10.200556, mean_q: 20.790556\n",
            "  2541/50000: episode: 53, duration: 0.670s, episode steps: 173, steps per second: 258, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.245 [-1.710, 0.892], loss: 0.556282, mean_absolute_error: 10.965059, mean_q: 22.352482\n",
            "  2788/50000: episode: 54, duration: 0.979s, episode steps: 247, steps per second: 252, episode reward: 247.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.240 [-0.988, 2.201], loss: 0.632330, mean_absolute_error: 11.969845, mean_q: 24.420197\n",
            "  3005/50000: episode: 55, duration: 0.910s, episode steps: 217, steps per second: 239, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.240 [-1.990, 0.831], loss: 0.740237, mean_absolute_error: 13.136676, mean_q: 26.804415\n",
            "  3164/50000: episode: 56, duration: 0.644s, episode steps: 159, steps per second: 247, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.352 [-2.439, 1.013], loss: 0.843558, mean_absolute_error: 13.971498, mean_q: 28.526840\n",
            "  3382/50000: episode: 57, duration: 0.898s, episode steps: 218, steps per second: 243, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.223 [-1.850, 0.797], loss: 0.792207, mean_absolute_error: 14.888749, mean_q: 30.404741\n",
            "  3558/50000: episode: 58, duration: 0.755s, episode steps: 176, steps per second: 233, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.350 [-2.921, 0.907], loss: 1.033354, mean_absolute_error: 15.887329, mean_q: 32.345860\n",
            "  3882/50000: episode: 59, duration: 1.217s, episode steps: 324, steps per second: 266, episode reward: 324.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.215 [-1.107, 2.443], loss: 1.090731, mean_absolute_error: 17.089792, mean_q: 34.800919\n",
            "  4165/50000: episode: 60, duration: 1.071s, episode steps: 283, steps per second: 264, episode reward: 283.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.245 [-1.075, 2.414], loss: 1.115393, mean_absolute_error: 18.538879, mean_q: 37.742470\n",
            "  4452/50000: episode: 61, duration: 1.137s, episode steps: 287, steps per second: 252, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.161 [-1.643, 0.876], loss: 1.280773, mean_absolute_error: 19.843197, mean_q: 40.446682\n",
            "  4635/50000: episode: 62, duration: 0.735s, episode steps: 183, steps per second: 249, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.325 [-2.382, 0.729], loss: 1.672276, mean_absolute_error: 20.937185, mean_q: 42.572899\n",
            "  4864/50000: episode: 63, duration: 1.003s, episode steps: 229, steps per second: 228, episode reward: 229.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.287 [-2.930, 1.315], loss: 1.689324, mean_absolute_error: 21.792259, mean_q: 44.258503\n",
            "  5104/50000: episode: 64, duration: 0.988s, episode steps: 240, steps per second: 243, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.274 [-2.537, 0.871], loss: 1.785593, mean_absolute_error: 22.812853, mean_q: 46.362278\n",
            "  5280/50000: episode: 65, duration: 3.272s, episode steps: 176, steps per second: 54, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.249 [-1.677, 0.718], loss: 1.686962, mean_absolute_error: 23.698853, mean_q: 48.240284\n",
            "  5488/50000: episode: 66, duration: 1.015s, episode steps: 208, steps per second: 205, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.311 [-2.549, 0.723], loss: 1.382470, mean_absolute_error: 24.417171, mean_q: 49.658577\n",
            "  5699/50000: episode: 67, duration: 0.788s, episode steps: 211, steps per second: 268, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.304 [-2.411, 0.835], loss: 1.922551, mean_absolute_error: 25.370993, mean_q: 51.521511\n",
            "  6045/50000: episode: 68, duration: 1.331s, episode steps: 346, steps per second: 260, episode reward: 346.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.201 [-1.147, 2.406], loss: 2.152971, mean_absolute_error: 26.468061, mean_q: 53.759132\n",
            "  6223/50000: episode: 69, duration: 0.679s, episode steps: 178, steps per second: 262, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.363 [-2.745, 0.820], loss: 1.679972, mean_absolute_error: 27.507568, mean_q: 55.875141\n",
            "  6457/50000: episode: 70, duration: 0.959s, episode steps: 234, steps per second: 244, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.267 [-2.385, 0.878], loss: 2.208898, mean_absolute_error: 28.199503, mean_q: 57.324478\n",
            "  6683/50000: episode: 71, duration: 0.898s, episode steps: 226, steps per second: 252, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.304 [-0.852, 2.412], loss: 2.352127, mean_absolute_error: 29.022625, mean_q: 58.940750\n",
            "  6924/50000: episode: 72, duration: 1.015s, episode steps: 241, steps per second: 237, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.270 [-2.710, 0.942], loss: 2.564193, mean_absolute_error: 29.972437, mean_q: 60.781689\n",
            "  7108/50000: episode: 73, duration: 0.679s, episode steps: 184, steps per second: 271, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.346 [-2.798, 0.841], loss: 2.554137, mean_absolute_error: 30.703968, mean_q: 62.363052\n",
            "  7351/50000: episode: 74, duration: 0.919s, episode steps: 243, steps per second: 264, episode reward: 243.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.292 [-0.980, 2.430], loss: 2.642625, mean_absolute_error: 31.455969, mean_q: 63.739063\n",
            "  7657/50000: episode: 75, duration: 1.191s, episode steps: 306, steps per second: 257, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.209 [-2.729, 0.809], loss: 2.308501, mean_absolute_error: 32.280224, mean_q: 65.490005\n",
            "  7912/50000: episode: 76, duration: 1.027s, episode steps: 255, steps per second: 248, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.276 [-1.027, 2.421], loss: 2.560941, mean_absolute_error: 33.342510, mean_q: 67.638718\n",
            "  8098/50000: episode: 77, duration: 0.758s, episode steps: 186, steps per second: 246, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.343 [-2.550, 0.912], loss: 2.828339, mean_absolute_error: 34.055271, mean_q: 69.012573\n",
            "  8557/50000: episode: 78, duration: 1.796s, episode steps: 459, steps per second: 256, episode reward: 459.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.104 [-1.678, 0.822], loss: 2.198689, mean_absolute_error: 34.957584, mean_q: 70.986542\n",
            "  8831/50000: episode: 79, duration: 1.066s, episode steps: 274, steps per second: 257, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.241 [-2.744, 1.143], loss: 3.237605, mean_absolute_error: 36.074612, mean_q: 73.157799\n",
            "  9047/50000: episode: 80, duration: 0.895s, episode steps: 216, steps per second: 241, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.301 [-2.519, 0.912], loss: 2.402152, mean_absolute_error: 36.864326, mean_q: 74.697868\n",
            "  9264/50000: episode: 81, duration: 0.831s, episode steps: 217, steps per second: 261, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.298 [-2.732, 0.803], loss: 2.914623, mean_absolute_error: 37.251106, mean_q: 75.463806\n",
            "  9488/50000: episode: 82, duration: 0.849s, episode steps: 224, steps per second: 264, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.287 [-2.707, 1.040], loss: 1.932093, mean_absolute_error: 37.869785, mean_q: 76.828804\n",
            "  9734/50000: episode: 83, duration: 0.969s, episode steps: 246, steps per second: 254, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.273 [-2.580, 0.860], loss: 3.067068, mean_absolute_error: 38.443417, mean_q: 77.889023\n",
            "  9975/50000: episode: 84, duration: 0.954s, episode steps: 241, steps per second: 253, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.271 [-2.897, 0.779], loss: 2.233843, mean_absolute_error: 38.944057, mean_q: 78.998207\n",
            " 10328/50000: episode: 85, duration: 1.424s, episode steps: 353, steps per second: 248, episode reward: 353.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.188 [-2.421, 0.847], loss: 2.597057, mean_absolute_error: 39.910393, mean_q: 80.941895\n",
            " 10596/50000: episode: 86, duration: 0.988s, episode steps: 268, steps per second: 271, episode reward: 268.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.239 [-3.134, 1.169], loss: 2.735794, mean_absolute_error: 40.420715, mean_q: 81.946159\n",
            " 10800/50000: episode: 87, duration: 0.753s, episode steps: 204, steps per second: 271, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.316 [-2.911, 1.065], loss: 2.617413, mean_absolute_error: 41.097878, mean_q: 83.259918\n",
            " 11011/50000: episode: 88, duration: 0.856s, episode steps: 211, steps per second: 246, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.311 [-2.611, 0.807], loss: 2.665431, mean_absolute_error: 41.489254, mean_q: 84.102608\n",
            " 11230/50000: episode: 89, duration: 0.845s, episode steps: 219, steps per second: 259, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.272 [-2.233, 0.636], loss: 2.611428, mean_absolute_error: 42.097656, mean_q: 85.321579\n",
            " 11431/50000: episode: 90, duration: 0.781s, episode steps: 201, steps per second: 257, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.326 [-2.520, 0.782], loss: 2.496773, mean_absolute_error: 42.326332, mean_q: 85.701164\n",
            " 11620/50000: episode: 91, duration: 0.746s, episode steps: 189, steps per second: 253, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.343 [-2.509, 0.883], loss: 2.575145, mean_absolute_error: 42.599533, mean_q: 86.258629\n",
            " 11843/50000: episode: 92, duration: 0.909s, episode steps: 223, steps per second: 245, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.299 [-2.438, 0.773], loss: 2.703830, mean_absolute_error: 43.052677, mean_q: 87.200378\n",
            " 12048/50000: episode: 93, duration: 0.768s, episode steps: 205, steps per second: 267, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.321 [-2.571, 0.732], loss: 2.119452, mean_absolute_error: 43.590740, mean_q: 88.359253\n",
            " 12259/50000: episode: 94, duration: 0.811s, episode steps: 211, steps per second: 260, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.310 [-2.413, 0.765], loss: 2.927907, mean_absolute_error: 44.040783, mean_q: 89.200661\n",
            " 12509/50000: episode: 95, duration: 1.006s, episode steps: 250, steps per second: 249, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.277 [-0.969, 2.442], loss: 3.046725, mean_absolute_error: 44.149876, mean_q: 89.343086\n",
            " 12758/50000: episode: 96, duration: 1.048s, episode steps: 249, steps per second: 238, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.267 [-2.699, 0.766], loss: 3.597925, mean_absolute_error: 44.661732, mean_q: 90.326958\n",
            " 12929/50000: episode: 97, duration: 0.774s, episode steps: 171, steps per second: 221, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.299 [-2.016, 0.728], loss: 2.615112, mean_absolute_error: 44.992744, mean_q: 91.149330\n",
            " 13203/50000: episode: 98, duration: 1.103s, episode steps: 274, steps per second: 248, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.239 [-3.107, 1.016], loss: 2.431306, mean_absolute_error: 45.110489, mean_q: 91.431694\n",
            " 13471/50000: episode: 99, duration: 1.112s, episode steps: 268, steps per second: 241, episode reward: 268.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.244 [-2.564, 0.848], loss: 2.812700, mean_absolute_error: 45.601418, mean_q: 92.294685\n",
            " 13653/50000: episode: 100, duration: 0.679s, episode steps: 182, steps per second: 268, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.348 [-3.002, 1.116], loss: 3.714816, mean_absolute_error: 45.983421, mean_q: 93.223389\n",
            " 13832/50000: episode: 101, duration: 0.674s, episode steps: 179, steps per second: 266, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.358 [-2.740, 0.943], loss: 2.432739, mean_absolute_error: 46.407124, mean_q: 93.866074\n",
            " 14048/50000: episode: 102, duration: 0.842s, episode steps: 216, steps per second: 256, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.294 [-2.970, 1.072], loss: 2.617291, mean_absolute_error: 46.505314, mean_q: 94.188385\n",
            " 14266/50000: episode: 103, duration: 0.908s, episode steps: 218, steps per second: 240, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.296 [-2.966, 1.433], loss: 1.801039, mean_absolute_error: 46.863712, mean_q: 94.798164\n",
            " 14472/50000: episode: 104, duration: 0.882s, episode steps: 206, steps per second: 233, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.249 [-1.832, 0.899], loss: 3.744413, mean_absolute_error: 47.279022, mean_q: 95.681908\n",
            " 14691/50000: episode: 105, duration: 0.941s, episode steps: 219, steps per second: 233, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.284 [-3.555, 1.801], loss: 2.845683, mean_absolute_error: 47.202698, mean_q: 95.490349\n",
            " 14893/50000: episode: 106, duration: 0.969s, episode steps: 202, steps per second: 208, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.314 [-2.958, 0.918], loss: 1.938217, mean_absolute_error: 47.769001, mean_q: 96.725807\n",
            " 15082/50000: episode: 107, duration: 0.745s, episode steps: 189, steps per second: 254, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.289 [-2.012, 1.033], loss: 2.605563, mean_absolute_error: 47.789455, mean_q: 96.739189\n",
            " 15291/50000: episode: 108, duration: 0.792s, episode steps: 209, steps per second: 264, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.227 [-1.959, 0.842], loss: 2.985704, mean_absolute_error: 47.926762, mean_q: 96.939461\n",
            " 15549/50000: episode: 109, duration: 0.948s, episode steps: 258, steps per second: 272, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.245 [-3.301, 1.415], loss: 2.399269, mean_absolute_error: 48.308872, mean_q: 97.535774\n",
            " 15755/50000: episode: 110, duration: 0.789s, episode steps: 206, steps per second: 261, episode reward: 206.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.319 [-2.557, 0.977], loss: 2.757385, mean_absolute_error: 48.441303, mean_q: 97.994400\n",
            " 15959/50000: episode: 111, duration: 0.816s, episode steps: 204, steps per second: 250, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.314 [-2.959, 0.845], loss: 2.480342, mean_absolute_error: 48.477749, mean_q: 98.080574\n",
            " 16157/50000: episode: 112, duration: 0.723s, episode steps: 198, steps per second: 274, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.236 [-1.798, 0.751], loss: 2.518779, mean_absolute_error: 48.383217, mean_q: 97.828522\n",
            " 16498/50000: episode: 113, duration: 1.247s, episode steps: 341, steps per second: 274, episode reward: 341.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.179 [-3.138, 1.601], loss: 2.463004, mean_absolute_error: 48.993126, mean_q: 98.957069\n",
            " 16722/50000: episode: 114, duration: 0.860s, episode steps: 224, steps per second: 260, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.277 [-2.996, 1.187], loss: 2.730855, mean_absolute_error: 49.304340, mean_q: 99.529724\n",
            " 16953/50000: episode: 115, duration: 0.883s, episode steps: 231, steps per second: 262, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.288 [-2.789, 0.780], loss: 3.516902, mean_absolute_error: 49.623230, mean_q: 100.168991\n",
            " 17302/50000: episode: 116, duration: 1.319s, episode steps: 349, steps per second: 265, episode reward: 349.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.188 [-3.004, 1.475], loss: 2.457252, mean_absolute_error: 49.491890, mean_q: 100.030266\n",
            " 17528/50000: episode: 117, duration: 0.931s, episode steps: 226, steps per second: 243, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.291 [-2.545, 0.834], loss: 1.986718, mean_absolute_error: 49.132233, mean_q: 99.439087\n",
            " 17758/50000: episode: 118, duration: 0.838s, episode steps: 230, steps per second: 275, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.286 [-2.975, 0.896], loss: 2.939451, mean_absolute_error: 49.819912, mean_q: 100.634171\n",
            " 17961/50000: episode: 119, duration: 0.741s, episode steps: 203, steps per second: 274, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.226 [-1.638, 0.848], loss: 3.715297, mean_absolute_error: 49.825138, mean_q: 100.619698\n",
            " 18220/50000: episode: 120, duration: 0.939s, episode steps: 259, steps per second: 276, episode reward: 259.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.258 [-2.741, 1.241], loss: 2.324761, mean_absolute_error: 49.781414, mean_q: 100.635910\n",
            " 18402/50000: episode: 121, duration: 0.665s, episode steps: 182, steps per second: 274, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.326 [-2.835, 1.549], loss: 2.785555, mean_absolute_error: 49.863548, mean_q: 100.746269\n",
            " 18867/50000: episode: 122, duration: 1.752s, episode steps: 465, steps per second: 265, episode reward: 465.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.106 [-4.359, 3.692], loss: 2.478754, mean_absolute_error: 50.373859, mean_q: 101.678680\n",
            " 19044/50000: episode: 123, duration: 0.646s, episode steps: 177, steps per second: 274, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.289 [-4.388, 3.751], loss: 3.243319, mean_absolute_error: 50.382118, mean_q: 101.734726\n",
            " 19305/50000: episode: 124, duration: 1.034s, episode steps: 261, steps per second: 252, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.229 [-3.938, 2.698], loss: 2.229402, mean_absolute_error: 50.778122, mean_q: 102.592659\n",
            " 19530/50000: episode: 125, duration: 0.868s, episode steps: 225, steps per second: 259, episode reward: 225.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.244 [-4.345, 3.529], loss: 1.541220, mean_absolute_error: 50.739227, mean_q: 102.560852\n",
            " 19701/50000: episode: 126, duration: 3.155s, episode steps: 171, steps per second: 54, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.439 [0.000, 1.000], mean observation: -0.284 [-3.936, 3.231], loss: 2.563308, mean_absolute_error: 50.678455, mean_q: 102.312202\n",
            " 19969/50000: episode: 127, duration: 1.171s, episode steps: 268, steps per second: 229, episode reward: 268.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.212 [-4.498, 3.780], loss: 1.869571, mean_absolute_error: 51.016857, mean_q: 102.896660\n",
            " 20261/50000: episode: 128, duration: 1.165s, episode steps: 292, steps per second: 251, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.218 [-3.672, 1.806], loss: 2.306047, mean_absolute_error: 50.949200, mean_q: 102.891693\n",
            " 20519/50000: episode: 129, duration: 0.954s, episode steps: 258, steps per second: 271, episode reward: 258.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.231 [-4.154, 2.999], loss: 2.153500, mean_absolute_error: 51.112495, mean_q: 103.195358\n",
            " 20863/50000: episode: 130, duration: 1.286s, episode steps: 344, steps per second: 268, episode reward: 344.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.208 [-1.039, 2.405], loss: 3.232237, mean_absolute_error: 51.186001, mean_q: 103.250389\n",
            " 21070/50000: episode: 131, duration: 0.766s, episode steps: 207, steps per second: 270, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.298 [-3.167, 1.267], loss: 2.637062, mean_absolute_error: 51.565735, mean_q: 103.941963\n",
            " 21341/50000: episode: 132, duration: 1.049s, episode steps: 271, steps per second: 258, episode reward: 271.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.148 [-1.617, 0.913], loss: 3.111403, mean_absolute_error: 51.726719, mean_q: 104.227554\n",
            " 21573/50000: episode: 133, duration: 0.944s, episode steps: 232, steps per second: 246, episode reward: 232.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.282 [-2.909, 0.961], loss: 3.011024, mean_absolute_error: 51.488235, mean_q: 103.837204\n",
            " 21798/50000: episode: 134, duration: 0.921s, episode steps: 225, steps per second: 244, episode reward: 225.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.270 [-2.374, 0.869], loss: 1.967611, mean_absolute_error: 51.394165, mean_q: 103.602882\n",
            " 22069/50000: episode: 135, duration: 0.972s, episode steps: 271, steps per second: 279, episode reward: 271.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.246 [-2.414, 0.804], loss: 1.659409, mean_absolute_error: 51.440807, mean_q: 103.868179\n",
            " 22344/50000: episode: 136, duration: 1.015s, episode steps: 275, steps per second: 271, episode reward: 275.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.240 [-2.430, 0.830], loss: 1.656354, mean_absolute_error: 51.695801, mean_q: 104.262695\n",
            " 22574/50000: episode: 137, duration: 0.835s, episode steps: 230, steps per second: 275, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.278 [-2.953, 1.015], loss: 2.322144, mean_absolute_error: 51.808353, mean_q: 104.485657\n",
            " 22815/50000: episode: 138, duration: 0.929s, episode steps: 241, steps per second: 260, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.268 [-2.543, 0.828], loss: 2.542173, mean_absolute_error: 51.561237, mean_q: 103.932770\n",
            " 23116/50000: episode: 139, duration: 1.168s, episode steps: 301, steps per second: 258, episode reward: 301.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.217 [-2.802, 0.870], loss: 1.957069, mean_absolute_error: 51.194378, mean_q: 103.203514\n",
            " 23305/50000: episode: 140, duration: 0.768s, episode steps: 189, steps per second: 246, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.341 [-2.741, 1.077], loss: 2.295847, mean_absolute_error: 51.778214, mean_q: 104.374733\n",
            " 23720/50000: episode: 141, duration: 1.520s, episode steps: 415, steps per second: 273, episode reward: 415.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.162 [-2.771, 1.241], loss: 2.175787, mean_absolute_error: 51.641483, mean_q: 104.101051\n",
            " 23921/50000: episode: 142, duration: 0.737s, episode steps: 201, steps per second: 273, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.322 [-2.763, 0.887], loss: 1.621736, mean_absolute_error: 51.646980, mean_q: 104.174225\n",
            " 24286/50000: episode: 143, duration: 1.384s, episode steps: 365, steps per second: 264, episode reward: 365.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.173 [-2.403, 1.120], loss: 1.992454, mean_absolute_error: 51.711533, mean_q: 104.261047\n",
            " 24548/50000: episode: 144, duration: 1.021s, episode steps: 262, steps per second: 257, episode reward: 262.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.248 [-2.446, 0.916], loss: 2.329273, mean_absolute_error: 51.760818, mean_q: 104.333458\n",
            " 24840/50000: episode: 145, duration: 1.182s, episode steps: 292, steps per second: 247, episode reward: 292.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.237 [-1.181, 2.416], loss: 2.059534, mean_absolute_error: 51.809170, mean_q: 104.493950\n",
            " 25042/50000: episode: 146, duration: 0.764s, episode steps: 202, steps per second: 264, episode reward: 202.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.317 [-3.356, 1.352], loss: 1.989109, mean_absolute_error: 51.895279, mean_q: 104.621071\n",
            " 25238/50000: episode: 147, duration: 0.732s, episode steps: 196, steps per second: 268, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.323 [-2.447, 1.073], loss: 2.711861, mean_absolute_error: 51.766323, mean_q: 104.265648\n",
            " 25493/50000: episode: 148, duration: 0.938s, episode steps: 255, steps per second: 272, episode reward: 255.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.271 [-0.970, 2.425], loss: 2.005557, mean_absolute_error: 51.625050, mean_q: 103.879257\n",
            " 25912/50000: episode: 149, duration: 1.535s, episode steps: 419, steps per second: 273, episode reward: 419.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.169 [-1.079, 2.406], loss: 2.122540, mean_absolute_error: 51.785240, mean_q: 104.318245\n",
            " 26115/50000: episode: 150, duration: 0.772s, episode steps: 203, steps per second: 263, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.318 [-2.411, 1.082], loss: 2.022860, mean_absolute_error: 51.849464, mean_q: 104.486076\n",
            " 26615/50000: episode: 151, duration: 1.984s, episode steps: 500, steps per second: 252, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.019 [-1.120, 0.908], loss: 2.047161, mean_absolute_error: 51.848045, mean_q: 104.479248\n",
            " 26810/50000: episode: 152, duration: 0.732s, episode steps: 195, steps per second: 266, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.311 [-3.976, 2.436], loss: 1.723680, mean_absolute_error: 52.235325, mean_q: 105.206642\n",
            " 27081/50000: episode: 153, duration: 1.016s, episode steps: 271, steps per second: 267, episode reward: 271.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.241 [-2.408, 1.058], loss: 1.708613, mean_absolute_error: 52.025059, mean_q: 104.757637\n",
            " 27341/50000: episode: 154, duration: 0.984s, episode steps: 260, steps per second: 264, episode reward: 260.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.249 [-2.599, 1.051], loss: 1.662688, mean_absolute_error: 51.994610, mean_q: 104.703499\n",
            " 27678/50000: episode: 155, duration: 1.328s, episode steps: 337, steps per second: 254, episode reward: 337.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.209 [-1.061, 2.419], loss: 1.932732, mean_absolute_error: 51.779701, mean_q: 104.346893\n",
            " 27919/50000: episode: 156, duration: 0.949s, episode steps: 241, steps per second: 254, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.269 [-3.166, 1.232], loss: 3.392896, mean_absolute_error: 51.936195, mean_q: 104.582306\n",
            " 28296/50000: episode: 157, duration: 1.502s, episode steps: 377, steps per second: 251, episode reward: 377.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.184 [-0.993, 2.556], loss: 2.622050, mean_absolute_error: 51.974499, mean_q: 104.695755\n",
            " 28509/50000: episode: 158, duration: 0.804s, episode steps: 213, steps per second: 265, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.303 [-2.423, 1.003], loss: 1.609200, mean_absolute_error: 52.126751, mean_q: 105.025223\n",
            " 29009/50000: episode: 159, duration: 1.931s, episode steps: 500, steps per second: 259, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.071 [-1.153, 1.153], loss: 1.508369, mean_absolute_error: 51.749748, mean_q: 104.225861\n",
            " 29237/50000: episode: 160, duration: 0.919s, episode steps: 228, steps per second: 248, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.281 [-2.533, 0.983], loss: 3.883562, mean_absolute_error: 52.097034, mean_q: 104.812622\n",
            " 29522/50000: episode: 161, duration: 1.137s, episode steps: 285, steps per second: 251, episode reward: 285.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.244 [-1.085, 2.399], loss: 1.360667, mean_absolute_error: 52.213379, mean_q: 105.158424\n",
            " 29731/50000: episode: 162, duration: 0.806s, episode steps: 209, steps per second: 259, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.326 [-1.094, 2.430], loss: 2.289158, mean_absolute_error: 51.881653, mean_q: 104.474487\n",
            " 30038/50000: episode: 163, duration: 1.153s, episode steps: 307, steps per second: 266, episode reward: 307.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.225 [-1.046, 2.402], loss: 1.293833, mean_absolute_error: 51.839558, mean_q: 104.335114\n",
            " 30225/50000: episode: 164, duration: 0.708s, episode steps: 187, steps per second: 264, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.321 [-0.948, 2.138], loss: 2.343934, mean_absolute_error: 52.073238, mean_q: 104.772369\n",
            " 30438/50000: episode: 165, duration: 0.815s, episode steps: 213, steps per second: 262, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.304 [-2.417, 0.994], loss: 3.617406, mean_absolute_error: 51.949135, mean_q: 104.580719\n",
            " 30750/50000: episode: 166, duration: 1.254s, episode steps: 312, steps per second: 249, episode reward: 312.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.224 [-1.160, 2.365], loss: 2.138780, mean_absolute_error: 51.750793, mean_q: 104.130272\n",
            " 30975/50000: episode: 167, duration: 0.936s, episode steps: 225, steps per second: 240, episode reward: 225.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.290 [-1.160, 2.284], loss: 3.147074, mean_absolute_error: 51.537994, mean_q: 103.608818\n",
            " 31380/50000: episode: 168, duration: 1.563s, episode steps: 405, steps per second: 259, episode reward: 405.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.176 [-1.123, 2.414], loss: 1.997400, mean_absolute_error: 51.792305, mean_q: 104.226822\n",
            " 31579/50000: episode: 169, duration: 0.758s, episode steps: 199, steps per second: 263, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.314 [-3.154, 1.197], loss: 3.163507, mean_absolute_error: 51.704517, mean_q: 104.142097\n",
            " 31805/50000: episode: 170, duration: 0.847s, episode steps: 226, steps per second: 267, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.270 [-0.986, 2.331], loss: 3.237917, mean_absolute_error: 51.819759, mean_q: 104.238075\n",
            " 32033/50000: episode: 171, duration: 0.891s, episode steps: 228, steps per second: 256, episode reward: 228.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.284 [-1.203, 2.223], loss: 2.860558, mean_absolute_error: 51.813931, mean_q: 104.344795\n",
            " 32216/50000: episode: 172, duration: 0.758s, episode steps: 183, steps per second: 241, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.318 [-1.012, 2.060], loss: 1.135317, mean_absolute_error: 51.854103, mean_q: 104.360588\n",
            " 32495/50000: episode: 173, duration: 1.143s, episode steps: 279, steps per second: 244, episode reward: 279.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.222 [-0.839, 2.089], loss: 3.157974, mean_absolute_error: 51.738407, mean_q: 104.081192\n",
            " 32732/50000: episode: 174, duration: 1.014s, episode steps: 237, steps per second: 234, episode reward: 237.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.286 [-1.147, 2.358], loss: 3.745959, mean_absolute_error: 51.873466, mean_q: 104.241806\n",
            " 32982/50000: episode: 175, duration: 0.954s, episode steps: 250, steps per second: 262, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.283 [-1.019, 2.416], loss: 1.741305, mean_absolute_error: 51.920803, mean_q: 104.363701\n",
            " 33321/50000: episode: 176, duration: 1.335s, episode steps: 339, steps per second: 254, episode reward: 339.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.201 [-2.772, 1.301], loss: 1.651958, mean_absolute_error: 51.961906, mean_q: 104.453606\n",
            " 33588/50000: episode: 177, duration: 1.062s, episode steps: 267, steps per second: 251, episode reward: 267.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.260 [-1.220, 2.432], loss: 1.197022, mean_absolute_error: 51.870766, mean_q: 104.346313\n",
            " 33885/50000: episode: 178, duration: 1.129s, episode steps: 297, steps per second: 263, episode reward: 297.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.219 [-1.058, 2.205], loss: 1.286062, mean_absolute_error: 51.867996, mean_q: 104.293129\n",
            " 34102/50000: episode: 179, duration: 0.895s, episode steps: 217, steps per second: 243, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.282 [-1.074, 2.114], loss: 1.703724, mean_absolute_error: 51.815815, mean_q: 104.116089\n",
            " 34348/50000: episode: 180, duration: 0.951s, episode steps: 246, steps per second: 259, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.279 [-1.112, 2.354], loss: 2.832645, mean_absolute_error: 51.574455, mean_q: 103.678230\n",
            " 34579/50000: episode: 181, duration: 0.920s, episode steps: 231, steps per second: 251, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.261 [-1.055, 2.060], loss: 2.861592, mean_absolute_error: 51.782280, mean_q: 104.189735\n",
            " 34903/50000: episode: 182, duration: 1.246s, episode steps: 324, steps per second: 260, episode reward: 324.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.180 [-2.080, 1.081], loss: 2.490332, mean_absolute_error: 51.684933, mean_q: 103.877365\n",
            " 35303/50000: episode: 183, duration: 1.546s, episode steps: 400, steps per second: 259, episode reward: 400.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.142 [-1.203, 1.852], loss: 2.044717, mean_absolute_error: 51.588974, mean_q: 103.736816\n",
            " 35606/50000: episode: 184, duration: 1.211s, episode steps: 303, steps per second: 250, episode reward: 303.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.233 [-1.233, 2.361], loss: 1.526543, mean_absolute_error: 51.438320, mean_q: 103.498772\n",
            " 35839/50000: episode: 185, duration: 0.875s, episode steps: 233, steps per second: 266, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.266 [-1.270, 2.093], loss: 1.230675, mean_absolute_error: 51.457962, mean_q: 103.538910\n",
            " 36077/50000: episode: 186, duration: 0.877s, episode steps: 238, steps per second: 271, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.259 [-1.176, 2.236], loss: 1.525486, mean_absolute_error: 51.806362, mean_q: 104.193558\n",
            " 36331/50000: episode: 187, duration: 0.963s, episode steps: 254, steps per second: 264, episode reward: 254.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.279 [-1.202, 2.395], loss: 3.152421, mean_absolute_error: 51.408859, mean_q: 103.325630\n",
            " 36545/50000: episode: 188, duration: 0.863s, episode steps: 214, steps per second: 248, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.327 [-1.351, 2.412], loss: 2.753470, mean_absolute_error: 51.693687, mean_q: 103.733116\n",
            " 36742/50000: episode: 189, duration: 0.816s, episode steps: 197, steps per second: 241, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.350 [-1.043, 2.428], loss: 1.195631, mean_absolute_error: 51.481068, mean_q: 103.567024\n",
            " 37033/50000: episode: 190, duration: 1.150s, episode steps: 291, steps per second: 253, episode reward: 291.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.248 [-1.292, 2.436], loss: 2.358481, mean_absolute_error: 51.644962, mean_q: 103.769608\n",
            " 37222/50000: episode: 191, duration: 0.697s, episode steps: 189, steps per second: 271, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.326 [-1.066, 2.156], loss: 1.149521, mean_absolute_error: 51.631302, mean_q: 103.866714\n",
            " 37445/50000: episode: 192, duration: 0.824s, episode steps: 223, steps per second: 271, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.260 [-1.177, 1.982], loss: 1.236458, mean_absolute_error: 51.708519, mean_q: 103.878441\n",
            " 37686/50000: episode: 193, duration: 0.872s, episode steps: 241, steps per second: 277, episode reward: 241.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.281 [-1.183, 2.330], loss: 3.299462, mean_absolute_error: 51.430573, mean_q: 103.252396\n",
            " 38016/50000: episode: 194, duration: 1.235s, episode steps: 330, steps per second: 267, episode reward: 330.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.191 [-1.238, 2.252], loss: 0.911486, mean_absolute_error: 51.138733, mean_q: 102.807457\n",
            " 38255/50000: episode: 195, duration: 0.943s, episode steps: 239, steps per second: 253, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.279 [-1.247, 2.239], loss: 3.073963, mean_absolute_error: 51.339424, mean_q: 103.186081\n",
            " 38466/50000: episode: 196, duration: 0.796s, episode steps: 211, steps per second: 265, episode reward: 211.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.270 [-1.080, 1.923], loss: 1.404183, mean_absolute_error: 51.451496, mean_q: 103.401741\n",
            " 38700/50000: episode: 197, duration: 0.978s, episode steps: 234, steps per second: 239, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.278 [-1.226, 2.205], loss: 2.467521, mean_absolute_error: 51.232254, mean_q: 102.932793\n",
            " 38879/50000: episode: 198, duration: 0.651s, episode steps: 179, steps per second: 275, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.349 [-1.091, 2.185], loss: 1.612066, mean_absolute_error: 51.084648, mean_q: 102.651337\n",
            " 39087/50000: episode: 199, duration: 0.776s, episode steps: 208, steps per second: 268, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.283 [-1.260, 1.968], loss: 1.230290, mean_absolute_error: 51.330872, mean_q: 103.167000\n",
            " 39333/50000: episode: 200, duration: 0.919s, episode steps: 246, steps per second: 268, episode reward: 246.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.265 [-1.212, 2.143], loss: 1.221078, mean_absolute_error: 51.322243, mean_q: 103.134750\n",
            " 39548/50000: episode: 201, duration: 0.828s, episode steps: 215, steps per second: 260, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.294 [-1.123, 2.161], loss: 2.520517, mean_absolute_error: 51.302845, mean_q: 103.085411\n",
            " 39778/50000: episode: 202, duration: 0.910s, episode steps: 230, steps per second: 253, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.274 [-0.864, 2.125], loss: 1.868692, mean_absolute_error: 51.021858, mean_q: 102.547653\n",
            " 40013/50000: episode: 203, duration: 0.868s, episode steps: 235, steps per second: 271, episode reward: 235.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.292 [-1.376, 2.260], loss: 1.621167, mean_absolute_error: 51.179920, mean_q: 102.825157\n",
            " 40247/50000: episode: 204, duration: 0.968s, episode steps: 234, steps per second: 242, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.267 [-0.900, 2.079], loss: 1.643423, mean_absolute_error: 51.060585, mean_q: 102.720985\n",
            " 40541/50000: episode: 205, duration: 1.097s, episode steps: 294, steps per second: 268, episode reward: 294.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.231 [-1.326, 2.202], loss: 1.209449, mean_absolute_error: 51.218143, mean_q: 102.913574\n",
            " 40807/50000: episode: 206, duration: 1.021s, episode steps: 266, steps per second: 260, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.217 [-1.133, 2.317], loss: 2.050461, mean_absolute_error: 51.229500, mean_q: 102.916542\n",
            " 41068/50000: episode: 207, duration: 0.989s, episode steps: 261, steps per second: 264, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.275 [-1.248, 2.423], loss: 1.116835, mean_absolute_error: 51.268463, mean_q: 103.057243\n",
            " 41334/50000: episode: 208, duration: 1.040s, episode steps: 266, steps per second: 256, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.238 [-1.257, 2.102], loss: 2.261145, mean_absolute_error: 51.303070, mean_q: 103.045937\n",
            " 41541/50000: episode: 209, duration: 0.835s, episode steps: 207, steps per second: 248, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.329 [-1.239, 2.337], loss: 1.033672, mean_absolute_error: 51.039383, mean_q: 102.716675\n",
            " 41813/50000: episode: 210, duration: 1.108s, episode steps: 272, steps per second: 245, episode reward: 272.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.203 [-1.265, 2.503], loss: 3.115527, mean_absolute_error: 51.012024, mean_q: 102.522415\n",
            " 42004/50000: episode: 211, duration: 0.736s, episode steps: 191, steps per second: 259, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.305 [-1.140, 2.004], loss: 1.071068, mean_absolute_error: 51.180740, mean_q: 102.818413\n",
            " 42320/50000: episode: 212, duration: 1.200s, episode steps: 316, steps per second: 263, episode reward: 316.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.182 [-1.386, 2.558], loss: 0.978030, mean_absolute_error: 51.174244, mean_q: 102.893639\n",
            " 42613/50000: episode: 213, duration: 1.109s, episode steps: 293, steps per second: 264, episode reward: 293.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.240 [-1.371, 2.352], loss: 1.967868, mean_absolute_error: 51.323914, mean_q: 103.116348\n",
            " 42844/50000: episode: 214, duration: 0.928s, episode steps: 231, steps per second: 249, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.289 [-1.261, 2.278], loss: 1.604330, mean_absolute_error: 51.324345, mean_q: 103.056190\n",
            " 43071/50000: episode: 215, duration: 0.938s, episode steps: 227, steps per second: 242, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.244 [-1.235, 1.872], loss: 1.614339, mean_absolute_error: 50.920315, mean_q: 102.309334\n",
            " 43298/50000: episode: 216, duration: 0.900s, episode steps: 227, steps per second: 252, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.271 [-0.956, 2.160], loss: 3.867254, mean_absolute_error: 51.126675, mean_q: 102.568466\n",
            " 43520/50000: episode: 217, duration: 4.244s, episode steps: 222, steps per second: 52, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.245 [-1.167, 1.875], loss: 1.471492, mean_absolute_error: 51.399010, mean_q: 103.316338\n",
            " 43837/50000: episode: 218, duration: 1.496s, episode steps: 317, steps per second: 212, episode reward: 317.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.175 [-1.182, 1.809], loss: 1.035022, mean_absolute_error: 50.896893, mean_q: 102.290169\n",
            " 44157/50000: episode: 219, duration: 1.340s, episode steps: 320, steps per second: 239, episode reward: 320.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.206 [-1.309, 2.158], loss: 3.108525, mean_absolute_error: 50.972519, mean_q: 102.445084\n",
            " 44391/50000: episode: 220, duration: 0.958s, episode steps: 234, steps per second: 244, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.210 [-1.401, 2.735], loss: 1.630728, mean_absolute_error: 50.810581, mean_q: 102.043465\n",
            " 44569/50000: episode: 221, duration: 0.708s, episode steps: 178, steps per second: 252, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.354 [-1.255, 2.200], loss: 1.853710, mean_absolute_error: 50.227924, mean_q: 100.893456\n",
            " 44757/50000: episode: 222, duration: 0.837s, episode steps: 188, steps per second: 225, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.281 [-0.933, 1.824], loss: 1.011168, mean_absolute_error: 50.972240, mean_q: 102.439621\n",
            " 44969/50000: episode: 223, duration: 0.990s, episode steps: 212, steps per second: 214, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.237 [-1.245, 2.224], loss: 1.303621, mean_absolute_error: 50.439449, mean_q: 101.322006\n",
            " 45250/50000: episode: 224, duration: 1.080s, episode steps: 281, steps per second: 260, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.235 [-1.186, 2.163], loss: 1.338027, mean_absolute_error: 50.722641, mean_q: 101.870987\n",
            " 45499/50000: episode: 225, duration: 0.937s, episode steps: 249, steps per second: 266, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.226 [-1.310, 2.153], loss: 1.450214, mean_absolute_error: 50.500504, mean_q: 101.459488\n",
            " 45750/50000: episode: 226, duration: 0.942s, episode steps: 251, steps per second: 267, episode reward: 251.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.262 [-0.805, 2.190], loss: 1.051774, mean_absolute_error: 50.774387, mean_q: 102.083153\n",
            " 46069/50000: episode: 227, duration: 1.182s, episode steps: 319, steps per second: 270, episode reward: 319.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.181 [-1.558, 2.063], loss: 2.308935, mean_absolute_error: 50.661755, mean_q: 101.728020\n",
            " 46458/50000: episode: 228, duration: 1.524s, episode steps: 389, steps per second: 255, episode reward: 389.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.169 [-1.269, 2.208], loss: 1.466213, mean_absolute_error: 50.405663, mean_q: 101.254013\n",
            " 46873/50000: episode: 229, duration: 1.624s, episode steps: 415, steps per second: 256, episode reward: 415.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.132 [-1.141, 2.270], loss: 1.129021, mean_absolute_error: 50.214325, mean_q: 100.894920\n",
            " 47103/50000: episode: 230, duration: 0.851s, episode steps: 230, steps per second: 270, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.242 [-0.991, 1.999], loss: 2.200286, mean_absolute_error: 50.293488, mean_q: 100.925713\n",
            " 47495/50000: episode: 231, duration: 1.433s, episode steps: 392, steps per second: 274, episode reward: 392.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.150 [-1.242, 2.629], loss: 1.229033, mean_absolute_error: 49.984055, mean_q: 100.389420\n",
            " 47698/50000: episode: 232, duration: 0.774s, episode steps: 203, steps per second: 262, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.257 [-1.112, 1.831], loss: 1.981397, mean_absolute_error: 50.228371, mean_q: 100.784531\n",
            " 48007/50000: episode: 233, duration: 1.167s, episode steps: 309, steps per second: 265, episode reward: 309.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.169 [-1.473, 1.812], loss: 2.212928, mean_absolute_error: 49.849426, mean_q: 100.129639\n",
            " 48201/50000: episode: 234, duration: 0.766s, episode steps: 194, steps per second: 253, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.264 [-1.201, 2.523], loss: 1.665004, mean_absolute_error: 49.812305, mean_q: 99.922844\n",
            " 48424/50000: episode: 235, duration: 0.899s, episode steps: 223, steps per second: 248, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.266 [-1.068, 2.062], loss: 2.226376, mean_absolute_error: 49.651360, mean_q: 99.623985\n",
            " 48687/50000: episode: 236, duration: 0.959s, episode steps: 263, steps per second: 274, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.206 [-1.061, 2.199], loss: 1.254763, mean_absolute_error: 49.869595, mean_q: 100.137642\n",
            " 48913/50000: episode: 237, duration: 0.857s, episode steps: 226, steps per second: 264, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.252 [-1.728, 2.722], loss: 1.537652, mean_absolute_error: 49.769497, mean_q: 99.867813\n",
            " 49156/50000: episode: 238, duration: 0.900s, episode steps: 243, steps per second: 270, episode reward: 243.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.290 [-1.627, 2.400], loss: 1.396736, mean_absolute_error: 49.653759, mean_q: 99.684845\n",
            " 49615/50000: episode: 239, duration: 1.803s, episode steps: 459, steps per second: 255, episode reward: 459.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.120 [-1.509, 1.801], loss: 1.759541, mean_absolute_error: 49.707695, mean_q: 99.811447\n",
            " 49968/50000: episode: 240, duration: 1.347s, episode steps: 353, steps per second: 262, episode reward: 353.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.501 [0.000, 1.000], mean observation: 0.174 [-1.558, 2.499], loss: 1.021897, mean_absolute_error: 49.457218, mean_q: 99.408287\n",
            "done, took 210.531 seconds\n",
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-27857ca6a93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'duel_dqn_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mENV_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_weights.h5f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for CartPole-v1, you cannot call reset() unless the episode is over."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0f2JkGZvwzo",
        "colab_type": "code",
        "outputId": "94cedfea-eea7-423b-eea2-2ae187a8916a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "# Reexecute this cell after crash.\n",
        "dqn.done = True\n",
        "show_video()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                                    loop controls style=\"height: 400px;\">\n",
              "                                    <source src=\"data:video/mp4;base64,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\" \n",
              "                                    type=\"video/mp4\" /> </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}