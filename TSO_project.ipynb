{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSO_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abstractguy/TSO_project/blob/master/TSO_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-AxnvAVyzQQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df9273be-1578-4f49-e086-d64211e8ccba"
      },
      "source": [
        "!pip install gym pyvirtualdisplay pyglet==1.3.2 keras-rl\n",
        "!apt-get install -y xvfb python-opengl ffmpeg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.0MB/s \n",
            "\u001b[?25hCollecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 19.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.4)\n",
            "Collecting EasyProcess (from pyvirtualdisplay)\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Building wheels for collected packages: keras-rl\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48380 sha256=9b0d9e85f03e8e64d37e80ebe5484bb198e4941d296cccc145338197e53b41d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built keras-rl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay, pyglet, keras-rl\n",
            "  Found existing installation: pyglet 1.4.1\n",
            "    Uninstalling pyglet-1.4.1:\n",
            "      Successfully uninstalled pyglet-1.4.1\n",
            "Successfully installed EasyProcess-0.2.7 keras-rl-0.4.2 pyglet-1.3.2 pyvirtualdisplay-0.2.4\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,682 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 1,280 kB in 1s (1,288 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "c9d851f4-361f-4119-c7ca-6bd243bb2823"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install cmake\n",
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup\n",
        "!pip install gym[atari]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.176)] [1 InRelease 14.2 kB/88.7\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.176)] [1 InRelease 14.2 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.176)] [1 InRelease 14.2 kB/88.7\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.176)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [1 InRelease 46.0 kB/88.7 k\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\r                                                                         \rHit:6 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Conn\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Connecting to ppa.launchpa\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [760 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [628 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [926 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,279 kB]\n",
            "Fetched 3,849 kB in 3s (1,502 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 12 not upgraded.\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.2.0)\n",
            "Collecting ez_setup\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/2c/743df41bd6b3298706dfe91b0c7ecdc47f2dc1a3104abeb6e9aa4a45fa5d/ez_setup-0.9.tar.gz\n",
            "Building wheels for collected packages: ez-setup\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez-setup: filename=ez_setup-0.9-cp36-none-any.whl size=11014 sha256=52f4af59bd2ebf68264e778d641f71e54452602a396826b42c2a8a93a6603caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e8/6b/3d5ff5a3efd7b5338d1e173ac981771e2628ceb2f7866d49ad\n",
            "Successfully built ez-setup\n",
            "Installing collected packages: ez-setup\n",
            "Successfully installed ez-setup-0.9\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.15)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "5a076e16-cf20-4feb-cdae-0d202904aec7"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0829 15:06:56.797302 140579198191488 abstractdisplay.py:151] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THd_6WRUw3GL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3b34e528-7cc7-4895-a4e1-3fdf510f1f5a"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "\n",
        "# Get the environment and extract the number of actions.\n",
        "env = wrap_env(gym.make(ENV_NAME))\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "# Next, we build a very simple model.\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions))\n",
        "model.add(Activation('linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()\n",
        "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
        "               target_model_update=1e-2, policy=policy)\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "dqn.fit(env, nb_steps=50000, visualize=True, verbose=2)\n",
        "\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)\n",
        "\n",
        "dqn.test(env, nb_episodes=5, visualize=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "W0829 15:06:57.952326 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0829 15:06:58.038553 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0829 15:06:58.079318 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0829 15:06:58.230731 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0829 15:06:58.232044 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 658\n",
            "Trainable params: 658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0829 15:06:58.590951 140579198191488 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    31/50000: episode: 1, duration: 4.510s, episode steps: 31, steps per second: 7, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.013 [-1.185, 1.776], loss: 0.462303, mean_absolute_error: 0.519564, mean_q: 0.093226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    44/50000: episode: 2, duration: 0.398s, episode steps: 13, steps per second: 33, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.069 [-1.867, 1.224], loss: 0.354684, mean_absolute_error: 0.542652, mean_q: 0.285883\n",
            "    64/50000: episode: 3, duration: 0.178s, episode steps: 20, steps per second: 112, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.070 [-1.375, 0.833], loss: 0.230246, mean_absolute_error: 0.553541, mean_q: 0.488437\n",
            "    83/50000: episode: 4, duration: 0.124s, episode steps: 19, steps per second: 153, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.074 [-0.817, 1.505], loss: 0.119990, mean_absolute_error: 0.599525, mean_q: 0.793231\n",
            "    98/50000: episode: 5, duration: 0.090s, episode steps: 15, steps per second: 167, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.368, 0.825], loss: 0.063130, mean_absolute_error: 0.687666, mean_q: 1.146067\n",
            "   114/50000: episode: 6, duration: 0.098s, episode steps: 16, steps per second: 164, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.072 [-3.056, 1.962], loss: 0.034851, mean_absolute_error: 0.708807, mean_q: 1.280957\n",
            "   128/50000: episode: 7, duration: 0.090s, episode steps: 14, steps per second: 155, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.111 [-0.934, 1.483], loss: 0.025891, mean_absolute_error: 0.747921, mean_q: 1.439131\n",
            "   151/50000: episode: 8, duration: 0.128s, episode steps: 23, steps per second: 179, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.050 [-1.760, 2.679], loss: 0.025296, mean_absolute_error: 0.821410, mean_q: 1.619317\n",
            "   162/50000: episode: 9, duration: 0.286s, episode steps: 11, steps per second: 38, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.133 [-1.133, 1.962], loss: 0.046176, mean_absolute_error: 0.905821, mean_q: 1.776260\n",
            "   175/50000: episode: 10, duration: 0.117s, episode steps: 13, steps per second: 111, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.118 [-2.439, 1.549], loss: 0.037527, mean_absolute_error: 0.929243, mean_q: 1.820055\n",
            "   201/50000: episode: 11, duration: 0.156s, episode steps: 26, steps per second: 166, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.346 [0.000, 1.000], mean observation: 0.032 [-1.588, 2.376], loss: 0.032762, mean_absolute_error: 0.985946, mean_q: 2.004984\n",
            "   259/50000: episode: 12, duration: 0.324s, episode steps: 58, steps per second: 179, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-1.274, 1.190], loss: 0.066327, mean_absolute_error: 1.157219, mean_q: 2.287825\n",
            "   292/50000: episode: 13, duration: 0.185s, episode steps: 33, steps per second: 179, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: 0.067 [-1.337, 2.339], loss: 0.058426, mean_absolute_error: 1.314123, mean_q: 2.606223\n",
            "   333/50000: episode: 14, duration: 0.236s, episode steps: 41, steps per second: 174, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.415 [0.000, 1.000], mean observation: -0.043 [-1.709, 1.982], loss: 0.082315, mean_absolute_error: 1.493731, mean_q: 2.920005\n",
            "   348/50000: episode: 15, duration: 0.089s, episode steps: 15, steps per second: 169, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.083 [-0.795, 1.373], loss: 0.078099, mean_absolute_error: 1.636086, mean_q: 3.221624\n",
            "   379/50000: episode: 16, duration: 0.189s, episode steps: 31, steps per second: 164, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.065 [-1.104, 0.598], loss: 0.117554, mean_absolute_error: 1.714792, mean_q: 3.354098\n",
            "   414/50000: episode: 17, duration: 0.189s, episode steps: 35, steps per second: 185, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.092 [-0.765, 1.191], loss: 0.106388, mean_absolute_error: 1.867712, mean_q: 3.638321\n",
            "   432/50000: episode: 18, duration: 0.108s, episode steps: 18, steps per second: 166, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.076 [-1.578, 2.541], loss: 0.119937, mean_absolute_error: 1.975284, mean_q: 3.845428\n",
            "   444/50000: episode: 19, duration: 0.069s, episode steps: 12, steps per second: 173, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.089 [-2.546, 1.593], loss: 0.190687, mean_absolute_error: 2.089602, mean_q: 4.058412\n",
            "   465/50000: episode: 20, duration: 0.116s, episode steps: 21, steps per second: 181, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.079 [-0.598, 1.298], loss: 0.141121, mean_absolute_error: 2.116169, mean_q: 4.101525\n",
            "   494/50000: episode: 21, duration: 0.165s, episode steps: 29, steps per second: 175, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.125 [-0.376, 0.847], loss: 0.146516, mean_absolute_error: 2.232996, mean_q: 4.298965\n",
            "   521/50000: episode: 22, duration: 0.145s, episode steps: 27, steps per second: 186, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.032 [-1.374, 1.985], loss: 0.221611, mean_absolute_error: 2.344919, mean_q: 4.482502\n",
            "   545/50000: episode: 23, duration: 0.148s, episode steps: 24, steps per second: 162, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.104 [-0.959, 1.957], loss: 0.146629, mean_absolute_error: 2.447093, mean_q: 4.758866\n",
            "   560/50000: episode: 24, duration: 0.092s, episode steps: 15, steps per second: 163, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.093 [-1.358, 2.226], loss: 0.170059, mean_absolute_error: 2.521801, mean_q: 4.886302\n",
            "   576/50000: episode: 25, duration: 0.099s, episode steps: 16, steps per second: 161, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.066 [-0.798, 1.227], loss: 0.265078, mean_absolute_error: 2.565448, mean_q: 4.863560\n",
            "   599/50000: episode: 26, duration: 0.142s, episode steps: 23, steps per second: 162, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.055 [-1.624, 0.999], loss: 0.304851, mean_absolute_error: 2.650094, mean_q: 4.957744\n",
            "   612/50000: episode: 27, duration: 0.085s, episode steps: 13, steps per second: 153, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.124 [-1.739, 0.975], loss: 0.199894, mean_absolute_error: 2.712748, mean_q: 5.176043\n",
            "   628/50000: episode: 28, duration: 0.365s, episode steps: 16, steps per second: 44, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.083 [-1.234, 2.172], loss: 0.244055, mean_absolute_error: 2.749502, mean_q: 5.224127\n",
            "   647/50000: episode: 29, duration: 0.179s, episode steps: 19, steps per second: 106, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.059 [-1.064, 0.639], loss: 0.236412, mean_absolute_error: 2.805452, mean_q: 5.339579\n",
            "   676/50000: episode: 30, duration: 0.176s, episode steps: 29, steps per second: 165, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.080 [-0.566, 1.470], loss: 0.259353, mean_absolute_error: 2.909014, mean_q: 5.553969\n",
            "   699/50000: episode: 31, duration: 0.139s, episode steps: 23, steps per second: 166, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.042 [-1.010, 1.532], loss: 0.275609, mean_absolute_error: 3.007346, mean_q: 5.750638\n",
            "   708/50000: episode: 32, duration: 0.056s, episode steps: 9, steps per second: 161, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.114 [-1.219, 1.872], loss: 0.274956, mean_absolute_error: 3.062017, mean_q: 5.869633\n",
            "   726/50000: episode: 33, duration: 0.105s, episode steps: 18, steps per second: 171, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-1.238, 0.806], loss: 0.243800, mean_absolute_error: 3.147195, mean_q: 6.066792\n",
            "   753/50000: episode: 34, duration: 0.149s, episode steps: 27, steps per second: 181, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.407 [0.000, 1.000], mean observation: 0.078 [-0.942, 1.877], loss: 0.242902, mean_absolute_error: 3.195193, mean_q: 6.238541\n",
            "   788/50000: episode: 35, duration: 0.205s, episode steps: 35, steps per second: 171, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.115 [-0.346, 1.070], loss: 0.247224, mean_absolute_error: 3.334018, mean_q: 6.503189\n",
            "   814/50000: episode: 36, duration: 0.185s, episode steps: 26, steps per second: 140, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-1.582, 0.959], loss: 0.372040, mean_absolute_error: 3.454742, mean_q: 6.664027\n",
            "   823/50000: episode: 37, duration: 0.063s, episode steps: 9, steps per second: 142, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-1.951, 1.194], loss: 0.262862, mean_absolute_error: 3.594791, mean_q: 7.024075\n",
            "   832/50000: episode: 38, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.145 [-1.354, 2.301], loss: 0.346145, mean_absolute_error: 3.530308, mean_q: 6.815420\n",
            "   849/50000: episode: 39, duration: 0.093s, episode steps: 17, steps per second: 182, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.100 [-0.914, 0.583], loss: 0.299358, mean_absolute_error: 3.632360, mean_q: 7.106375\n",
            "   879/50000: episode: 40, duration: 0.172s, episode steps: 30, steps per second: 175, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.100 [-0.599, 0.985], loss: 0.440886, mean_absolute_error: 3.743880, mean_q: 7.213218\n",
            "   951/50000: episode: 41, duration: 0.401s, episode steps: 72, steps per second: 180, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-1.284, 1.537], loss: 0.361475, mean_absolute_error: 3.943714, mean_q: 7.718813\n",
            "   975/50000: episode: 42, duration: 0.142s, episode steps: 24, steps per second: 168, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.058 [-0.847, 1.630], loss: 0.420610, mean_absolute_error: 4.154863, mean_q: 8.148604\n",
            "  1003/50000: episode: 43, duration: 0.174s, episode steps: 28, steps per second: 161, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: 0.038 [-1.804, 1.394], loss: 0.389469, mean_absolute_error: 4.227808, mean_q: 8.326131\n",
            "  1080/50000: episode: 44, duration: 0.440s, episode steps: 77, steps per second: 175, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.036 [-1.193, 1.184], loss: 0.432401, mean_absolute_error: 4.460099, mean_q: 8.810246\n",
            "  1143/50000: episode: 45, duration: 0.342s, episode steps: 63, steps per second: 184, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.182 [-1.085, 1.326], loss: 0.455394, mean_absolute_error: 4.747056, mean_q: 9.403994\n",
            "  1251/50000: episode: 46, duration: 0.567s, episode steps: 108, steps per second: 190, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.243 [-0.935, 1.417], loss: 0.533734, mean_absolute_error: 5.082722, mean_q: 10.050126\n",
            "  1283/50000: episode: 47, duration: 0.162s, episode steps: 32, steps per second: 197, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: -0.027 [-2.145, 1.226], loss: 0.697579, mean_absolute_error: 5.377717, mean_q: 10.596123\n",
            "  1359/50000: episode: 48, duration: 0.416s, episode steps: 76, steps per second: 183, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: -0.057 [-1.206, 0.775], loss: 0.429221, mean_absolute_error: 5.571720, mean_q: 11.069766\n",
            "  1429/50000: episode: 49, duration: 0.393s, episode steps: 70, steps per second: 178, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.543 [0.000, 1.000], mean observation: 0.264 [-0.708, 1.466], loss: 0.429375, mean_absolute_error: 5.871118, mean_q: 11.821609\n",
            "  1518/50000: episode: 50, duration: 0.507s, episode steps: 89, steps per second: 176, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.093 [-1.357, 1.353], loss: 0.558316, mean_absolute_error: 6.215071, mean_q: 12.496807\n",
            "  1622/50000: episode: 51, duration: 0.618s, episode steps: 104, steps per second: 168, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.297 [-1.451, 0.655], loss: 0.517686, mean_absolute_error: 6.718000, mean_q: 13.498341\n",
            "  1751/50000: episode: 52, duration: 0.747s, episode steps: 129, steps per second: 173, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.274 [-1.518, 0.755], loss: 0.807518, mean_absolute_error: 7.225848, mean_q: 14.556651\n",
            "  1938/50000: episode: 53, duration: 1.038s, episode steps: 187, steps per second: 180, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.146 [-1.275, 1.458], loss: 0.832581, mean_absolute_error: 7.997337, mean_q: 16.167747\n",
            "  2138/50000: episode: 54, duration: 1.154s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.093 [-1.093, 1.298], loss: 0.908237, mean_absolute_error: 8.903518, mean_q: 18.020214\n",
            "  2273/50000: episode: 55, duration: 0.753s, episode steps: 135, steps per second: 179, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.181 [-1.011, 0.975], loss: 1.009981, mean_absolute_error: 9.614713, mean_q: 19.466536\n",
            "  2452/50000: episode: 56, duration: 1.079s, episode steps: 179, steps per second: 166, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.208 [-1.512, 1.040], loss: 1.208304, mean_absolute_error: 10.331330, mean_q: 20.892380\n",
            "  2590/50000: episode: 57, duration: 0.778s, episode steps: 138, steps per second: 177, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.170 [-1.032, 1.539], loss: 1.286882, mean_absolute_error: 11.089697, mean_q: 22.444788\n",
            "  2790/50000: episode: 58, duration: 1.057s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.020 [-1.042, 1.158], loss: 1.039131, mean_absolute_error: 11.931875, mean_q: 24.233179\n",
            "  2990/50000: episode: 59, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.102 [-0.856, 1.457], loss: 1.259207, mean_absolute_error: 12.868592, mean_q: 26.113001\n",
            "  3190/50000: episode: 60, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.961, 0.747], loss: 1.620404, mean_absolute_error: 13.889060, mean_q: 28.122652\n",
            "  3390/50000: episode: 61, duration: 0.988s, episode steps: 200, steps per second: 202, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.123 [-1.134, 0.745], loss: 1.716362, mean_absolute_error: 14.713958, mean_q: 29.853802\n",
            "  3590/50000: episode: 62, duration: 0.993s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.300 [-2.173, 0.699], loss: 2.298104, mean_absolute_error: 15.805367, mean_q: 31.979548\n",
            "  3790/50000: episode: 63, duration: 0.972s, episode steps: 200, steps per second: 206, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.255 [-1.814, 0.778], loss: 2.424087, mean_absolute_error: 16.664391, mean_q: 33.700241\n",
            "  3990/50000: episode: 64, duration: 0.951s, episode steps: 200, steps per second: 210, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.188 [-1.335, 0.914], loss: 2.620118, mean_absolute_error: 17.570913, mean_q: 35.527000\n",
            "  4183/50000: episode: 65, duration: 3.175s, episode steps: 193, steps per second: 61, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.362 [-2.434, 0.770], loss: 2.706080, mean_absolute_error: 18.377388, mean_q: 37.132942\n",
            "  4383/50000: episode: 66, duration: 1.082s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.271 [-1.873, 1.075], loss: 2.857142, mean_absolute_error: 19.184649, mean_q: 38.788342\n",
            "  4583/50000: episode: 67, duration: 1.016s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.255 [-1.739, 0.684], loss: 2.378964, mean_absolute_error: 20.054018, mean_q: 40.528980\n",
            "  4783/50000: episode: 68, duration: 1.019s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.264 [-0.957, 2.210], loss: 2.896694, mean_absolute_error: 20.911247, mean_q: 42.242558\n",
            "  4983/50000: episode: 69, duration: 0.976s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.231 [-1.575, 0.715], loss: 2.888776, mean_absolute_error: 21.505156, mean_q: 43.440693\n",
            "  5183/50000: episode: 70, duration: 0.982s, episode steps: 200, steps per second: 204, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.121 [-0.970, 1.322], loss: 2.935987, mean_absolute_error: 22.280649, mean_q: 45.011753\n",
            "  5348/50000: episode: 71, duration: 0.875s, episode steps: 165, steps per second: 188, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.312 [-0.972, 1.972], loss: 3.280063, mean_absolute_error: 22.922531, mean_q: 46.328110\n",
            "  5548/50000: episode: 72, duration: 1.013s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.303 [-0.811, 2.598], loss: 3.504288, mean_absolute_error: 23.422169, mean_q: 47.317314\n",
            "  5748/50000: episode: 73, duration: 0.996s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.210 [-1.413, 0.829], loss: 3.337350, mean_absolute_error: 24.127823, mean_q: 48.738461\n",
            "  5931/50000: episode: 74, duration: 0.903s, episode steps: 183, steps per second: 203, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.362 [-0.926, 2.423], loss: 2.911250, mean_absolute_error: 24.823746, mean_q: 50.100323\n",
            "  6131/50000: episode: 75, duration: 1.013s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.301 [-0.977, 2.210], loss: 3.527593, mean_absolute_error: 25.296724, mean_q: 51.112568\n",
            "  6331/50000: episode: 76, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.293 [-0.927, 2.258], loss: 3.446119, mean_absolute_error: 25.686373, mean_q: 51.978851\n",
            "  6531/50000: episode: 77, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.081 [-0.729, 0.691], loss: 4.221788, mean_absolute_error: 26.419884, mean_q: 53.393333\n",
            "  6728/50000: episode: 78, duration: 1.063s, episode steps: 197, steps per second: 185, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.339 [-1.133, 2.433], loss: 3.181551, mean_absolute_error: 27.028379, mean_q: 54.662624\n",
            "  6928/50000: episode: 79, duration: 1.057s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.094 [-0.901, 0.793], loss: 4.876036, mean_absolute_error: 27.675722, mean_q: 55.898106\n",
            "  7128/50000: episode: 80, duration: 1.019s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.160 [-1.445, 0.737], loss: 4.701143, mean_absolute_error: 28.188139, mean_q: 56.926815\n",
            "  7328/50000: episode: 81, duration: 1.036s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.203 [-0.757, 1.503], loss: 5.351638, mean_absolute_error: 28.750317, mean_q: 57.990036\n",
            "  7528/50000: episode: 82, duration: 1.042s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.159 [-1.088, 0.856], loss: 4.433212, mean_absolute_error: 29.380806, mean_q: 59.393715\n",
            "  7728/50000: episode: 83, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.129 [-0.797, 1.286], loss: 4.819334, mean_absolute_error: 30.000504, mean_q: 60.522163\n",
            "  7928/50000: episode: 84, duration: 1.044s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.198 [-0.768, 1.438], loss: 5.311421, mean_absolute_error: 30.613758, mean_q: 61.773506\n",
            "  8128/50000: episode: 85, duration: 1.048s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.330 [-0.923, 2.376], loss: 3.914510, mean_absolute_error: 30.949102, mean_q: 62.518574\n",
            "  8328/50000: episode: 86, duration: 1.045s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.242 [-0.859, 2.230], loss: 3.691355, mean_absolute_error: 31.137758, mean_q: 62.875401\n",
            "  8528/50000: episode: 87, duration: 0.999s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.038 [-1.039, 1.003], loss: 4.611490, mean_absolute_error: 31.541698, mean_q: 63.552235\n",
            "  8728/50000: episode: 88, duration: 0.987s, episode steps: 200, steps per second: 203, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.125 [-0.765, 1.091], loss: 4.687410, mean_absolute_error: 32.312222, mean_q: 65.114647\n",
            "  8927/50000: episode: 89, duration: 0.959s, episode steps: 199, steps per second: 207, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.334 [-0.881, 2.551], loss: 4.454623, mean_absolute_error: 32.622681, mean_q: 65.802292\n",
            "  9127/50000: episode: 90, duration: 1.020s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.050 [-0.730, 0.930], loss: 5.144100, mean_absolute_error: 33.073730, mean_q: 66.671425\n",
            "  9327/50000: episode: 91, duration: 1.004s, episode steps: 200, steps per second: 199, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.081 [-0.709, 0.796], loss: 4.105216, mean_absolute_error: 33.824276, mean_q: 68.261017\n",
            "  9527/50000: episode: 92, duration: 1.169s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.752, 0.920], loss: 5.369084, mean_absolute_error: 34.374222, mean_q: 69.307144\n",
            "  9727/50000: episode: 93, duration: 1.155s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.046 [-1.053, 0.772], loss: 4.568801, mean_absolute_error: 34.751595, mean_q: 70.134651\n",
            "  9927/50000: episode: 94, duration: 1.116s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.812, 0.631], loss: 5.700551, mean_absolute_error: 35.203316, mean_q: 70.954880\n",
            " 10079/50000: episode: 95, duration: 0.817s, episode steps: 152, steps per second: 186, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.282 [-0.883, 1.538], loss: 7.249463, mean_absolute_error: 35.410843, mean_q: 71.364235\n",
            " 10279/50000: episode: 96, duration: 1.047s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.281 [-0.724, 2.065], loss: 4.514543, mean_absolute_error: 35.756691, mean_q: 72.217323\n",
            " 10479/50000: episode: 97, duration: 1.025s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.140 [-1.331, 0.971], loss: 9.000667, mean_absolute_error: 36.067825, mean_q: 72.583771\n",
            " 10679/50000: episode: 98, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.193 [-0.929, 1.512], loss: 6.942512, mean_absolute_error: 36.270397, mean_q: 73.073990\n",
            " 10872/50000: episode: 99, duration: 1.033s, episode steps: 193, steps per second: 187, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.336 [-0.746, 2.420], loss: 5.029040, mean_absolute_error: 36.460693, mean_q: 73.489670\n",
            " 11072/50000: episode: 100, duration: 1.118s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.151 [-1.161, 0.751], loss: 4.885128, mean_absolute_error: 36.718903, mean_q: 74.096092\n",
            " 11272/50000: episode: 101, duration: 1.009s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.121 [-1.110, 0.724], loss: 8.279363, mean_absolute_error: 37.085251, mean_q: 74.606369\n",
            " 11472/50000: episode: 102, duration: 1.045s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.650, 0.638], loss: 7.059239, mean_absolute_error: 37.272446, mean_q: 74.950882\n",
            " 11672/50000: episode: 103, duration: 1.082s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.177 [-0.776, 1.316], loss: 7.628345, mean_absolute_error: 37.813457, mean_q: 76.078560\n",
            " 11872/50000: episode: 104, duration: 1.113s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.278 [-0.674, 2.063], loss: 5.461517, mean_absolute_error: 38.032936, mean_q: 76.643196\n",
            " 12072/50000: episode: 105, duration: 1.008s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.624, 0.899], loss: 5.345076, mean_absolute_error: 38.061935, mean_q: 76.773361\n",
            " 12272/50000: episode: 106, duration: 1.021s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.181 [-0.768, 1.655], loss: 5.661619, mean_absolute_error: 38.561375, mean_q: 77.737259\n",
            " 12466/50000: episode: 107, duration: 0.983s, episode steps: 194, steps per second: 197, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.332 [-0.935, 2.419], loss: 4.740283, mean_absolute_error: 38.753452, mean_q: 78.147766\n",
            " 12641/50000: episode: 108, duration: 0.860s, episode steps: 175, steps per second: 204, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.359 [-0.661, 2.619], loss: 6.643947, mean_absolute_error: 38.799419, mean_q: 78.157440\n",
            " 12841/50000: episode: 109, duration: 1.008s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.249 [-0.768, 1.868], loss: 6.166136, mean_absolute_error: 38.972908, mean_q: 78.478561\n",
            " 13041/50000: episode: 110, duration: 1.030s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.152 [-0.812, 1.303], loss: 7.093404, mean_absolute_error: 39.085712, mean_q: 78.763153\n",
            " 13241/50000: episode: 111, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.102 [-0.574, 0.996], loss: 5.637414, mean_absolute_error: 39.513634, mean_q: 79.580559\n",
            " 13441/50000: episode: 112, duration: 1.086s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.175 [-0.720, 1.309], loss: 6.033208, mean_absolute_error: 39.565285, mean_q: 79.570625\n",
            " 13608/50000: episode: 113, duration: 0.852s, episode steps: 167, steps per second: 196, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.394 [-0.900, 2.401], loss: 6.413595, mean_absolute_error: 39.705399, mean_q: 79.918091\n",
            " 13808/50000: episode: 114, duration: 1.020s, episode steps: 200, steps per second: 196, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.145 [-0.764, 1.260], loss: 7.203520, mean_absolute_error: 39.626350, mean_q: 79.734291\n",
            " 14008/50000: episode: 115, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.196 [-0.688, 1.417], loss: 6.198714, mean_absolute_error: 40.228535, mean_q: 80.941452\n",
            " 14196/50000: episode: 116, duration: 1.046s, episode steps: 188, steps per second: 180, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.357 [-0.910, 2.431], loss: 7.596766, mean_absolute_error: 39.969090, mean_q: 80.467651\n",
            " 14396/50000: episode: 117, duration: 1.079s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.129 [-1.554, 1.299], loss: 6.177517, mean_absolute_error: 40.294609, mean_q: 81.021141\n",
            " 14596/50000: episode: 118, duration: 1.068s, episode steps: 200, steps per second: 187, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.164 [-1.725, 1.394], loss: 3.616638, mean_absolute_error: 40.250546, mean_q: 81.129074\n",
            " 14796/50000: episode: 119, duration: 1.088s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.295 [-2.611, 1.632], loss: 5.585776, mean_absolute_error: 40.168762, mean_q: 80.791740\n",
            " 14996/50000: episode: 120, duration: 1.168s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.127 [-1.449, 0.826], loss: 4.983087, mean_absolute_error: 40.583534, mean_q: 81.653831\n",
            " 15191/50000: episode: 121, duration: 1.094s, episode steps: 195, steps per second: 178, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.322 [-2.470, 1.523], loss: 6.079286, mean_absolute_error: 40.471851, mean_q: 81.374054\n",
            " 15367/50000: episode: 122, duration: 0.960s, episode steps: 176, steps per second: 183, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.362 [-0.776, 2.564], loss: 6.623444, mean_absolute_error: 40.671658, mean_q: 81.794724\n",
            " 15567/50000: episode: 123, duration: 1.096s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.204 [-1.540, 1.032], loss: 5.133991, mean_absolute_error: 40.634575, mean_q: 81.825829\n",
            " 15767/50000: episode: 124, duration: 1.145s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.290 [-0.886, 2.204], loss: 8.242899, mean_absolute_error: 40.821205, mean_q: 82.060455\n",
            " 15967/50000: episode: 125, duration: 1.111s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.138 [-0.794, 1.300], loss: 2.936618, mean_absolute_error: 40.836796, mean_q: 82.293777\n",
            " 16167/50000: episode: 126, duration: 3.273s, episode steps: 200, steps per second: 61, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.244 [-1.831, 1.053], loss: 9.305925, mean_absolute_error: 40.730282, mean_q: 81.861702\n",
            " 16367/50000: episode: 127, duration: 1.160s, episode steps: 200, steps per second: 172, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.175 [-0.674, 1.518], loss: 4.596361, mean_absolute_error: 40.905987, mean_q: 82.259628\n",
            " 16551/50000: episode: 128, duration: 0.919s, episode steps: 184, steps per second: 200, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.349 [-0.924, 2.405], loss: 4.883299, mean_absolute_error: 40.893208, mean_q: 82.300568\n",
            " 16751/50000: episode: 129, duration: 1.100s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.284 [-0.747, 2.076], loss: 7.292642, mean_absolute_error: 40.723587, mean_q: 81.869072\n",
            " 16935/50000: episode: 130, duration: 1.035s, episode steps: 184, steps per second: 178, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.366 [-0.790, 2.427], loss: 3.545462, mean_absolute_error: 40.817104, mean_q: 82.129562\n",
            " 17135/50000: episode: 131, duration: 1.114s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.171 [-1.913, 1.670], loss: 4.508247, mean_absolute_error: 40.737114, mean_q: 81.950134\n",
            " 17313/50000: episode: 132, duration: 0.856s, episode steps: 178, steps per second: 208, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.369 [-0.839, 2.428], loss: 5.000846, mean_absolute_error: 40.663498, mean_q: 81.859001\n",
            " 17503/50000: episode: 133, duration: 0.906s, episode steps: 190, steps per second: 210, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: 0.352 [-1.095, 2.438], loss: 5.127462, mean_absolute_error: 40.899963, mean_q: 82.319672\n",
            " 17703/50000: episode: 134, duration: 1.015s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.163 [-1.053, 1.315], loss: 4.984665, mean_absolute_error: 40.820114, mean_q: 82.164841\n",
            " 17903/50000: episode: 135, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.179 [-0.790, 1.461], loss: 7.901644, mean_absolute_error: 41.038639, mean_q: 82.511475\n",
            " 18103/50000: episode: 136, duration: 0.967s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.302 [-0.893, 2.190], loss: 3.210104, mean_absolute_error: 40.960995, mean_q: 82.413399\n",
            " 18303/50000: episode: 137, duration: 0.942s, episode steps: 200, steps per second: 212, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.836, 1.105], loss: 5.063974, mean_absolute_error: 40.652130, mean_q: 81.799690\n",
            " 18503/50000: episode: 138, duration: 0.973s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.252 [-0.856, 1.857], loss: 5.263267, mean_absolute_error: 41.140186, mean_q: 82.728226\n",
            " 18703/50000: episode: 139, duration: 1.047s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.202 [-1.068, 1.627], loss: 3.361653, mean_absolute_error: 40.892559, mean_q: 82.272179\n",
            " 18903/50000: episode: 140, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.512, 1.330], loss: 4.427405, mean_absolute_error: 40.940941, mean_q: 82.314529\n",
            " 19103/50000: episode: 141, duration: 1.027s, episode steps: 200, steps per second: 195, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.319 [-0.932, 2.317], loss: 4.659416, mean_absolute_error: 41.192245, mean_q: 82.818924\n",
            " 19303/50000: episode: 142, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.231 [-2.276, 1.761], loss: 2.622443, mean_absolute_error: 41.485302, mean_q: 83.534363\n",
            " 19471/50000: episode: 143, duration: 0.873s, episode steps: 168, steps per second: 192, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.365 [-1.014, 2.203], loss: 2.755473, mean_absolute_error: 40.846020, mean_q: 82.197395\n",
            " 19671/50000: episode: 144, duration: 1.084s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.118 [-1.728, 1.548], loss: 4.241416, mean_absolute_error: 41.030060, mean_q: 82.539803\n",
            " 19871/50000: episode: 145, duration: 1.083s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.158 [-1.054, 1.887], loss: 4.238256, mean_absolute_error: 41.429878, mean_q: 83.337555\n",
            " 20071/50000: episode: 146, duration: 1.040s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.228 [-2.103, 1.789], loss: 4.278892, mean_absolute_error: 41.389233, mean_q: 83.208908\n",
            " 20271/50000: episode: 147, duration: 1.097s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.139 [-1.562, 1.599], loss: 5.540512, mean_absolute_error: 41.415051, mean_q: 83.200203\n",
            " 20471/50000: episode: 148, duration: 1.144s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.316 [-1.384, 2.302], loss: 3.614923, mean_absolute_error: 41.276451, mean_q: 83.084473\n",
            " 20671/50000: episode: 149, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.008 [-1.112, 0.839], loss: 3.734926, mean_absolute_error: 41.173073, mean_q: 82.868729\n",
            " 20871/50000: episode: 150, duration: 1.059s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.219 [-1.749, 1.602], loss: 5.917262, mean_absolute_error: 41.549156, mean_q: 83.502472\n",
            " 21071/50000: episode: 151, duration: 1.220s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.152 [-1.345, 1.517], loss: 6.057483, mean_absolute_error: 41.746059, mean_q: 83.886848\n",
            " 21271/50000: episode: 152, duration: 1.171s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.134 [-1.547, 1.110], loss: 5.263321, mean_absolute_error: 41.879780, mean_q: 84.142303\n",
            " 21471/50000: episode: 153, duration: 1.087s, episode steps: 200, steps per second: 184, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.035 [-1.128, 0.971], loss: 5.167870, mean_absolute_error: 41.888008, mean_q: 84.221481\n",
            " 21671/50000: episode: 154, duration: 1.152s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.113 [-0.818, 0.921], loss: 6.105395, mean_absolute_error: 41.951065, mean_q: 84.299622\n",
            " 21871/50000: episode: 155, duration: 1.094s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.310 [-1.045, 2.265], loss: 7.682604, mean_absolute_error: 41.798115, mean_q: 84.032990\n",
            " 22071/50000: episode: 156, duration: 1.121s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.044 [-0.976, 1.056], loss: 6.019296, mean_absolute_error: 42.205551, mean_q: 84.791122\n",
            " 22271/50000: episode: 157, duration: 1.148s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.032 [-0.983, 0.969], loss: 6.220676, mean_absolute_error: 42.126026, mean_q: 84.517677\n",
            " 22471/50000: episode: 158, duration: 1.117s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.099 [-1.513, 1.458], loss: 8.509675, mean_absolute_error: 41.889950, mean_q: 84.043922\n",
            " 22671/50000: episode: 159, duration: 1.075s, episode steps: 200, steps per second: 186, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.303 [-1.039, 2.119], loss: 7.287428, mean_absolute_error: 42.034637, mean_q: 84.381111\n",
            " 22871/50000: episode: 160, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.084 [-0.850, 1.168], loss: 5.508698, mean_absolute_error: 42.306114, mean_q: 85.001427\n",
            " 23066/50000: episode: 161, duration: 1.123s, episode steps: 195, steps per second: 174, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.342 [-0.692, 2.414], loss: 7.529133, mean_absolute_error: 42.075890, mean_q: 84.435616\n",
            " 23255/50000: episode: 162, duration: 1.015s, episode steps: 189, steps per second: 186, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.353 [-1.120, 2.417], loss: 6.533229, mean_absolute_error: 42.064297, mean_q: 84.511276\n",
            " 23455/50000: episode: 163, duration: 1.059s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.001 [-1.008, 1.113], loss: 7.879355, mean_absolute_error: 41.753262, mean_q: 83.754982\n",
            " 23629/50000: episode: 164, duration: 0.937s, episode steps: 174, steps per second: 186, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.383 [-1.024, 2.434], loss: 7.353754, mean_absolute_error: 41.833817, mean_q: 83.931923\n",
            " 23829/50000: episode: 165, duration: 1.109s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.119 [-1.635, 1.415], loss: 5.713031, mean_absolute_error: 41.693233, mean_q: 83.759254\n",
            " 24029/50000: episode: 166, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.112 [-0.836, 1.257], loss: 6.367781, mean_absolute_error: 41.321083, mean_q: 82.984131\n",
            " 24229/50000: episode: 167, duration: 1.151s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.051 [-0.987, 1.016], loss: 6.492446, mean_absolute_error: 41.540714, mean_q: 83.523308\n",
            " 24429/50000: episode: 168, duration: 1.110s, episode steps: 200, steps per second: 180, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.133 [-1.000, 1.360], loss: 9.525436, mean_absolute_error: 41.784317, mean_q: 83.859276\n",
            " 24629/50000: episode: 169, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.183 [-1.895, 1.587], loss: 6.957173, mean_absolute_error: 41.751114, mean_q: 83.963768\n",
            " 24829/50000: episode: 170, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.025 [-0.961, 0.864], loss: 5.860880, mean_absolute_error: 41.572445, mean_q: 83.563965\n",
            " 25029/50000: episode: 171, duration: 1.061s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-1.205, 1.274], loss: 7.162912, mean_absolute_error: 41.736561, mean_q: 83.908981\n",
            " 25223/50000: episode: 172, duration: 1.026s, episode steps: 194, steps per second: 189, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.349 [-1.144, 2.420], loss: 7.811005, mean_absolute_error: 41.891548, mean_q: 84.100014\n",
            " 25423/50000: episode: 173, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.060 [-0.870, 1.067], loss: 6.594340, mean_absolute_error: 41.423817, mean_q: 83.175842\n",
            " 25623/50000: episode: 174, duration: 1.142s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-1.045, 1.110], loss: 7.719844, mean_absolute_error: 41.852943, mean_q: 84.045120\n",
            " 25823/50000: episode: 175, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.160 [-0.828, 1.321], loss: 6.059289, mean_absolute_error: 41.634853, mean_q: 83.664238\n",
            " 26023/50000: episode: 176, duration: 1.128s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.277 [-1.896, 1.325], loss: 7.588993, mean_absolute_error: 41.778847, mean_q: 83.858574\n",
            " 26223/50000: episode: 177, duration: 1.188s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.093 [-1.493, 1.214], loss: 4.035043, mean_absolute_error: 41.570023, mean_q: 83.594391\n",
            " 26415/50000: episode: 178, duration: 1.115s, episode steps: 192, steps per second: 172, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.356 [-0.912, 2.411], loss: 6.190619, mean_absolute_error: 41.601448, mean_q: 83.628883\n",
            " 26606/50000: episode: 179, duration: 1.130s, episode steps: 191, steps per second: 169, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.351 [-0.889, 2.414], loss: 4.888783, mean_absolute_error: 41.390911, mean_q: 83.161064\n",
            " 26806/50000: episode: 180, duration: 1.189s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.164 [-1.880, 1.642], loss: 4.522511, mean_absolute_error: 40.986919, mean_q: 82.296455\n",
            " 27006/50000: episode: 181, duration: 1.215s, episode steps: 200, steps per second: 165, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.152 [-1.010, 1.709], loss: 8.129839, mean_absolute_error: 41.425465, mean_q: 83.048874\n",
            " 27206/50000: episode: 182, duration: 1.064s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.180 [-1.731, 1.302], loss: 7.071133, mean_absolute_error: 41.393566, mean_q: 83.088417\n",
            " 27406/50000: episode: 183, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.030 [-1.047, 0.977], loss: 8.030725, mean_absolute_error: 41.151279, mean_q: 82.558838\n",
            " 27606/50000: episode: 184, duration: 1.133s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.252 [-1.944, 1.662], loss: 6.822597, mean_absolute_error: 41.060356, mean_q: 82.426277\n",
            " 27798/50000: episode: 185, duration: 1.000s, episode steps: 192, steps per second: 192, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.258 [-1.010, 1.657], loss: 6.684483, mean_absolute_error: 40.726131, mean_q: 81.732925\n",
            " 27998/50000: episode: 186, duration: 1.035s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.150 [-1.707, 1.495], loss: 6.034904, mean_absolute_error: 41.028561, mean_q: 82.411835\n",
            " 28198/50000: episode: 187, duration: 1.123s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.290 [-2.071, 1.705], loss: 6.789847, mean_absolute_error: 40.890373, mean_q: 82.031708\n",
            " 28385/50000: episode: 188, duration: 1.142s, episode steps: 187, steps per second: 164, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.208 [-1.383, 1.841], loss: 6.916481, mean_absolute_error: 40.747406, mean_q: 81.809639\n",
            " 28568/50000: episode: 189, duration: 1.006s, episode steps: 183, steps per second: 182, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.530 [0.000, 1.000], mean observation: 0.356 [-1.235, 2.406], loss: 5.888505, mean_absolute_error: 40.578217, mean_q: 81.355125\n",
            " 28768/50000: episode: 190, duration: 1.135s, episode steps: 200, steps per second: 176, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.127 [-1.687, 1.608], loss: 6.142612, mean_absolute_error: 40.597240, mean_q: 81.493889\n",
            " 28963/50000: episode: 191, duration: 1.046s, episode steps: 195, steps per second: 186, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.177 [-1.026, 1.536], loss: 6.122954, mean_absolute_error: 40.614742, mean_q: 81.547295\n",
            " 29163/50000: episode: 192, duration: 1.092s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.161 [-1.879, 1.550], loss: 3.152848, mean_absolute_error: 40.518497, mean_q: 81.401520\n",
            " 29363/50000: episode: 193, duration: 1.013s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.146 [-1.314, 1.497], loss: 6.718234, mean_absolute_error: 40.382114, mean_q: 80.931862\n",
            " 29563/50000: episode: 194, duration: 0.966s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.182 [-1.920, 1.717], loss: 6.484143, mean_absolute_error: 40.382648, mean_q: 80.960617\n",
            " 29763/50000: episode: 195, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.108 [-1.658, 1.502], loss: 7.566287, mean_absolute_error: 40.417213, mean_q: 80.938568\n",
            " 29963/50000: episode: 196, duration: 0.917s, episode steps: 200, steps per second: 218, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.244 [-1.394, 1.717], loss: 4.038337, mean_absolute_error: 40.198952, mean_q: 80.703033\n",
            " 30159/50000: episode: 197, duration: 0.912s, episode steps: 196, steps per second: 215, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.214 [-1.174, 1.431], loss: 5.115258, mean_absolute_error: 40.359234, mean_q: 80.998886\n",
            " 30359/50000: episode: 198, duration: 0.952s, episode steps: 200, steps per second: 210, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.281 [-1.288, 2.002], loss: 6.150007, mean_absolute_error: 40.057178, mean_q: 80.274162\n",
            " 30559/50000: episode: 199, duration: 0.944s, episode steps: 200, steps per second: 212, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.225 [-1.087, 2.074], loss: 7.085661, mean_absolute_error: 39.787884, mean_q: 79.774628\n",
            " 30759/50000: episode: 200, duration: 0.946s, episode steps: 200, steps per second: 211, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.014 [-1.263, 1.179], loss: 4.534856, mean_absolute_error: 40.125778, mean_q: 80.543915\n",
            " 30959/50000: episode: 201, duration: 0.997s, episode steps: 200, steps per second: 201, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.331 [-0.967, 2.326], loss: 6.356389, mean_absolute_error: 39.579247, mean_q: 79.264061\n",
            " 31143/50000: episode: 202, duration: 0.862s, episode steps: 184, steps per second: 213, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.350 [-0.870, 2.425], loss: 6.712863, mean_absolute_error: 39.313950, mean_q: 78.890579\n",
            " 31343/50000: episode: 203, duration: 0.954s, episode steps: 200, steps per second: 210, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.094 [-1.258, 1.124], loss: 3.247297, mean_absolute_error: 39.431553, mean_q: 79.278671\n",
            " 31543/50000: episode: 204, duration: 1.043s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.113 [-1.056, 1.450], loss: 4.833821, mean_absolute_error: 39.599968, mean_q: 79.463997\n",
            " 31743/50000: episode: 205, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.206 [-1.914, 1.641], loss: 5.634696, mean_absolute_error: 39.473110, mean_q: 79.135666\n",
            " 31943/50000: episode: 206, duration: 1.016s, episode steps: 200, steps per second: 197, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.169 [-2.029, 1.674], loss: 7.367156, mean_absolute_error: 39.258495, mean_q: 78.599358\n",
            " 32143/50000: episode: 207, duration: 0.985s, episode steps: 200, steps per second: 203, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.266 [-2.278, 1.667], loss: 7.666507, mean_absolute_error: 39.252762, mean_q: 78.649887\n",
            " 32336/50000: episode: 208, duration: 0.892s, episode steps: 193, steps per second: 216, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.305 [-1.998, 1.188], loss: 7.406470, mean_absolute_error: 39.157658, mean_q: 78.448784\n",
            " 32536/50000: episode: 209, duration: 1.011s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.110 [-1.119, 1.336], loss: 3.715008, mean_absolute_error: 39.191158, mean_q: 78.675652\n",
            " 32736/50000: episode: 210, duration: 1.045s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.100 [-1.977, 1.639], loss: 4.651964, mean_absolute_error: 39.251251, mean_q: 78.742615\n",
            " 32936/50000: episode: 211, duration: 1.001s, episode steps: 200, steps per second: 200, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.268 [-1.930, 1.635], loss: 3.659979, mean_absolute_error: 38.928234, mean_q: 78.187523\n",
            " 33136/50000: episode: 212, duration: 1.080s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.271 [-2.129, 1.945], loss: 3.496319, mean_absolute_error: 38.924908, mean_q: 78.151459\n",
            " 33336/50000: episode: 213, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.066 [-1.413, 1.338], loss: 4.103060, mean_absolute_error: 39.006439, mean_q: 78.226746\n",
            " 33536/50000: episode: 214, duration: 0.965s, episode steps: 200, steps per second: 207, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.232 [-1.146, 1.680], loss: 6.896170, mean_absolute_error: 39.321758, mean_q: 78.722282\n",
            " 33736/50000: episode: 215, duration: 0.977s, episode steps: 200, steps per second: 205, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.138 [-1.004, 1.161], loss: 4.887296, mean_absolute_error: 39.311710, mean_q: 78.827049\n",
            " 33936/50000: episode: 216, duration: 0.949s, episode steps: 200, steps per second: 211, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.236 [-0.968, 1.846], loss: 5.517300, mean_absolute_error: 38.948658, mean_q: 78.114128\n",
            " 34136/50000: episode: 217, duration: 3.244s, episode steps: 200, steps per second: 62, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.279 [-0.970, 1.942], loss: 6.868785, mean_absolute_error: 38.719215, mean_q: 77.592728\n",
            " 34336/50000: episode: 218, duration: 1.175s, episode steps: 200, steps per second: 170, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.073 [-1.764, 1.791], loss: 3.736785, mean_absolute_error: 38.828636, mean_q: 77.884331\n",
            " 34536/50000: episode: 219, duration: 1.083s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.257 [-2.068, 1.817], loss: 9.109454, mean_absolute_error: 38.961765, mean_q: 77.887932\n",
            " 34736/50000: episode: 220, duration: 1.122s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.215 [-2.086, 2.012], loss: 3.858045, mean_absolute_error: 38.501041, mean_q: 77.286865\n",
            " 34936/50000: episode: 221, duration: 1.055s, episode steps: 200, steps per second: 190, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.238 [-1.537, 2.008], loss: 4.308050, mean_absolute_error: 38.846836, mean_q: 77.851517\n",
            " 35136/50000: episode: 222, duration: 1.041s, episode steps: 200, steps per second: 192, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.135 [-1.772, 1.703], loss: 3.537161, mean_absolute_error: 38.576569, mean_q: 77.378792\n",
            " 35336/50000: episode: 223, duration: 1.064s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.172 [-1.188, 1.415], loss: 6.315264, mean_absolute_error: 38.300087, mean_q: 76.724365\n",
            " 35536/50000: episode: 224, duration: 1.068s, episode steps: 200, steps per second: 187, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.195 [-1.928, 1.936], loss: 6.341177, mean_absolute_error: 38.795433, mean_q: 77.768768\n",
            " 35735/50000: episode: 225, duration: 1.017s, episode steps: 199, steps per second: 196, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.274 [-1.914, 1.814], loss: 5.824564, mean_absolute_error: 38.170380, mean_q: 76.672073\n",
            " 35928/50000: episode: 226, duration: 1.014s, episode steps: 193, steps per second: 190, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.368 [-1.190, 2.421], loss: 3.847944, mean_absolute_error: 38.232807, mean_q: 76.783485\n",
            " 36128/50000: episode: 227, duration: 1.045s, episode steps: 200, steps per second: 191, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.218 [-1.956, 1.727], loss: 7.274172, mean_absolute_error: 38.347939, mean_q: 76.804062\n",
            " 36299/50000: episode: 228, duration: 0.960s, episode steps: 171, steps per second: 178, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.170 [-1.035, 1.761], loss: 4.663798, mean_absolute_error: 38.581516, mean_q: 77.391983\n",
            " 36499/50000: episode: 229, duration: 1.140s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.163 [-1.666, 1.732], loss: 4.658148, mean_absolute_error: 38.443790, mean_q: 77.097389\n",
            " 36690/50000: episode: 230, duration: 1.122s, episode steps: 191, steps per second: 170, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.292 [-1.941, 1.841], loss: 4.832474, mean_absolute_error: 38.448540, mean_q: 77.100220\n",
            " 36890/50000: episode: 231, duration: 1.063s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.226 [-1.768, 1.764], loss: 4.328780, mean_absolute_error: 38.328094, mean_q: 77.016464\n",
            " 37090/50000: episode: 232, duration: 1.057s, episode steps: 200, steps per second: 189, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.233 [-1.182, 2.073], loss: 3.349820, mean_absolute_error: 38.151802, mean_q: 76.686638\n",
            " 37290/50000: episode: 233, duration: 1.034s, episode steps: 200, steps per second: 193, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.302 [-2.127, 1.867], loss: 5.407612, mean_absolute_error: 38.401344, mean_q: 77.009178\n",
            " 37490/50000: episode: 234, duration: 1.063s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.115 [-1.353, 1.571], loss: 5.676135, mean_absolute_error: 38.081810, mean_q: 76.460594\n",
            " 37688/50000: episode: 235, duration: 1.133s, episode steps: 198, steps per second: 175, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.208 [-2.072, 1.979], loss: 4.907492, mean_absolute_error: 38.356384, mean_q: 77.019386\n",
            " 37888/50000: episode: 236, duration: 1.131s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.197 [-2.096, 1.949], loss: 4.371325, mean_absolute_error: 38.165264, mean_q: 76.498398\n",
            " 38088/50000: episode: 237, duration: 1.150s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.129 [-1.346, 1.279], loss: 5.607934, mean_absolute_error: 38.478340, mean_q: 77.114143\n",
            " 38288/50000: episode: 238, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.237 [-2.137, 2.070], loss: 5.474631, mean_absolute_error: 38.394848, mean_q: 76.917763\n",
            " 38488/50000: episode: 239, duration: 1.115s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.136 [-2.075, 1.831], loss: 7.261538, mean_absolute_error: 38.273506, mean_q: 76.601433\n",
            " 38688/50000: episode: 240, duration: 1.099s, episode steps: 200, steps per second: 182, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.219 [-2.096, 2.167], loss: 6.361725, mean_absolute_error: 38.367825, mean_q: 76.832710\n",
            " 38888/50000: episode: 241, duration: 1.140s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: -0.083 [-1.949, 1.654], loss: 5.007324, mean_absolute_error: 38.208508, mean_q: 76.555885\n",
            " 39088/50000: episode: 242, duration: 1.149s, episode steps: 200, steps per second: 174, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.163 [-2.002, 1.904], loss: 7.334718, mean_absolute_error: 38.397839, mean_q: 76.869247\n",
            " 39288/50000: episode: 243, duration: 1.131s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.078 [-1.549, 1.902], loss: 6.271398, mean_absolute_error: 38.131653, mean_q: 76.384659\n",
            " 39488/50000: episode: 244, duration: 1.156s, episode steps: 200, steps per second: 173, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.228 [-2.132, 2.061], loss: 5.630085, mean_absolute_error: 38.307606, mean_q: 76.856438\n",
            " 39688/50000: episode: 245, duration: 1.131s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.106 [-1.768, 1.788], loss: 5.174871, mean_absolute_error: 38.186554, mean_q: 76.630119\n",
            " 39888/50000: episode: 246, duration: 1.235s, episode steps: 200, steps per second: 162, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.126 [-2.133, 2.051], loss: 5.310601, mean_absolute_error: 38.414291, mean_q: 76.994247\n",
            " 40088/50000: episode: 247, duration: 1.250s, episode steps: 200, steps per second: 160, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.165 [-1.934, 1.609], loss: 5.099593, mean_absolute_error: 38.288700, mean_q: 76.824425\n",
            " 40288/50000: episode: 248, duration: 1.207s, episode steps: 200, steps per second: 166, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.216 [-1.946, 1.906], loss: 5.406204, mean_absolute_error: 38.273228, mean_q: 76.781601\n",
            " 40488/50000: episode: 249, duration: 1.218s, episode steps: 200, steps per second: 164, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.223 [-1.948, 2.006], loss: 4.704355, mean_absolute_error: 38.403458, mean_q: 77.021858\n",
            " 40688/50000: episode: 250, duration: 1.238s, episode steps: 200, steps per second: 161, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.249 [-1.921, 1.937], loss: 6.420270, mean_absolute_error: 38.407810, mean_q: 77.083870\n",
            " 40888/50000: episode: 251, duration: 1.168s, episode steps: 200, steps per second: 171, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.137 [-1.754, 1.255], loss: 6.058465, mean_absolute_error: 38.427696, mean_q: 77.082962\n",
            " 41088/50000: episode: 252, duration: 1.193s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.292 [-2.284, 2.119], loss: 5.644978, mean_absolute_error: 38.388168, mean_q: 77.114723\n",
            " 41288/50000: episode: 253, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.264 [-2.222, 1.949], loss: 5.571630, mean_absolute_error: 38.267933, mean_q: 76.897919\n",
            " 41488/50000: episode: 254, duration: 1.141s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.297 [-2.089, 1.870], loss: 5.605234, mean_absolute_error: 38.424404, mean_q: 77.161400\n",
            " 41688/50000: episode: 255, duration: 1.120s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.327 [-2.230, 1.800], loss: 5.951698, mean_absolute_error: 38.275990, mean_q: 76.752121\n",
            " 41888/50000: episode: 256, duration: 1.103s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.282 [-2.050, 1.780], loss: 9.228570, mean_absolute_error: 37.994705, mean_q: 76.110420\n",
            " 42088/50000: episode: 257, duration: 1.189s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.279 [-1.977, 1.719], loss: 3.804357, mean_absolute_error: 38.084145, mean_q: 76.548477\n",
            " 42196/50000: episode: 258, duration: 0.691s, episode steps: 108, steps per second: 156, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.236 [-2.240, 1.701], loss: 5.837947, mean_absolute_error: 38.201347, mean_q: 76.518929\n",
            " 42362/50000: episode: 259, duration: 1.070s, episode steps: 166, steps per second: 155, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.259 [-1.957, 1.334], loss: 4.342844, mean_absolute_error: 37.831654, mean_q: 75.859680\n",
            " 42547/50000: episode: 260, duration: 1.184s, episode steps: 185, steps per second: 156, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.233 [-2.289, 1.989], loss: 5.700717, mean_absolute_error: 37.988697, mean_q: 76.152397\n",
            " 42747/50000: episode: 261, duration: 1.266s, episode steps: 200, steps per second: 158, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.177 [-1.399, 1.672], loss: 6.620592, mean_absolute_error: 37.997837, mean_q: 76.131828\n",
            " 42947/50000: episode: 262, duration: 1.260s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.259 [-2.108, 1.774], loss: 4.869777, mean_absolute_error: 37.557213, mean_q: 75.292419\n",
            " 43043/50000: episode: 263, duration: 0.610s, episode steps: 96, steps per second: 157, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.176 [-2.275, 1.946], loss: 8.442578, mean_absolute_error: 37.988766, mean_q: 76.106842\n",
            " 43226/50000: episode: 264, duration: 1.135s, episode steps: 183, steps per second: 161, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: -0.365 [-2.314, 1.898], loss: 5.536810, mean_absolute_error: 37.741776, mean_q: 75.623894\n",
            " 43426/50000: episode: 265, duration: 1.261s, episode steps: 200, steps per second: 159, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.324 [-1.295, 2.278], loss: 7.387465, mean_absolute_error: 37.576378, mean_q: 75.283058\n",
            " 43596/50000: episode: 266, duration: 1.071s, episode steps: 170, steps per second: 159, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.109 [-2.072, 1.661], loss: 5.728652, mean_absolute_error: 37.189529, mean_q: 74.638672\n",
            " 43712/50000: episode: 267, duration: 0.681s, episode steps: 116, steps per second: 170, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.244 [-1.896, 1.337], loss: 6.086871, mean_absolute_error: 37.448841, mean_q: 75.053284\n",
            " 43819/50000: episode: 268, duration: 0.596s, episode steps: 107, steps per second: 180, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.281 [-2.644, 2.006], loss: 8.239276, mean_absolute_error: 37.226048, mean_q: 74.571129\n",
            " 43961/50000: episode: 269, duration: 0.771s, episode steps: 142, steps per second: 184, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.186 [-2.095, 1.606], loss: 6.126109, mean_absolute_error: 37.653046, mean_q: 75.381554\n",
            " 44101/50000: episode: 270, duration: 0.768s, episode steps: 140, steps per second: 182, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.215 [-2.060, 1.316], loss: 4.916317, mean_absolute_error: 37.576157, mean_q: 75.364845\n",
            " 44246/50000: episode: 271, duration: 0.810s, episode steps: 145, steps per second: 179, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.263 [-2.634, 1.836], loss: 4.615166, mean_absolute_error: 37.467876, mean_q: 75.264961\n",
            " 44362/50000: episode: 272, duration: 0.654s, episode steps: 116, steps per second: 177, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.255 [-2.426, 1.850], loss: 5.492930, mean_absolute_error: 37.615997, mean_q: 75.609932\n",
            " 44481/50000: episode: 273, duration: 0.666s, episode steps: 119, steps per second: 179, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.371 [-2.485, 1.840], loss: 3.043114, mean_absolute_error: 37.460697, mean_q: 75.404655\n",
            " 44574/50000: episode: 274, duration: 0.498s, episode steps: 93, steps per second: 187, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.246 [-2.285, 1.640], loss: 5.598658, mean_absolute_error: 37.359547, mean_q: 75.117828\n",
            " 44774/50000: episode: 275, duration: 1.119s, episode steps: 200, steps per second: 179, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.187 [-1.440, 1.531], loss: 4.873688, mean_absolute_error: 37.381962, mean_q: 75.243416\n",
            " 44961/50000: episode: 276, duration: 1.054s, episode steps: 187, steps per second: 177, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.291 [-2.659, 1.720], loss: 6.665021, mean_absolute_error: 37.527729, mean_q: 75.407494\n",
            " 45141/50000: episode: 277, duration: 1.002s, episode steps: 180, steps per second: 180, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.332 [-2.863, 2.165], loss: 3.799872, mean_absolute_error: 37.341656, mean_q: 75.023819\n",
            " 45310/50000: episode: 278, duration: 0.954s, episode steps: 169, steps per second: 177, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.371 [-2.260, 1.399], loss: 4.384791, mean_absolute_error: 37.107994, mean_q: 74.618141\n",
            " 45510/50000: episode: 279, duration: 1.062s, episode steps: 200, steps per second: 188, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.232 [-2.081, 1.031], loss: 6.316696, mean_absolute_error: 37.515648, mean_q: 75.327522\n",
            " 45675/50000: episode: 280, duration: 0.910s, episode steps: 165, steps per second: 181, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.349 [-2.412, 1.400], loss: 5.418984, mean_absolute_error: 37.392761, mean_q: 75.238869\n",
            " 45826/50000: episode: 281, duration: 0.844s, episode steps: 151, steps per second: 179, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.369 [-2.750, 1.515], loss: 3.496655, mean_absolute_error: 37.297382, mean_q: 75.036240\n",
            " 45952/50000: episode: 282, duration: 0.701s, episode steps: 126, steps per second: 180, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.271 [-2.836, 1.994], loss: 6.779900, mean_absolute_error: 37.649254, mean_q: 75.485451\n",
            " 46134/50000: episode: 283, duration: 1.017s, episode steps: 182, steps per second: 179, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.338 [-2.425, 1.205], loss: 5.104190, mean_absolute_error: 37.664223, mean_q: 75.463409\n",
            " 46288/50000: episode: 284, duration: 0.867s, episode steps: 154, steps per second: 178, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.352 [-2.805, 1.392], loss: 5.446306, mean_absolute_error: 37.519127, mean_q: 75.176254\n",
            " 46463/50000: episode: 285, duration: 0.961s, episode steps: 175, steps per second: 182, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.204 [-2.806, 2.072], loss: 5.711954, mean_absolute_error: 37.283924, mean_q: 74.733269\n",
            " 46623/50000: episode: 286, duration: 0.889s, episode steps: 160, steps per second: 180, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.348 [-3.038, 2.009], loss: 5.651345, mean_absolute_error: 37.573193, mean_q: 75.434738\n",
            " 46751/50000: episode: 287, duration: 0.732s, episode steps: 128, steps per second: 175, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.383 [-2.458, 1.337], loss: 5.504364, mean_absolute_error: 37.773716, mean_q: 75.873398\n",
            " 46891/50000: episode: 288, duration: 0.810s, episode steps: 140, steps per second: 173, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.381 [-2.430, 1.352], loss: 6.393878, mean_absolute_error: 37.415020, mean_q: 75.182968\n",
            " 47073/50000: episode: 289, duration: 1.088s, episode steps: 182, steps per second: 167, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.369 [-2.599, 1.777], loss: 3.981551, mean_absolute_error: 37.653343, mean_q: 75.755455\n",
            " 47245/50000: episode: 290, duration: 1.072s, episode steps: 172, steps per second: 160, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.387 [-2.438, 1.368], loss: 4.125016, mean_absolute_error: 37.594757, mean_q: 75.635750\n",
            " 47409/50000: episode: 291, duration: 0.962s, episode steps: 164, steps per second: 170, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.247 [-2.197, 1.278], loss: 2.397830, mean_absolute_error: 37.840122, mean_q: 76.234657\n",
            " 47578/50000: episode: 292, duration: 0.979s, episode steps: 169, steps per second: 173, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.330 [-2.812, 1.404], loss: 4.578056, mean_absolute_error: 37.551029, mean_q: 75.548500\n",
            " 47707/50000: episode: 293, duration: 0.743s, episode steps: 129, steps per second: 174, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.193 [-2.604, 1.992], loss: 9.404159, mean_absolute_error: 37.563694, mean_q: 75.227051\n",
            " 47880/50000: episode: 294, duration: 0.965s, episode steps: 173, steps per second: 179, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.249 [-2.637, 1.622], loss: 5.663305, mean_absolute_error: 37.833042, mean_q: 75.984337\n",
            " 48080/50000: episode: 295, duration: 1.126s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.121 [-1.526, 1.146], loss: 6.423147, mean_absolute_error: 37.929256, mean_q: 76.057487\n",
            " 48240/50000: episode: 296, duration: 0.953s, episode steps: 160, steps per second: 168, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.255 [-2.441, 1.431], loss: 4.989137, mean_absolute_error: 37.769299, mean_q: 75.893829\n",
            " 48440/50000: episode: 297, duration: 1.146s, episode steps: 200, steps per second: 175, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.043 [-1.477, 1.343], loss: 5.529249, mean_absolute_error: 38.099873, mean_q: 76.378380\n",
            " 48640/50000: episode: 298, duration: 1.124s, episode steps: 200, steps per second: 178, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.059 [-1.348, 1.221], loss: 5.247892, mean_absolute_error: 38.107487, mean_q: 76.432327\n",
            " 48840/50000: episode: 299, duration: 1.105s, episode steps: 200, steps per second: 181, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.252 [-2.766, 1.644], loss: 5.508431, mean_absolute_error: 37.805721, mean_q: 75.928619\n",
            " 49040/50000: episode: 300, duration: 1.091s, episode steps: 200, steps per second: 183, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.022 [-1.324, 1.364], loss: 5.252112, mean_absolute_error: 37.953083, mean_q: 76.240334\n",
            " 49240/50000: episode: 301, duration: 1.129s, episode steps: 200, steps per second: 177, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.000 [-1.206, 1.272], loss: 5.997325, mean_absolute_error: 38.081726, mean_q: 76.503929\n",
            " 49440/50000: episode: 302, duration: 1.031s, episode steps: 200, steps per second: 194, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.032 [-1.351, 1.186], loss: 6.087235, mean_absolute_error: 38.208805, mean_q: 76.659363\n",
            " 49640/50000: episode: 303, duration: 1.011s, episode steps: 200, steps per second: 198, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.222 [-2.304, 1.648], loss: 7.210724, mean_absolute_error: 38.008663, mean_q: 76.198334\n",
            " 49840/50000: episode: 304, duration: 1.081s, episode steps: 200, steps per second: 185, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.064 [-1.746, 1.396], loss: 3.740543, mean_absolute_error: 38.006413, mean_q: 76.366585\n",
            "done, took 284.077 seconds\n",
            "Testing for 5 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1f9198f1b091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dqn_{}_weights.h5f'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for CartPole-v0, you cannot call reset() unless the episode is over."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZKAKGIh8Jn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn.done = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0f2JkGZvwzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "241b5ed3-310f-4cb9-c467-7b74f7a54f71"
      },
      "source": [
        "show_video()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}