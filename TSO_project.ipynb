{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TSO_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abstractguy/TSO_project/blob/master/TSO_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCelFzWY9MBI",
        "colab_type": "code",
        "outputId": "79b4b196-39d3-4458-9a1f-e958d0ac272f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install cmake xvfb python-opengl ffmpeg libgle3\n",
        "!pip install --upgrade setuptools\n",
        "!pip install ez_setup gym gym[all] pyvirtualdisplay pyglet==1.3.2 keras-rl"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [628 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [926 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [760 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,279 kB]\n",
            "Fetched 3,849 kB in 2s (1,632 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "ffmpeg is already the newest version (7:3.4.6-0ubuntu0.18.04.1).\n",
            "The following NEW packages will be installed:\n",
            "  libgle3 python-opengl xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 12 not upgraded.\n",
            "Need to get 1,318 kB of archives.\n",
            "After this operation, 7,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgle3 amd64 3.1.0-7.2 [37.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.3 [783 kB]\n",
            "Fetched 1,318 kB in 1s (1,384 kB/s)\n",
            "Selecting previously unselected package libgle3:amd64.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../libgle3_3.1.0-7.2_amd64.deb ...\n",
            "Unpacking libgle3:amd64 (3.1.0-7.2) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.3_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up libgle3:amd64 (3.1.0-7.2) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.3) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.2.0)\n",
            "Collecting ez_setup\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/2c/743df41bd6b3298706dfe91b0c7ecdc47f2dc1a3104abeb6e9aa4a45fa5d/ez_setup-0.9.tar.gz\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/ad/b15f252bfb0f1693ad3150b55a44a674f3cba711cacdbb9ae2f03f143d19/PyVirtualDisplay-0.2.4-py2.py3-none-any.whl\n",
            "Collecting pyglet==1.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.3MB/s \n",
            "\u001b[?25hCollecting keras-rl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/87/4b57eff8e4bd834cea0a75cd6c58198c9e42be29b600db9c14fafa72ec07/keras-rl-0.4.2.tar.gz (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 21.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.1)\n",
            "Collecting EasyProcess (from pyvirtualdisplay)\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/29/40040d1d64a224a5e44df9572794a66494618ffe5c77199214aeceedb8a7/EasyProcess-0.2.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.3.2) (0.16.0)\n",
            "Requirement already satisfied: keras>=2.0.7 in /usr/local/lib/python3.6/dist-packages (from keras-rl) (2.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.7->keras-rl) (3.13)\n",
            "Building wheels for collected packages: ez-setup, keras-rl\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ez-setup: filename=ez_setup-0.9-cp36-none-any.whl size=11014 sha256=f0c7e7eb8a7c85df84b9508fa3479a4bb911c0d7bcaf93ea878c4c2a5747c9e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e8/6b/3d5ff5a3efd7b5338d1e173ac981771e2628ceb2f7866d49ad\n",
            "  Building wheel for keras-rl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rl: filename=keras_rl-0.4.2-cp36-none-any.whl size=48379 sha256=57575a56f56311d20a91fad94f46fb0423d134a4abeee885c6a3cd5f15073942\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/4d/84/9254c9f2e8f51865cb0dac8e79da85330c735551d31f73c894\n",
            "Successfully built ez-setup keras-rl\n",
            "Installing collected packages: ez-setup, EasyProcess, pyvirtualdisplay, pyglet, keras-rl\n",
            "  Found existing installation: pyglet 1.4.2\n",
            "    Uninstalling pyglet-1.4.2:\n",
            "      Successfully uninstalled pyglet-1.4.2\n",
            "Successfully installed EasyProcess-0.2.7 ez-setup-0.9 keras-rl-0.4.2 pyglet-1.3.2 pyvirtualdisplay-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdb2JwZy4jGj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f011f65-f538-4f15-bd58-75ec4b002e88"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym import wrappers\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Activation, Flatten, Input, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.agents import DDPGAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.processors import WhiteningNormalizerProcessor\n",
        "from rl.random import OrnsteinUhlenbeckProcess"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQEtc28G4niA",
        "colab_type": "code",
        "outputId": "789f3eaa-643f-4d75-e399-4b6a2a01e86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0831 20:03:50.223879 139735571728256 abstractdisplay.py:151] xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9UWeToN4r7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# env = wrap_env(env)\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq34pCSUDPT3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "810f8a97-fd98-4305-b267-423aa9b219e9"
      },
      "source": [
        "ENV_NAME = 'CartPole-v1'\n",
        "\n",
        "env = wrap_env(gym.make(ENV_NAME))\n",
        "np.random.seed(123)\n",
        "env.seed(123)\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_actions, activation='linear'))\n",
        "print(model.summary())\n",
        "\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "policy = BoltzmannQPolicy()\n",
        "\n",
        "# Enable the dueling network (dueling_type is one of {'avg', 'max', 'naive'}).\n",
        "dqn = DQNAgent(model=model, \n",
        "               nb_actions=nb_actions, \n",
        "               memory=memory, \n",
        "               nb_steps_warmup=20,\n",
        "               enable_dueling_network=True, \n",
        "               dueling_type='avg', \n",
        "               target_model_update=1e-2, \n",
        "               policy=policy)\n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "dqn.save_weights('duel_dqn_' + ENV_NAME + '_weights.h5f', overwrite=True)\n",
        "dqn.test(env, nb_episodes=10, visualize=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0831 20:03:50.582734 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0831 20:03:50.769872 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0831 20:03:50.880560 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0831 20:03:51.064103 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0831 20:03:51.067173 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 658\n",
            "Trainable params: 658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0831 20:03:51.737111 139735571728256 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    21/50000: episode: 1, duration: 3.364s, episode steps: 21, steps per second: 6, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.060 [-1.001, 1.740], loss: --, mean_absolute_error: --, mean_q: --\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/rl/memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    76/50000: episode: 2, duration: 0.863s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: -0.024 [-1.761, 0.673], loss: 0.369168, mean_absolute_error: 0.485287, mean_q: 0.099140\n",
            "    89/50000: episode: 3, duration: 0.126s, episode steps: 13, steps per second: 103, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.121 [-1.322, 2.247], loss: 0.188961, mean_absolute_error: 0.508663, mean_q: 0.424914\n",
            "   104/50000: episode: 4, duration: 0.052s, episode steps: 15, steps per second: 291, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.105 [-2.328, 1.354], loss: 0.094596, mean_absolute_error: 0.530759, mean_q: 0.674266\n",
            "   145/50000: episode: 5, duration: 0.148s, episode steps: 41, steps per second: 277, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.366 [0.000, 1.000], mean observation: -0.087 [-2.071, 2.378], loss: 0.026622, mean_absolute_error: 0.633856, mean_q: 1.114194\n",
            "   170/50000: episode: 6, duration: 0.089s, episode steps: 25, steps per second: 280, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.065 [-1.654, 0.809], loss: 0.010624, mean_absolute_error: 0.696479, mean_q: 1.302963\n",
            "   210/50000: episode: 7, duration: 0.154s, episode steps: 40, steps per second: 259, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: 0.028 [-1.135, 1.956], loss: 0.010400, mean_absolute_error: 0.795752, mean_q: 1.517555\n",
            "   224/50000: episode: 8, duration: 0.057s, episode steps: 14, steps per second: 247, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.104 [-1.587, 0.793], loss: 0.010548, mean_absolute_error: 0.889550, mean_q: 1.713469\n",
            "   250/50000: episode: 9, duration: 0.432s, episode steps: 26, steps per second: 60, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.051 [-2.106, 1.230], loss: 0.011920, mean_absolute_error: 0.961304, mean_q: 1.860737\n",
            "   269/50000: episode: 10, duration: 0.140s, episode steps: 19, steps per second: 135, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.087 [-2.301, 1.360], loss: 0.015536, mean_absolute_error: 1.045118, mean_q: 2.031580\n",
            "   302/50000: episode: 11, duration: 0.117s, episode steps: 33, steps per second: 282, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.092 [-0.808, 1.340], loss: 0.015474, mean_absolute_error: 1.141714, mean_q: 2.233865\n",
            "   312/50000: episode: 12, duration: 0.037s, episode steps: 10, steps per second: 273, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.108 [-1.860, 1.215], loss: 0.023272, mean_absolute_error: 1.229191, mean_q: 2.414267\n",
            "   332/50000: episode: 13, duration: 0.077s, episode steps: 20, steps per second: 261, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.047 [-2.121, 1.409], loss: 0.029015, mean_absolute_error: 1.284957, mean_q: 2.491915\n",
            "   352/50000: episode: 14, duration: 0.085s, episode steps: 20, steps per second: 236, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.062 [-2.228, 1.389], loss: 0.034831, mean_absolute_error: 1.383906, mean_q: 2.688032\n",
            "   376/50000: episode: 15, duration: 0.088s, episode steps: 24, steps per second: 274, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.067 [-0.998, 1.851], loss: 0.030690, mean_absolute_error: 1.471469, mean_q: 2.899585\n",
            "   396/50000: episode: 16, duration: 0.075s, episode steps: 20, steps per second: 268, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.049 [-1.871, 1.183], loss: 0.041158, mean_absolute_error: 1.559119, mean_q: 3.053719\n",
            "   424/50000: episode: 17, duration: 0.102s, episode steps: 28, steps per second: 275, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.013 [-0.957, 1.416], loss: 0.035986, mean_absolute_error: 1.669878, mean_q: 3.278728\n",
            "   434/50000: episode: 18, duration: 0.039s, episode steps: 10, steps per second: 259, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-1.664, 0.973], loss: 0.076397, mean_absolute_error: 1.751219, mean_q: 3.402742\n",
            "   447/50000: episode: 19, duration: 0.049s, episode steps: 13, steps per second: 267, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.084 [-1.509, 1.002], loss: 0.078450, mean_absolute_error: 1.803201, mean_q: 3.494439\n",
            "   461/50000: episode: 20, duration: 0.051s, episode steps: 14, steps per second: 273, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.776, 1.178], loss: 0.073836, mean_absolute_error: 1.855058, mean_q: 3.609204\n",
            "   487/50000: episode: 21, duration: 0.095s, episode steps: 26, steps per second: 273, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.103 [-0.576, 1.174], loss: 0.086664, mean_absolute_error: 1.943146, mean_q: 3.762619\n",
            "   520/50000: episode: 22, duration: 0.106s, episode steps: 33, steps per second: 312, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.066 [-0.601, 1.185], loss: 0.097579, mean_absolute_error: 2.069301, mean_q: 4.007934\n",
            "   544/50000: episode: 23, duration: 0.085s, episode steps: 24, steps per second: 284, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.075 [-0.949, 1.745], loss: 0.119907, mean_absolute_error: 2.193168, mean_q: 4.249209\n",
            "   568/50000: episode: 24, duration: 0.091s, episode steps: 24, steps per second: 262, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.044 [-1.515, 0.809], loss: 0.136908, mean_absolute_error: 2.291646, mean_q: 4.421530\n",
            "   585/50000: episode: 25, duration: 0.063s, episode steps: 17, steps per second: 270, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.107 [-1.373, 0.774], loss: 0.135011, mean_absolute_error: 2.375628, mean_q: 4.571916\n",
            "   608/50000: episode: 26, duration: 0.082s, episode steps: 23, steps per second: 279, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: -0.051 [-1.078, 0.793], loss: 0.105203, mean_absolute_error: 2.461941, mean_q: 4.780407\n",
            "   619/50000: episode: 27, duration: 0.043s, episode steps: 11, steps per second: 256, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.141 [-1.806, 0.977], loss: 0.116763, mean_absolute_error: 2.496617, mean_q: 4.873757\n",
            "   644/50000: episode: 28, duration: 0.418s, episode steps: 25, steps per second: 60, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.061 [-1.044, 1.994], loss: 0.156010, mean_absolute_error: 2.640139, mean_q: 5.134787\n",
            "   663/50000: episode: 29, duration: 0.135s, episode steps: 19, steps per second: 141, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.031 [-1.826, 1.219], loss: 0.175515, mean_absolute_error: 2.709508, mean_q: 5.254528\n",
            "   705/50000: episode: 30, duration: 0.152s, episode steps: 42, steps per second: 276, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.048 [-1.446, 0.654], loss: 0.149207, mean_absolute_error: 2.838376, mean_q: 5.547205\n",
            "   719/50000: episode: 31, duration: 0.054s, episode steps: 14, steps per second: 260, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.107 [-1.231, 0.751], loss: 0.221295, mean_absolute_error: 2.982374, mean_q: 5.764388\n",
            "   808/50000: episode: 32, duration: 0.309s, episode steps: 89, steps per second: 288, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.088 [-1.590, 1.435], loss: 0.244349, mean_absolute_error: 3.194555, mean_q: 6.193737\n",
            "   829/50000: episode: 33, duration: 0.073s, episode steps: 21, steps per second: 290, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.063 [-2.023, 1.204], loss: 0.212704, mean_absolute_error: 3.456088, mean_q: 6.730004\n",
            "   854/50000: episode: 34, duration: 0.090s, episode steps: 25, steps per second: 278, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.074 [-0.930, 1.578], loss: 0.339539, mean_absolute_error: 3.526937, mean_q: 6.803306\n",
            "   886/50000: episode: 35, duration: 0.119s, episode steps: 32, steps per second: 270, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.092 [-0.446, 1.380], loss: 0.226474, mean_absolute_error: 3.636015, mean_q: 7.117517\n",
            "   915/50000: episode: 36, duration: 0.103s, episode steps: 29, steps per second: 281, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: 0.046 [-0.820, 1.581], loss: 0.321127, mean_absolute_error: 3.705033, mean_q: 7.223368\n",
            "   936/50000: episode: 37, duration: 0.082s, episode steps: 21, steps per second: 256, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.051 [-0.955, 1.344], loss: 0.439163, mean_absolute_error: 3.860600, mean_q: 7.489444\n",
            "   978/50000: episode: 38, duration: 0.166s, episode steps: 42, steps per second: 253, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.023 [-1.710, 2.111], loss: 0.328069, mean_absolute_error: 3.987052, mean_q: 7.785751\n",
            "  1025/50000: episode: 39, duration: 0.167s, episode steps: 47, steps per second: 281, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.046 [-0.597, 1.056], loss: 0.329395, mean_absolute_error: 4.143834, mean_q: 8.138965\n",
            "  1045/50000: episode: 40, duration: 0.075s, episode steps: 20, steps per second: 267, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.339, 0.776], loss: 0.396666, mean_absolute_error: 4.320509, mean_q: 8.489374\n",
            "  1054/50000: episode: 41, duration: 0.034s, episode steps: 9, steps per second: 267, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.144 [-0.976, 1.579], loss: 0.451159, mean_absolute_error: 4.327595, mean_q: 8.475331\n",
            "  1087/50000: episode: 42, duration: 0.118s, episode steps: 33, steps per second: 280, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.125 [-1.039, 0.538], loss: 0.315610, mean_absolute_error: 4.468739, mean_q: 8.785538\n",
            "  1099/50000: episode: 43, duration: 0.041s, episode steps: 12, steps per second: 294, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.133 [-0.557, 1.154], loss: 0.483204, mean_absolute_error: 4.566322, mean_q: 8.899878\n",
            "  1123/50000: episode: 44, duration: 0.093s, episode steps: 24, steps per second: 259, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.353, 0.645], loss: 0.345977, mean_absolute_error: 4.631224, mean_q: 9.078202\n",
            "  1140/50000: episode: 45, duration: 0.057s, episode steps: 17, steps per second: 298, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.060 [-1.252, 0.825], loss: 0.377577, mean_absolute_error: 4.726898, mean_q: 9.296554\n",
            "  1149/50000: episode: 46, duration: 0.036s, episode steps: 9, steps per second: 252, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.153 [-1.141, 1.959], loss: 0.216883, mean_absolute_error: 4.717631, mean_q: 9.282364\n",
            "  1191/50000: episode: 47, duration: 0.154s, episode steps: 42, steps per second: 274, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.012 [-1.209, 1.398], loss: 0.373095, mean_absolute_error: 4.850404, mean_q: 9.530047\n",
            "  1225/50000: episode: 48, duration: 0.118s, episode steps: 34, steps per second: 289, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.014 [-1.468, 1.307], loss: 0.394549, mean_absolute_error: 5.037533, mean_q: 9.908056\n",
            "  1258/50000: episode: 49, duration: 0.125s, episode steps: 33, steps per second: 263, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.083 [-0.610, 1.275], loss: 0.392514, mean_absolute_error: 5.134025, mean_q: 10.124112\n",
            "  1280/50000: episode: 50, duration: 0.078s, episode steps: 22, steps per second: 282, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.094 [-1.030, 0.570], loss: 0.415089, mean_absolute_error: 5.212101, mean_q: 10.251122\n",
            "  1426/50000: episode: 51, duration: 0.497s, episode steps: 146, steps per second: 294, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: -0.075 [-1.192, 1.235], loss: 0.419602, mean_absolute_error: 5.553270, mean_q: 11.006020\n",
            "  1539/50000: episode: 52, duration: 0.397s, episode steps: 113, steps per second: 284, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: 0.101 [-1.504, 1.670], loss: 0.494783, mean_absolute_error: 6.059057, mean_q: 12.086287\n",
            "  1688/50000: episode: 53, duration: 0.509s, episode steps: 149, steps per second: 293, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.237 [-1.700, 1.350], loss: 0.532556, mean_absolute_error: 6.612072, mean_q: 13.345180\n",
            "  1728/50000: episode: 54, duration: 0.141s, episode steps: 40, steps per second: 283, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.052 [-1.343, 1.432], loss: 0.635352, mean_absolute_error: 6.950248, mean_q: 14.060745\n",
            "  1917/50000: episode: 55, duration: 0.642s, episode steps: 189, steps per second: 294, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.192 [-1.615, 0.838], loss: 0.604406, mean_absolute_error: 7.609272, mean_q: 15.431895\n",
            "  2024/50000: episode: 56, duration: 0.390s, episode steps: 107, steps per second: 274, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.151 [-1.168, 1.058], loss: 0.909849, mean_absolute_error: 8.306437, mean_q: 16.845442\n",
            "  2249/50000: episode: 57, duration: 0.724s, episode steps: 225, steps per second: 311, episode reward: 225.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.277 [-2.595, 0.882], loss: 0.907506, mean_absolute_error: 9.167953, mean_q: 18.650265\n",
            "  2439/50000: episode: 58, duration: 0.624s, episode steps: 190, steps per second: 304, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.288 [-2.173, 0.930], loss: 0.779926, mean_absolute_error: 10.086788, mean_q: 20.672766\n",
            "  2619/50000: episode: 59, duration: 0.599s, episode steps: 180, steps per second: 300, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.367 [-2.428, 0.917], loss: 0.979920, mean_absolute_error: 11.005081, mean_q: 22.490877\n",
            "  2841/50000: episode: 60, duration: 0.727s, episode steps: 222, steps per second: 305, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.265 [-2.107, 1.344], loss: 1.209704, mean_absolute_error: 12.023653, mean_q: 24.546757\n",
            "  3037/50000: episode: 61, duration: 0.642s, episode steps: 196, steps per second: 305, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.341 [-2.411, 0.826], loss: 1.278556, mean_absolute_error: 12.940577, mean_q: 26.452847\n",
            "  3234/50000: episode: 62, duration: 0.635s, episode steps: 197, steps per second: 310, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.329 [-2.416, 0.865], loss: 1.112851, mean_absolute_error: 13.829374, mean_q: 28.377949\n",
            "  3409/50000: episode: 63, duration: 0.567s, episode steps: 175, steps per second: 309, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.368 [-2.534, 0.906], loss: 1.162589, mean_absolute_error: 14.764777, mean_q: 30.235992\n",
            "  3598/50000: episode: 64, duration: 0.627s, episode steps: 189, steps per second: 301, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.352 [-2.522, 0.956], loss: 1.787652, mean_absolute_error: 15.629774, mean_q: 31.875057\n",
            "  3773/50000: episode: 65, duration: 2.449s, episode steps: 175, steps per second: 71, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.372 [-2.423, 0.854], loss: 1.601234, mean_absolute_error: 16.419807, mean_q: 33.508865\n",
            "  4004/50000: episode: 66, duration: 0.847s, episode steps: 231, steps per second: 273, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.283 [-2.401, 0.823], loss: 1.551564, mean_absolute_error: 17.413986, mean_q: 35.577236\n",
            "  4201/50000: episode: 67, duration: 0.661s, episode steps: 197, steps per second: 298, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.329 [-2.415, 0.862], loss: 1.616753, mean_absolute_error: 18.337896, mean_q: 37.501537\n",
            "  4439/50000: episode: 68, duration: 0.832s, episode steps: 238, steps per second: 286, episode reward: 238.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.279 [-2.413, 0.731], loss: 1.569883, mean_absolute_error: 19.420538, mean_q: 39.748287\n",
            "  4626/50000: episode: 69, duration: 0.649s, episode steps: 187, steps per second: 288, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.347 [-2.416, 0.994], loss: 2.229885, mean_absolute_error: 20.308493, mean_q: 41.443432\n",
            "  4846/50000: episode: 70, duration: 0.759s, episode steps: 220, steps per second: 290, episode reward: 220.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.299 [-2.436, 0.836], loss: 2.122910, mean_absolute_error: 21.258926, mean_q: 43.302299\n",
            "  5094/50000: episode: 71, duration: 0.848s, episode steps: 248, steps per second: 292, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.268 [-2.426, 0.866], loss: 2.337975, mean_absolute_error: 22.097258, mean_q: 45.008568\n",
            "  5317/50000: episode: 72, duration: 0.763s, episode steps: 223, steps per second: 292, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.292 [-2.422, 0.829], loss: 2.080838, mean_absolute_error: 23.177822, mean_q: 47.215176\n",
            "  5501/50000: episode: 73, duration: 0.637s, episode steps: 184, steps per second: 289, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.355 [-2.435, 0.678], loss: 1.853192, mean_absolute_error: 23.946156, mean_q: 48.818043\n",
            "  5749/50000: episode: 74, duration: 0.845s, episode steps: 248, steps per second: 293, episode reward: 248.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.264 [-2.532, 0.867], loss: 2.407633, mean_absolute_error: 24.810177, mean_q: 50.638664\n",
            "  5946/50000: episode: 75, duration: 0.641s, episode steps: 197, steps per second: 308, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.335 [-2.424, 0.722], loss: 1.943243, mean_absolute_error: 25.717163, mean_q: 52.532837\n",
            "  6360/50000: episode: 76, duration: 1.374s, episode steps: 414, steps per second: 301, episode reward: 414.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.189 [-1.101, 2.414], loss: 2.679806, mean_absolute_error: 27.062641, mean_q: 55.156193\n",
            "  6535/50000: episode: 77, duration: 0.573s, episode steps: 175, steps per second: 305, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.370 [-2.410, 0.701], loss: 1.989913, mean_absolute_error: 28.136272, mean_q: 57.480724\n",
            "  6880/50000: episode: 78, duration: 1.104s, episode steps: 345, steps per second: 313, episode reward: 345.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.238 [-0.831, 2.419], loss: 2.873265, mean_absolute_error: 29.181721, mean_q: 59.430912\n",
            "  7111/50000: episode: 79, duration: 0.761s, episode steps: 231, steps per second: 304, episode reward: 231.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.291 [-2.421, 0.863], loss: 2.703516, mean_absolute_error: 30.268185, mean_q: 61.582745\n",
            "  7316/50000: episode: 80, duration: 0.693s, episode steps: 205, steps per second: 296, episode reward: 205.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.326 [-2.440, 0.780], loss: 3.490713, mean_absolute_error: 31.072613, mean_q: 63.135590\n",
            "  7628/50000: episode: 81, duration: 1.018s, episode steps: 312, steps per second: 306, episode reward: 312.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.211 [-2.406, 0.795], loss: 3.230108, mean_absolute_error: 31.952467, mean_q: 64.939400\n",
            "  7840/50000: episode: 82, duration: 0.711s, episode steps: 212, steps per second: 298, episode reward: 212.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.322 [-2.443, 1.047], loss: 2.960438, mean_absolute_error: 32.857475, mean_q: 66.796326\n",
            "  8056/50000: episode: 83, duration: 0.722s, episode steps: 216, steps per second: 299, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.309 [-2.433, 0.761], loss: 2.856774, mean_absolute_error: 33.420967, mean_q: 67.945015\n",
            "  8265/50000: episode: 84, duration: 0.712s, episode steps: 209, steps per second: 293, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.310 [-2.412, 1.152], loss: 3.044409, mean_absolute_error: 34.203186, mean_q: 69.453293\n",
            "  8524/50000: episode: 85, duration: 0.861s, episode steps: 259, steps per second: 301, episode reward: 259.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.258 [-2.434, 0.914], loss: 3.121327, mean_absolute_error: 34.747971, mean_q: 70.710480\n",
            "  8774/50000: episode: 86, duration: 0.853s, episode steps: 250, steps per second: 293, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.262 [-2.434, 0.774], loss: 3.228848, mean_absolute_error: 35.648037, mean_q: 72.415352\n",
            "  8960/50000: episode: 87, duration: 0.678s, episode steps: 186, steps per second: 274, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.350 [-2.518, 0.889], loss: 3.691253, mean_absolute_error: 36.198391, mean_q: 73.541695\n",
            "  9148/50000: episode: 88, duration: 0.648s, episode steps: 188, steps per second: 290, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.348 [-2.411, 0.788], loss: 2.883549, mean_absolute_error: 36.651108, mean_q: 74.542183\n",
            "  9388/50000: episode: 89, duration: 0.840s, episode steps: 240, steps per second: 286, episode reward: 240.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.278 [-2.601, 0.810], loss: 4.035560, mean_absolute_error: 37.312157, mean_q: 75.716911\n",
            "  9596/50000: episode: 90, duration: 0.703s, episode steps: 208, steps per second: 296, episode reward: 208.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.313 [-2.404, 0.697], loss: 3.303680, mean_absolute_error: 37.974735, mean_q: 77.033707\n",
            "  9803/50000: episode: 91, duration: 0.703s, episode steps: 207, steps per second: 294, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.323 [-2.438, 0.636], loss: 3.163780, mean_absolute_error: 38.442867, mean_q: 78.069756\n",
            " 10059/50000: episode: 92, duration: 0.846s, episode steps: 256, steps per second: 303, episode reward: 256.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.262 [-2.416, 0.994], loss: 2.633021, mean_absolute_error: 39.171272, mean_q: 79.556740\n",
            " 10268/50000: episode: 93, duration: 0.694s, episode steps: 209, steps per second: 301, episode reward: 209.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.311 [-2.404, 0.997], loss: 3.023851, mean_absolute_error: 39.693676, mean_q: 80.624817\n",
            " 10482/50000: episode: 94, duration: 0.715s, episode steps: 214, steps per second: 299, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.308 [-2.438, 1.066], loss: 4.234980, mean_absolute_error: 40.080502, mean_q: 81.240417\n",
            " 10870/50000: episode: 95, duration: 1.288s, episode steps: 388, steps per second: 301, episode reward: 388.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.213 [-0.895, 2.416], loss: 2.761139, mean_absolute_error: 41.064873, mean_q: 83.334770\n",
            " 11105/50000: episode: 96, duration: 0.783s, episode steps: 235, steps per second: 300, episode reward: 235.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.284 [-2.432, 0.767], loss: 2.859252, mean_absolute_error: 41.770172, mean_q: 84.762337\n",
            " 11288/50000: episode: 97, duration: 0.613s, episode steps: 183, steps per second: 299, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.347 [-2.403, 0.740], loss: 2.939186, mean_absolute_error: 41.972446, mean_q: 84.975601\n",
            " 11584/50000: episode: 98, duration: 0.998s, episode steps: 296, steps per second: 297, episode reward: 296.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.226 [-2.536, 0.701], loss: 2.963611, mean_absolute_error: 42.682446, mean_q: 86.408798\n",
            " 11797/50000: episode: 99, duration: 0.728s, episode steps: 213, steps per second: 293, episode reward: 213.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.309 [-2.425, 0.813], loss: 2.235398, mean_absolute_error: 43.300289, mean_q: 87.645302\n",
            " 11976/50000: episode: 100, duration: 0.613s, episode steps: 179, steps per second: 292, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.356 [-2.422, 0.736], loss: 3.667683, mean_absolute_error: 43.535446, mean_q: 88.118668\n",
            " 12160/50000: episode: 101, duration: 0.612s, episode steps: 184, steps per second: 300, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.348 [-2.404, 0.836], loss: 2.277546, mean_absolute_error: 43.785091, mean_q: 88.647690\n",
            " 12378/50000: episode: 102, duration: 0.719s, episode steps: 218, steps per second: 303, episode reward: 218.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.301 [-2.409, 0.803], loss: 3.409411, mean_absolute_error: 43.996510, mean_q: 89.017342\n",
            " 12592/50000: episode: 103, duration: 0.730s, episode steps: 214, steps per second: 293, episode reward: 214.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.305 [-2.563, 0.947], loss: 3.059172, mean_absolute_error: 44.565769, mean_q: 90.106522\n",
            " 12787/50000: episode: 104, duration: 0.643s, episode steps: 195, steps per second: 303, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.336 [-2.425, 1.118], loss: 2.789803, mean_absolute_error: 44.879887, mean_q: 90.776741\n",
            " 13006/50000: episode: 105, duration: 0.728s, episode steps: 219, steps per second: 301, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.298 [-2.417, 0.726], loss: 2.729695, mean_absolute_error: 45.217373, mean_q: 91.451424\n",
            " 13196/50000: episode: 106, duration: 0.645s, episode steps: 190, steps per second: 295, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.340 [-2.409, 0.867], loss: 4.537601, mean_absolute_error: 45.508831, mean_q: 91.873398\n",
            " 13641/50000: episode: 107, duration: 1.449s, episode steps: 445, steps per second: 307, episode reward: 445.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.142 [-2.409, 1.023], loss: 3.673536, mean_absolute_error: 45.910225, mean_q: 92.788147\n",
            " 14030/50000: episode: 108, duration: 1.281s, episode steps: 389, steps per second: 304, episode reward: 389.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.207 [-0.868, 2.403], loss: 3.259766, mean_absolute_error: 46.568760, mean_q: 94.245079\n",
            " 14428/50000: episode: 109, duration: 1.314s, episode steps: 398, steps per second: 303, episode reward: 398.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.160 [-2.416, 0.865], loss: 3.600732, mean_absolute_error: 47.265213, mean_q: 95.683670\n",
            " 14622/50000: episode: 110, duration: 0.663s, episode steps: 194, steps per second: 292, episode reward: 194.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.334 [-2.426, 0.796], loss: 4.106476, mean_absolute_error: 47.299664, mean_q: 95.585976\n",
            " 14808/50000: episode: 111, duration: 0.617s, episode steps: 186, steps per second: 301, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.351 [-2.592, 0.776], loss: 2.532305, mean_absolute_error: 47.936066, mean_q: 97.002014\n",
            " 15053/50000: episode: 112, duration: 0.821s, episode steps: 245, steps per second: 298, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.265 [-2.435, 0.804], loss: 2.697421, mean_absolute_error: 48.139763, mean_q: 97.411041\n",
            " 15389/50000: episode: 113, duration: 1.108s, episode steps: 336, steps per second: 303, episode reward: 336.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.245 [-0.778, 2.415], loss: 3.683185, mean_absolute_error: 48.415688, mean_q: 97.852501\n",
            " 15596/50000: episode: 114, duration: 0.692s, episode steps: 207, steps per second: 299, episode reward: 207.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.314 [-2.423, 1.073], loss: 3.871579, mean_absolute_error: 48.885563, mean_q: 98.859627\n",
            " 15877/50000: episode: 115, duration: 0.919s, episode steps: 281, steps per second: 306, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.240 [-2.611, 0.906], loss: 2.298334, mean_absolute_error: 48.810066, mean_q: 98.854851\n",
            " 16211/50000: episode: 116, duration: 1.126s, episode steps: 334, steps per second: 297, episode reward: 334.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.191 [-2.436, 1.075], loss: 3.397034, mean_absolute_error: 49.519753, mean_q: 100.274048\n",
            " 16401/50000: episode: 117, duration: 0.640s, episode steps: 190, steps per second: 297, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.336 [-2.403, 0.911], loss: 4.592874, mean_absolute_error: 49.739380, mean_q: 100.460190\n",
            " 16604/50000: episode: 118, duration: 0.673s, episode steps: 203, steps per second: 302, episode reward: 203.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.315 [-2.606, 0.855], loss: 3.574672, mean_absolute_error: 49.838810, mean_q: 100.773094\n",
            " 16787/50000: episode: 119, duration: 0.613s, episode steps: 183, steps per second: 299, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.348 [-2.565, 1.014], loss: 4.260766, mean_absolute_error: 49.845715, mean_q: 100.634560\n",
            " 16986/50000: episode: 120, duration: 0.647s, episode steps: 199, steps per second: 308, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.321 [-2.531, 0.780], loss: 3.503882, mean_absolute_error: 50.059135, mean_q: 101.164635\n",
            " 17151/50000: episode: 121, duration: 0.542s, episode steps: 165, steps per second: 305, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.378 [-2.425, 1.096], loss: 2.664447, mean_absolute_error: 50.048901, mean_q: 101.244957\n",
            " 17415/50000: episode: 122, duration: 0.868s, episode steps: 264, steps per second: 304, episode reward: 264.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.246 [-2.404, 0.829], loss: 4.275167, mean_absolute_error: 50.138756, mean_q: 101.330421\n",
            " 17588/50000: episode: 123, duration: 0.581s, episode steps: 173, steps per second: 298, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.373 [-2.617, 0.714], loss: 3.149121, mean_absolute_error: 50.158485, mean_q: 101.456352\n",
            " 17869/50000: episode: 124, duration: 0.921s, episode steps: 281, steps per second: 305, episode reward: 281.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.237 [-2.420, 0.878], loss: 3.068679, mean_absolute_error: 50.161823, mean_q: 101.376472\n",
            " 18135/50000: episode: 125, duration: 0.883s, episode steps: 266, steps per second: 301, episode reward: 266.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.253 [-2.440, 0.802], loss: 3.477365, mean_absolute_error: 50.568001, mean_q: 102.198792\n",
            " 18335/50000: episode: 126, duration: 2.960s, episode steps: 200, steps per second: 68, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.328 [-2.438, 0.800], loss: 3.259235, mean_absolute_error: 50.640450, mean_q: 102.505432\n",
            " 18554/50000: episode: 127, duration: 0.815s, episode steps: 219, steps per second: 269, episode reward: 219.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.299 [-2.730, 0.970], loss: 3.588248, mean_absolute_error: 50.659763, mean_q: 102.374466\n",
            " 18919/50000: episode: 128, duration: 1.189s, episode steps: 365, steps per second: 307, episode reward: 365.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.181 [-2.525, 0.897], loss: 2.795195, mean_absolute_error: 50.856880, mean_q: 102.818687\n",
            " 19206/50000: episode: 129, duration: 0.961s, episode steps: 287, steps per second: 299, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.228 [-2.406, 0.917], loss: 4.187118, mean_absolute_error: 51.090931, mean_q: 103.196922\n",
            " 19596/50000: episode: 130, duration: 1.294s, episode steps: 390, steps per second: 301, episode reward: 390.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.216 [-0.980, 2.406], loss: 2.948949, mean_absolute_error: 51.133453, mean_q: 103.415466\n",
            " 19796/50000: episode: 131, duration: 0.661s, episode steps: 200, steps per second: 303, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.322 [-2.427, 1.237], loss: 2.305396, mean_absolute_error: 51.634884, mean_q: 104.282379\n",
            " 20095/50000: episode: 132, duration: 0.969s, episode steps: 299, steps per second: 308, episode reward: 299.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.221 [-2.440, 1.001], loss: 3.162298, mean_absolute_error: 51.840958, mean_q: 104.665474\n",
            " 20372/50000: episode: 133, duration: 0.929s, episode steps: 277, steps per second: 298, episode reward: 277.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.239 [-2.431, 0.927], loss: 2.496321, mean_absolute_error: 51.617950, mean_q: 104.304405\n",
            " 20599/50000: episode: 134, duration: 0.755s, episode steps: 227, steps per second: 301, episode reward: 227.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.297 [-2.443, 0.962], loss: 4.546569, mean_absolute_error: 51.745525, mean_q: 104.292587\n",
            " 20814/50000: episode: 135, duration: 0.716s, episode steps: 215, steps per second: 300, episode reward: 215.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.300 [-2.598, 1.023], loss: 3.387630, mean_absolute_error: 51.786488, mean_q: 104.414864\n",
            " 21059/50000: episode: 136, duration: 0.834s, episode steps: 245, steps per second: 294, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.275 [-2.414, 0.831], loss: 2.834877, mean_absolute_error: 51.781879, mean_q: 104.399307\n",
            " 21260/50000: episode: 137, duration: 0.683s, episode steps: 201, steps per second: 294, episode reward: 201.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.321 [-2.412, 1.130], loss: 2.856223, mean_absolute_error: 51.914860, mean_q: 104.718208\n",
            " 21477/50000: episode: 138, duration: 0.737s, episode steps: 217, steps per second: 294, episode reward: 217.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.307 [-2.408, 0.919], loss: 2.500313, mean_absolute_error: 51.668762, mean_q: 104.271927\n",
            " 21710/50000: episode: 139, duration: 0.801s, episode steps: 233, steps per second: 291, episode reward: 233.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.283 [-2.622, 1.014], loss: 2.673391, mean_absolute_error: 51.859432, mean_q: 104.795364\n",
            " 21899/50000: episode: 140, duration: 0.649s, episode steps: 189, steps per second: 291, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.350 [-2.408, 0.563], loss: 4.294762, mean_absolute_error: 51.729366, mean_q: 104.508698\n",
            " 22160/50000: episode: 141, duration: 0.880s, episode steps: 261, steps per second: 297, episode reward: 261.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.254 [-2.415, 0.836], loss: 3.284882, mean_absolute_error: 51.673607, mean_q: 104.453934\n",
            " 22383/50000: episode: 142, duration: 0.739s, episode steps: 223, steps per second: 302, episode reward: 223.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.298 [-2.406, 0.805], loss: 3.116823, mean_absolute_error: 51.716946, mean_q: 104.393570\n",
            " 22670/50000: episode: 143, duration: 0.944s, episode steps: 287, steps per second: 304, episode reward: 287.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.287 [-0.774, 2.410], loss: 2.886426, mean_absolute_error: 51.901615, mean_q: 104.822311\n",
            " 22933/50000: episode: 144, duration: 0.893s, episode steps: 263, steps per second: 295, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.250 [-2.601, 1.059], loss: 2.129292, mean_absolute_error: 52.059086, mean_q: 105.260269\n",
            " 23324/50000: episode: 145, duration: 1.306s, episode steps: 391, steps per second: 299, episode reward: 391.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.218 [-1.046, 2.419], loss: 2.500123, mean_absolute_error: 51.951275, mean_q: 104.971924\n",
            " 23494/50000: episode: 146, duration: 0.585s, episode steps: 170, steps per second: 291, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.385 [-2.431, 0.908], loss: 2.489575, mean_absolute_error: 51.980312, mean_q: 104.974701\n",
            " 23674/50000: episode: 147, duration: 0.595s, episode steps: 180, steps per second: 303, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.366 [-2.608, 1.011], loss: 3.074788, mean_absolute_error: 52.143078, mean_q: 105.270897\n",
            " 24086/50000: episode: 148, duration: 1.392s, episode steps: 412, steps per second: 296, episode reward: 412.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.209 [-1.081, 2.432], loss: 2.812292, mean_absolute_error: 52.294525, mean_q: 105.603439\n",
            " 24312/50000: episode: 149, duration: 0.768s, episode steps: 226, steps per second: 294, episode reward: 226.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.295 [-2.565, 0.793], loss: 3.098720, mean_absolute_error: 51.840031, mean_q: 104.616150\n",
            " 24491/50000: episode: 150, duration: 0.611s, episode steps: 179, steps per second: 293, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.364 [-2.419, 1.248], loss: 2.477927, mean_absolute_error: 52.574799, mean_q: 106.048874\n",
            " 24740/50000: episode: 151, duration: 0.819s, episode steps: 249, steps per second: 304, episode reward: 249.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.269 [-2.419, 0.789], loss: 2.937394, mean_absolute_error: 51.754406, mean_q: 104.420227\n",
            " 24944/50000: episode: 152, duration: 0.679s, episode steps: 204, steps per second: 300, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.325 [-2.444, 1.052], loss: 2.070559, mean_absolute_error: 51.396896, mean_q: 103.802620\n",
            " 25174/50000: episode: 153, duration: 0.736s, episode steps: 230, steps per second: 313, episode reward: 230.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.289 [-2.408, 0.943], loss: 2.065806, mean_absolute_error: 51.980309, mean_q: 104.948250\n",
            " 25408/50000: episode: 154, duration: 0.777s, episode steps: 234, steps per second: 301, episode reward: 234.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.289 [-2.401, 0.839], loss: 2.039841, mean_absolute_error: 51.791607, mean_q: 104.537247\n",
            " 25773/50000: episode: 155, duration: 1.179s, episode steps: 365, steps per second: 310, episode reward: 365.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.228 [-1.079, 2.410], loss: 3.737324, mean_absolute_error: 51.877449, mean_q: 104.529823\n",
            " 25989/50000: episode: 156, duration: 0.710s, episode steps: 216, steps per second: 304, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.304 [-2.404, 0.825], loss: 2.436276, mean_absolute_error: 51.911785, mean_q: 104.810944\n",
            " 26239/50000: episode: 157, duration: 0.819s, episode steps: 250, steps per second: 305, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.271 [-2.408, 0.967], loss: 2.227556, mean_absolute_error: 51.914940, mean_q: 104.824699\n",
            " 26482/50000: episode: 158, duration: 0.795s, episode steps: 243, steps per second: 306, episode reward: 243.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.281 [-2.438, 1.037], loss: 2.368242, mean_absolute_error: 51.759228, mean_q: 104.568275\n",
            " 26978/50000: episode: 159, duration: 1.605s, episode steps: 496, steps per second: 309, episode reward: 496.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.177 [-1.132, 2.422], loss: 2.660699, mean_absolute_error: 51.868176, mean_q: 104.588142\n",
            " 27214/50000: episode: 160, duration: 0.783s, episode steps: 236, steps per second: 301, episode reward: 236.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.287 [-2.515, 0.924], loss: 3.282988, mean_absolute_error: 51.897297, mean_q: 104.600815\n",
            " 27609/50000: episode: 161, duration: 1.281s, episode steps: 395, steps per second: 308, episode reward: 395.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.222 [-0.803, 2.405], loss: 2.384444, mean_absolute_error: 51.970680, mean_q: 104.853119\n",
            " 27992/50000: episode: 162, duration: 1.232s, episode steps: 383, steps per second: 311, episode reward: 383.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.229 [-0.893, 2.429], loss: 1.951797, mean_absolute_error: 51.941677, mean_q: 104.856415\n",
            " 28245/50000: episode: 163, duration: 0.830s, episode steps: 253, steps per second: 305, episode reward: 253.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.269 [-2.413, 0.980], loss: 2.600892, mean_absolute_error: 52.175892, mean_q: 105.332550\n",
            " 28678/50000: episode: 164, duration: 1.410s, episode steps: 433, steps per second: 307, episode reward: 433.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.203 [-0.886, 2.417], loss: 2.190110, mean_absolute_error: 51.940350, mean_q: 104.789330\n",
            " 28900/50000: episode: 165, duration: 0.722s, episode steps: 222, steps per second: 307, episode reward: 222.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.309 [-2.440, 0.809], loss: 2.666524, mean_absolute_error: 51.971439, mean_q: 104.790329\n",
            " 29319/50000: episode: 166, duration: 1.368s, episode steps: 419, steps per second: 306, episode reward: 419.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.205 [-1.039, 2.415], loss: 2.509595, mean_absolute_error: 51.967007, mean_q: 104.872299\n",
            " 29569/50000: episode: 167, duration: 0.818s, episode steps: 250, steps per second: 306, episode reward: 250.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.270 [-2.427, 0.960], loss: 2.725735, mean_absolute_error: 51.734798, mean_q: 104.416672\n",
            " 29903/50000: episode: 168, duration: 1.076s, episode steps: 334, steps per second: 310, episode reward: 334.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.207 [-2.419, 1.072], loss: 2.237681, mean_absolute_error: 52.012100, mean_q: 105.062462\n",
            " 30148/50000: episode: 169, duration: 0.796s, episode steps: 245, steps per second: 308, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.279 [-2.583, 0.918], loss: 2.600434, mean_absolute_error: 51.774498, mean_q: 104.653191\n",
            " 30451/50000: episode: 170, duration: 1.006s, episode steps: 303, steps per second: 301, episode reward: 303.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.225 [-2.413, 1.084], loss: 2.245486, mean_absolute_error: 51.968151, mean_q: 104.972755\n",
            " 30787/50000: episode: 171, duration: 1.106s, episode steps: 336, steps per second: 304, episode reward: 336.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.205 [-2.407, 0.889], loss: 2.838996, mean_absolute_error: 51.808937, mean_q: 104.593262\n",
            " 31160/50000: episode: 172, duration: 1.206s, episode steps: 373, steps per second: 309, episode reward: 373.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.231 [-1.001, 2.419], loss: 2.096892, mean_absolute_error: 51.883068, mean_q: 104.656052\n",
            " 31466/50000: episode: 173, duration: 1.001s, episode steps: 306, steps per second: 306, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.228 [-2.431, 0.910], loss: 2.460387, mean_absolute_error: 51.543385, mean_q: 104.024628\n",
            " 31731/50000: episode: 174, duration: 0.853s, episode steps: 265, steps per second: 311, episode reward: 265.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.265 [-2.439, 1.146], loss: 2.074798, mean_absolute_error: 52.052719, mean_q: 105.113541\n",
            " 32115/50000: episode: 175, duration: 1.246s, episode steps: 384, steps per second: 308, episode reward: 384.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.223 [-0.758, 2.412], loss: 2.640029, mean_absolute_error: 51.774662, mean_q: 104.394104\n",
            " 32319/50000: episode: 176, duration: 0.659s, episode steps: 204, steps per second: 309, episode reward: 204.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.331 [-2.437, 0.760], loss: 1.532168, mean_absolute_error: 51.683449, mean_q: 104.358917\n",
            " 32713/50000: episode: 177, duration: 1.252s, episode steps: 394, steps per second: 315, episode reward: 394.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.218 [-1.068, 2.425], loss: 2.010432, mean_absolute_error: 51.812122, mean_q: 104.598145\n",
            " 33213/50000: episode: 178, duration: 1.624s, episode steps: 500, steps per second: 308, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.017 [-1.115, 1.358], loss: 1.604505, mean_absolute_error: 52.075661, mean_q: 105.099358\n",
            " 33713/50000: episode: 179, duration: 1.637s, episode steps: 500, steps per second: 305, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.064 [-1.078, 0.963], loss: 2.024658, mean_absolute_error: 52.200890, mean_q: 105.236534\n",
            " 34074/50000: episode: 180, duration: 1.175s, episode steps: 361, steps per second: 307, episode reward: 361.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.191 [-2.433, 1.029], loss: 3.440338, mean_absolute_error: 52.107090, mean_q: 104.994873\n",
            " 34407/50000: episode: 181, duration: 1.090s, episode steps: 333, steps per second: 306, episode reward: 333.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.204 [-2.738, 1.066], loss: 1.713143, mean_absolute_error: 52.048798, mean_q: 104.995621\n",
            " 34646/50000: episode: 182, duration: 0.779s, episode steps: 239, steps per second: 307, episode reward: 239.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.282 [-2.403, 0.788], loss: 2.209277, mean_absolute_error: 51.925114, mean_q: 104.788589\n",
            " 34963/50000: episode: 183, duration: 1.032s, episode steps: 317, steps per second: 307, episode reward: 317.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.209 [-2.787, 1.060], loss: 1.762352, mean_absolute_error: 52.166073, mean_q: 105.257851\n",
            " 35179/50000: episode: 184, duration: 0.725s, episode steps: 216, steps per second: 298, episode reward: 216.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.306 [-3.020, 1.327], loss: 2.112200, mean_absolute_error: 51.998543, mean_q: 104.822823\n",
            " 35589/50000: episode: 185, duration: 1.320s, episode steps: 410, steps per second: 311, episode reward: 410.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.221 [-0.751, 2.415], loss: 1.682200, mean_absolute_error: 52.083050, mean_q: 105.074112\n",
            " 35899/50000: episode: 186, duration: 1.016s, episode steps: 310, steps per second: 305, episode reward: 310.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.214 [-2.776, 1.018], loss: 1.820954, mean_absolute_error: 51.835205, mean_q: 104.540489\n",
            " 36162/50000: episode: 187, duration: 0.866s, episode steps: 263, steps per second: 304, episode reward: 263.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.261 [-2.759, 0.950], loss: 2.602733, mean_absolute_error: 51.575928, mean_q: 103.960213\n",
            " 36468/50000: episode: 188, duration: 1.024s, episode steps: 306, steps per second: 299, episode reward: 306.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.231 [-2.431, 0.826], loss: 2.602186, mean_absolute_error: 51.950584, mean_q: 104.830223\n",
            " 36840/50000: episode: 189, duration: 1.196s, episode steps: 372, steps per second: 311, episode reward: 372.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.232 [-0.812, 2.415], loss: 3.001942, mean_absolute_error: 51.651009, mean_q: 104.318336\n",
            " 37085/50000: episode: 190, duration: 0.786s, episode steps: 245, steps per second: 312, episode reward: 245.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: -0.274 [-2.426, 0.807], loss: 3.154172, mean_absolute_error: 52.190853, mean_q: 105.298576\n",
            " 37585/50000: episode: 191, duration: 1.628s, episode steps: 500, steps per second: 307, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.153 [-0.862, 2.069], loss: 1.537171, mean_absolute_error: 51.571014, mean_q: 104.108261\n",
            " 38085/50000: episode: 192, duration: 1.625s, episode steps: 500, steps per second: 308, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.502 [0.000, 1.000], mean observation: 0.055 [-1.057, 0.909], loss: 3.329908, mean_absolute_error: 51.792999, mean_q: 104.527672\n",
            " 38359/50000: episode: 193, duration: 0.892s, episode steps: 274, steps per second: 307, episode reward: 274.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.251 [-2.589, 0.968], loss: 1.890391, mean_absolute_error: 51.737774, mean_q: 104.453651\n",
            " 38583/50000: episode: 194, duration: 0.749s, episode steps: 224, steps per second: 299, episode reward: 224.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.295 [-2.602, 1.112], loss: 3.008967, mean_absolute_error: 52.362354, mean_q: 105.499802\n",
            " 38984/50000: episode: 195, duration: 1.289s, episode steps: 401, steps per second: 311, episode reward: 401.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.169 [-2.718, 1.008], loss: 1.853362, mean_absolute_error: 51.562763, mean_q: 103.970673\n",
            " 39414/50000: episode: 196, duration: 1.389s, episode steps: 430, steps per second: 310, episode reward: 430.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.205 [-1.113, 2.419], loss: 1.983520, mean_absolute_error: 51.753860, mean_q: 104.355980\n",
            " 39815/50000: episode: 197, duration: 1.328s, episode steps: 401, steps per second: 302, episode reward: 401.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.221 [-1.079, 2.413], loss: 3.417697, mean_absolute_error: 51.749653, mean_q: 104.315208\n",
            " 40263/50000: episode: 198, duration: 1.437s, episode steps: 448, steps per second: 312, episode reward: 448.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.200 [-0.993, 2.424], loss: 3.035959, mean_absolute_error: 51.950474, mean_q: 104.742371\n",
            " 40739/50000: episode: 199, duration: 1.545s, episode steps: 476, steps per second: 308, episode reward: 476.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.196 [-0.837, 2.413], loss: 1.798034, mean_absolute_error: 51.755772, mean_q: 104.422920\n",
            " 41239/50000: episode: 200, duration: 1.607s, episode steps: 500, steps per second: 311, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.990, 0.992], loss: 2.973894, mean_absolute_error: 51.725452, mean_q: 104.335785\n",
            " 41622/50000: episode: 201, duration: 1.225s, episode steps: 383, steps per second: 313, episode reward: 383.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.235 [-0.997, 2.407], loss: 1.609789, mean_absolute_error: 51.917927, mean_q: 104.816315\n",
            " 42122/50000: episode: 202, duration: 1.633s, episode steps: 500, steps per second: 306, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.125 [-0.918, 1.635], loss: 2.547501, mean_absolute_error: 51.949837, mean_q: 104.840408\n",
            " 42560/50000: episode: 203, duration: 1.427s, episode steps: 438, steps per second: 307, episode reward: 438.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.207 [-0.823, 2.403], loss: 2.198150, mean_absolute_error: 52.103390, mean_q: 105.008530\n",
            " 43060/50000: episode: 204, duration: 1.639s, episode steps: 500, steps per second: 305, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.070 [-1.356, 1.063], loss: 3.263162, mean_absolute_error: 51.633369, mean_q: 104.068527\n",
            " 43546/50000: episode: 205, duration: 1.570s, episode steps: 486, steps per second: 309, episode reward: 486.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.192 [-0.790, 2.407], loss: 1.685630, mean_absolute_error: 52.366798, mean_q: 105.513100\n",
            " 43925/50000: episode: 206, duration: 1.221s, episode steps: 379, steps per second: 310, episode reward: 379.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.234 [-0.876, 2.406], loss: 2.288968, mean_absolute_error: 51.840237, mean_q: 104.469864\n",
            " 44425/50000: episode: 207, duration: 1.646s, episode steps: 500, steps per second: 304, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: -0.015 [-0.959, 0.956], loss: 2.128436, mean_absolute_error: 51.811394, mean_q: 104.519440\n",
            " 44925/50000: episode: 208, duration: 1.613s, episode steps: 500, steps per second: 310, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-1.049, 1.007], loss: 3.082603, mean_absolute_error: 52.079975, mean_q: 104.987106\n",
            " 45425/50000: episode: 209, duration: 1.625s, episode steps: 500, steps per second: 308, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.173 [-1.119, 2.281], loss: 4.166469, mean_absolute_error: 52.028553, mean_q: 104.806953\n",
            " 45923/50000: episode: 210, duration: 1.621s, episode steps: 498, steps per second: 307, episode reward: 498.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.182 [-0.932, 2.423], loss: 2.313129, mean_absolute_error: 51.967350, mean_q: 104.757545\n",
            " 46353/50000: episode: 211, duration: 1.396s, episode steps: 430, steps per second: 308, episode reward: 430.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.208 [-1.268, 2.421], loss: 3.674776, mean_absolute_error: 52.218796, mean_q: 105.058655\n",
            " 46853/50000: episode: 212, duration: 1.628s, episode steps: 500, steps per second: 307, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.163 [-0.994, 2.220], loss: 2.965473, mean_absolute_error: 52.327728, mean_q: 105.391304\n",
            " 47208/50000: episode: 213, duration: 1.152s, episode steps: 355, steps per second: 308, episode reward: 355.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.250 [-0.792, 2.422], loss: 2.745266, mean_absolute_error: 52.267948, mean_q: 105.233231\n",
            " 47613/50000: episode: 214, duration: 1.325s, episode steps: 405, steps per second: 306, episode reward: 405.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.219 [-1.078, 2.420], loss: 2.853643, mean_absolute_error: 52.369801, mean_q: 105.409286\n",
            " 48113/50000: episode: 215, duration: 1.629s, episode steps: 500, steps per second: 307, episode reward: 500.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.498 [0.000, 1.000], mean observation: 0.005 [-1.142, 1.130], loss: 1.889353, mean_absolute_error: 52.127392, mean_q: 104.978447\n",
            " 48493/50000: episode: 216, duration: 1.229s, episode steps: 380, steps per second: 309, episode reward: 380.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.229 [-0.953, 2.405], loss: 1.971951, mean_absolute_error: 52.071545, mean_q: 104.956436\n",
            " 48965/50000: episode: 217, duration: 6.409s, episode steps: 472, steps per second: 74, episode reward: 472.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.182 [-0.994, 2.369], loss: 1.389446, mean_absolute_error: 52.018242, mean_q: 104.818161\n",
            " 49243/50000: episode: 218, duration: 0.983s, episode steps: 278, steps per second: 283, episode reward: 278.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.250 [-2.373, 0.995], loss: 2.793256, mean_absolute_error: 52.190998, mean_q: 105.133095\n",
            " 49708/50000: episode: 219, duration: 1.521s, episode steps: 465, steps per second: 306, episode reward: 465.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.195 [-0.804, 2.413], loss: 2.400992, mean_absolute_error: 51.947918, mean_q: 104.633919\n",
            "done, took 179.761 seconds\n",
            "Testing for 10 episodes ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e34d7cd73d3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'duel_dqn_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mENV_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_weights.h5f'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/rl/core.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_before_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/stats_recorder.py\u001b[0m in \u001b[0;36mbefore_reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to reset environment which is not done. While the monitor is active for {}, you cannot call reset() unless the episode is over.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Tried to reset environment which is not done. While the monitor is active for CartPole-v1, you cannot call reset() unless the episode is over."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZKAKGIh8Jn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn.done = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0f2JkGZvwzo",
        "colab_type": "code",
        "outputId": "b502b87f-98ff-416a-fb9e-75bdb323a29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "show_video()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}